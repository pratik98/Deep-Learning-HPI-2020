{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "week2_804861.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rPbFM4OLV688"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEyj1dRAV68e",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "\n",
        "To start off, we will introduce the problem of regression.\n",
        "This is the task of predicting a *real valued target* $y$\n",
        "given a data point $\\mathbf{x}$.\n",
        "Regression problems are common in practice, arising\n",
        "whenever we want to predict a continuous numerical value.\n",
        "Some examples of regression problems include\n",
        "predicting house prices, stock prices,\n",
        "length of stay (for patients in the hospital),\n",
        "tomorrow's temperature, demand forecasting (for retail sales), and many more.\n",
        "Note that not every prediction problem is a regression problem.\n",
        "In subsequent sections we will discuss classification problems,\n",
        "where our predictions are discrete categories.\n",
        "\n",
        "## Basic Elements of Linear Regression\n",
        "\n",
        "Linear regression, which dates to Gauss and Legendre,\n",
        "is perhaps the simplest, and by far the most popular approach\n",
        "to solving regression problems.\n",
        "What makes linear regression *linear* is that\n",
        "we assume that the output truly can be expressed\n",
        "as a *linear* combination of the input features.\n",
        "\n",
        "\n",
        "### Linear Model\n",
        "\n",
        "To keep things simple, we will start with running example\n",
        "in which we consider the problem\n",
        "of estimating the price of a house (e.g. in dollars)\n",
        "based on area (e.g. in square feet) and age (e.g. in years).\n",
        "More formally, the assumption of linearity suggests\n",
        "that our model can be expressed in the following form:\n",
        "\n",
        "$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b$$\n",
        "\n",
        "In economics papers, it is common for authors to write out linear models in this format with a gigantic equation that spans multiple lines containing terms for every single feature.\n",
        "For the high-dimensional data that we often address in machine learning,\n",
        "writing out the entire model can be tedious.\n",
        "In these cases, we will find it more convenient to use linear algebra notation.\n",
        "In the case of $d$ variables, we could express our prediction $\\hat{y}$ as follows:\n",
        "\n",
        "$$\\hat{y} = w_1 \\cdot x_1 + ... + w_d \\cdot x_d + b$$\n",
        "\n",
        "or alternatively, collecting all features into a single vector $\\mathbf{x}$ and all parameters into a vector $\\mathbf{w}$, we can express our linear model as $\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b$.\n",
        "\n",
        "Above, the vector $\\mathbf{x}$ corresponds to a single data point.\n",
        "Commonly, we will want notation to refer to\n",
        "the entire dataset of all input data points.\n",
        "This matrix, often denoted using a capital letter $X$,\n",
        "is called the *design matrix* and contains one row for every example,\n",
        "and one column for every feature.\n",
        "\n",
        "Given a collection of data points $X$ and a vector\n",
        "containing the corresponding target values $\\mathbf{y}$,\n",
        "the goal of linear regression is to find\n",
        "the *weight* vector $w$ and bias term $b$\n",
        "(also called an *offset* or *intercept*)\n",
        "that associates each data point $\\mathbf{x}_i$\n",
        "with an approximation $\\hat{y}_i$ of its corresponding label $y_i$.\n",
        "\n",
        "Expressed in terms of a single data point,\n",
        "this gives us the expression (same as above)\n",
        "$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$.\n",
        "\n",
        "Finally, for a collection of data points $\\mathbf{X}$,\n",
        "the predictions $\\hat{\\mathbf{y}}$ can be expressed via the matrix-vector product:\n",
        "\n",
        "$${\\hat{\\mathbf{y}}} = X \\mathbf{w} + b$$\n",
        "\n",
        "Even if we believe that the best model\n",
        "to relate $\\mathbf{x}$ and $y$ is linear,\n",
        "it's unlikely that we'd find data where $y$\n",
        "lines up exactly as a linear function of $\\mathbf{x}$.\n",
        "For example, both the target values $y$ and the features $X$\n",
        "might be subject to some amount of measurement error.\n",
        "Thus even when we believe that the linearity assumption holds,\n",
        "we will typically incorporate a noise term to account for such errors.\n",
        "\n",
        "Before we can go about solving for the best setting of the parameters $w$ and $b$, we will need two more things:\n",
        "(i) some way to measure the quality of the current model\n",
        "and (ii) some way to manipulate the model to improve its quality.\n",
        "\n",
        "### Training Data\n",
        "\n",
        "The first thing that we need is training data.\n",
        "Sticking with our running example, we'll need some collection of examples\n",
        "for which we know both the actual selling price of each house\n",
        "as well as their corresponding area and age.\n",
        "Our goal is to identify model parameters\n",
        "that minimize the error between the predicted price and the real price.\n",
        "In the terminology of machine learning, the data set is called a ‘training data’ or ‘training set’, a house (often a house and its price) here comprises one ‘sample’, and its actual selling price is called a ‘label’.\n",
        "The two factors used to predict the label\n",
        "are called ‘features’ or 'covariates'.\n",
        "\n",
        "Typically, we will use $n$ to denote the number of samples in our dataset.\n",
        "We index the samples by $i$, denoting each input data point as $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$ and the corresponding label as $y^{(i)}$.\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "In model training, we need to measure the error\n",
        "between the predicted value and the real value of the price.\n",
        "Usually, we will choose a non-negative number as the error.\n",
        "The smaller the value, the smaller the error.\n",
        "A common choice is the square function.\n",
        "For given parameters $\\mathbf{w}$ and $b$,\n",
        "we can express the error of our prediction on a given a sample as follows:\n",
        "\n",
        "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,$$\n",
        "\n",
        "The constant $1/2$ is just for mathematical convenience,\n",
        "ensuring that after we take the derivative of the loss,\n",
        "the constant coefficient will be $1$.\n",
        "The smaller the error, the closer the predicted price is to the actual price, and when the two are equal, the error will be zero.\n",
        "\n",
        "Since the training dataset is given to us, and thus out of our control,\n",
        "the error is only a function of the model parameters.\n",
        "In machine learning, we call the function that measures the error the ‘loss function’.\n",
        "The squared error function used here is commonly referred to as ‘square loss’.\n",
        "\n",
        "To make things a bit more concrete, consider the example below where we plot a regression problem for a one-dimensional case, e.g. for a model where house prices depend only on area.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1ago9T6RAYLHwLZ-u6w41qba95RlLP-_z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEAYVNOFV68o",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is a single-layer neural network. \n",
        "\n",
        "As you can see, large differences between estimates $\\hat{y}^{(i)}$ and observations $y^{(i)}$ lead to even larger contributions in terms of the loss, due to the quadratic dependence. To measure the quality of a model on the entire dataset, we can simply average the losses on the training set.\n",
        "\n",
        "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
        "\n",
        "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$) that minimize the average loss across all training samples:\n",
        "\n",
        "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n",
        "\n",
        "\n",
        "### Analytic Solution\n",
        "\n",
        "Linear regression happens to be an unusually simple optimization problem.\n",
        "Unlike nearly every other model that we will encounter in this book,\n",
        "linear regression can be solved easily with a simple formula,\n",
        "yielding a global optimum.\n",
        "To start we can subsume the bias $b$ into the parameter $\\mathbf{w}$\n",
        "by appending a column to the design matrix consisting of all $1s$.\n",
        "Then our prediction problem is to minimize $||\\mathbf{y} - X\\mathbf{w}||$.\n",
        "Because this expression has a quadratic form it is clearly convex,\n",
        "and so long as the problem is not degenerate\n",
        "(our features are linearly independent), it is strictly convex.\n",
        "\n",
        "Thus there is just one global critical point on the loss surface\n",
        "corresponding to the global minimum.\n",
        "Taking the derivative of the loss with respect to $\\mathbf{w}$\n",
        "and setting it equal to 0 gives the analytic solution:\n",
        "\n",
        "$$\\mathbf{w}^* = (X^T X)^{-1}X^T y$$\n",
        "\n",
        "While simple problems like linear regression may admit analytic solutions,\n",
        "you should not get used to such good fortune.\n",
        "Although analytic solutions allow for nice mathematical analysis,\n",
        "the requirement of an analytic solution confines one to\n",
        "an restrictive set of models that would exclude all of deep learning.\n",
        "\n",
        "**Exercise:** Implement a function that computes and returns the list of optimal weights w* using the analytic solution formula. (Hint: inputs `X,y` are `numpy` arrays; you can doublecheck your answers by looking at shapes of X, w and y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnUytDo6W5ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PIaBZQFin3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analytic_weights(X, y):\n",
        "  ## write your code here\n",
        "   return torch.mv(torch.mm(torch.from_numpy(np.linalg.pinv(torch.mm(X.t(),X))), X.t()), y)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnEl9Q0jJabZ",
        "colab_type": "code",
        "outputId": "b47babd6-2807-40a8-edd7-37a6118f1582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "#  Testing code\n",
        "  ## write your code here\n",
        "  V_data = [1., 2., 3, 4, 10]\n",
        "  y = torch.tensor(V_data)\n",
        "  print(y)\n",
        "\n",
        "# Creates a matrix\n",
        "  M_data = [[1., 2., 40.], [4., 5., 55.], [7., 8., 88], [7., 5., 88], [7., 25., 88]]\n",
        "  X = torch.tensor(M_data)\n",
        "  print(X)\n",
        "  print(X.t())\n",
        "  print(torch.mm(X.t(),X))\n",
        "  print (torch.inverse(torch.mm(X.t(),X)) )\n",
        "  print(torch.mv(X.t(), y))\n",
        "  print  (torch.mv(torch.inverse(torch.mm(X.t(),X)), torch.mv(X.t(), y))) ## or\n",
        "  #print (torch.mv(torch.from_numpy(np.linalg.inv(torch.mm(X.t(),X))), torch.mv(X.t(), y))) #numpy\n",
        "  #print (torch.mv(torch.from_numpy(np.linalg.pinv(torch.mm(X.t(),X))), torch.mv(X.t(), y))) #numpy\n",
        "\n",
        "  print  (torch.mv(torch.mm(torch.inverse(torch.mm(X.t(),X)), X.t()), y)) ## or\n",
        "  print  (torch.mv(torch.mm(torch.from_numpy(np.linalg.inv(torch.mm(X.t(),X))), X.t()), y))\n",
        "  print  (torch.mv(torch.mm(torch.from_numpy(np.linalg.pinv(torch.mm(X.t(),X))), X.t()), y))\n",
        "  w  = torch.mv(torch.mm(torch.from_numpy(np.linalg.pinv(torch.mm(X.t(),X))), X.t()), y)\n",
        " # return  (torch.inverse(torch.mm(X.t()*X)))* torch.mm((X.t()* y))\n",
        "\n",
        "  print(X.shape)\n",
        "  print(w.shape)\n",
        "  #print(X.dot(w))\n",
        "  print(torch.matmul(X,w))\n",
        "\n",
        "  print(torch.matmul(X,analytic_weights(X,y)))\n",
        "  ## end of function"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.,  2.,  3.,  4., 10.])\n",
            "tensor([[ 1.,  2., 40.],\n",
            "        [ 4.,  5., 55.],\n",
            "        [ 7.,  8., 88.],\n",
            "        [ 7.,  5., 88.],\n",
            "        [ 7., 25., 88.]])\n",
            "tensor([[ 1.,  4.,  7.,  7.,  7.],\n",
            "        [ 2.,  5.,  8.,  5., 25.],\n",
            "        [40., 55., 88., 88., 88.]])\n",
            "tensor([[  164.,   288.,  2108.],\n",
            "        [  288.,   743.,  3699.],\n",
            "        [ 2108.,  3699., 27857.]])\n",
            "tensor([[ 2.3679e-01, -7.6055e-03, -1.6908e-02],\n",
            "        [-7.6055e-03,  4.2153e-03,  1.5796e-05],\n",
            "        [-1.6908e-02,  1.5797e-05,  1.3133e-03]])\n",
            "tensor([ 128.,  306., 1646.])\n",
            "tensor([0.1504, 0.3424, 0.0022])\n",
            "tensor([0.1504, 0.3424, 0.0022])\n",
            "tensor([0.1504, 0.3424, 0.0022])\n",
            "tensor([0.1504, 0.3424, 0.0022])\n",
            "torch.Size([5, 3])\n",
            "torch.Size([3])\n",
            "tensor([0.9249, 2.4369, 3.9893, 2.9622, 9.8096])\n",
            "tensor([0.9249, 2.4369, 3.9893, 2.9622, 9.8096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDrdifvvigHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtXSXktLJbDv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Gradient descent\n",
        "\n",
        "Even in cases where we cannot solve the models analytically,\n",
        "and even when the loss surfaces are high-dimensional and nonconvex,\n",
        "it turns out that we can still make progress.\n",
        "Moreover, when those difficult-to-optimize models are sufficiently superior for the task at hand, figuring out how to train them is well worth the trouble.\n",
        "\n",
        "The key trick behind nearly all of deep learning\n",
        "and that we will repeatedly throughout this book\n",
        "is to reduce the error gradually by iteratively updating the parameters,\n",
        "each step moving the parameters in the direction\n",
        "that incrementally lowers the loss function.\n",
        "This algorithm is called gradient descent.\n",
        "On convex loss surfaces it will eventually converge to a global minimum,\n",
        "and while the same can't be said for nonconvex surfaces,\n",
        "it will at least lead towards a (hopefully good) local minimum.\n",
        "\n",
        "The most naive application of gradient descent consists of taking the derivative of the true loss, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow. We must pass over the entire dataset before making a single update.\n",
        "Thus, we'll often settle for sampling a random mini-batch\n",
        "of examples every time we need to computer the update,\n",
        "a variant called *stochastic gradient descent*.\n",
        "\n",
        "In each iteration, we first randomly and uniformly sample a mini-batch $\\mathcal{B}$ consisting of a fixed number of training data examples.\n",
        "We then compute the derivative (gradient) of the average loss on the mini batch with regard to the model parameters.\n",
        "Finally, the product of this result and a predetermined step size $\\eta > 0$ are used to update the parameters in the direction that lowers the loss.\n",
        "\n",
        "We can express the update mathematically as follows ($\\partial$ denotes the partial derivative):\n",
        "\n",
        "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$$\n",
        "\n",
        "\n",
        "To summarize, steps of the algorithm are the following:\n",
        "(i) we initialize the values of the model parameters, typically at random;\n",
        "(ii) we iterate over the data many times,\n",
        "updating the parameters in each by moving the parameters in the direction of the negative gradient, as calculated on a random minibatch of data.\n",
        "\n",
        "\n",
        "For quadratic losses and linear functions we can write this out explicitly as follows. Note that $\\mathbf{w}$ and $\\mathbf{x}$ are vectors. Here the more elegant vector notation makes the math much more readable than expressing things in terms of coefficients, say $w_1, w_2, \\ldots w_d$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && =\n",
        "w - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\\n",
        "b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  && =\n",
        "b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "In the above equation $|\\mathcal{B}|$ represents the number of samples (batch size) in each mini-batch, $\\eta$ is referred to as ‘learning rate’ and takes a positive number. It should be emphasized that the values of the batch size and learning rate are set somewhat manually and are typically not learned through model training. Therefore, they are referred to as *hyper-parameters*. What we usually call *tuning hyper-parameters* refers to the adjustment of these terms. In the worst case this is performed through repeated trial and error until the appropriate hyper-parameters are found. A better approach is to learn these as parts of model training. This is an advanced topic and we do not cover them here for the sake of simplicity.\n",
        "\n",
        "### Model Prediction\n",
        "\n",
        "After completing the training process, we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$\n",
        "(in general the \"hat\" symbol denotes estimates).\n",
        "Note that the parameters that we learn via gradient descent\n",
        "are not exactly equal to the true minimizers of the loss on the training set,\n",
        "that's because gradient descent converges slowly to a local minimum but does not achieve it exactly.\n",
        "Moreover if the problem has multiple local minimum, we may not necessarily achieve the lowest minimum.\n",
        "Fortunately, for deep neural networks, finding parameters that minimize the loss *on training data* is seldom a significant problem. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called *generalization*. We return to these topics throughout the book.\n",
        "\n",
        "Given the learned linear regression model $\\hat{\\mathbf{w}}^\\top x + \\hat{b}$, we can now estimate the price of any house outside the training data set with area (square feet) as $x_1$ and house age (year) as $x_2$. Here, estimation also referred to as ‘model prediction’ or ‘model inference’.\n",
        "\n",
        "Note that calling this step 'inference' is a misnomer,\n",
        "but has become standard jargon in deep learning.\n",
        "In statistics, 'inference' means estimating parameters\n",
        "and outcomes based on other data.\n",
        "This misuse of terminology in deep learning\n",
        "can be a source of confusion when talking to statisticians.\n",
        "\n",
        "\n",
        "## From Linear Regression to Deep Networks\n",
        "\n",
        "So far we only talked about linear functions. While neural networks cover a much richer family of models, we can begin thinking of the linear model as a neural network by expressing it the language of neural networks. To begin, let's start by rewriting things in a 'layer' notation.\n",
        "\n",
        "### Neural Network Diagram\n",
        "\n",
        "Commonly, deep learning practitioners represent models visually using neural network diagrams. In Figure 3.1, we represent linear regression with a neural network diagram. The diagram shows the connectivity among the inputs and output, but does not depict the weights or biases (which are given implicitly).\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1-8TFMQ8pA1p4fpRTILCFXv_3X5a5RkST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzmFE2UnV68t",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is a single-layer neural network.\n",
        "\n",
        "In the above network, the inputs are $x_1, x_2, \\ldots x_d$.\n",
        "Sometimes the number of inputs are referred to as the feature dimension.\n",
        "For linear regression models, we act upon $d$ inputs and output $1$ value.\n",
        "Because there is just a single computed neuron (node) in the graph,\n",
        "we can think of linear models as neural networks consisting of just a single neuron. Since all inputs are connected to all outputs (in this case it's just one), this layer can also be regarded as an instance of a *fully-connected layer*, also commonly called a *dense layer*.\n",
        "\n",
        "### Biology\n",
        "\n",
        "Neural networks derive their name from their inspirations in neuroscience.\n",
        "Although linear regression predates computation neuroscience,\n",
        "many of the models we subsequently discuss truly owe to neural inspiration.\n",
        "To understand the neural inspiration for artificial neural networks\n",
        "it is worth while considering the basic structure of a neuron.\n",
        "For the purpose of the analogy it is sufficient to consider the *dendrites*\n",
        "(input terminals), the *nucleus* (CPU), the *axon* (output wire),\n",
        "and the *axon terminals* (output terminals)\n",
        "which connect to other neurons via *synapses*.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1S8LylGxrv-AZDmdSf3QFs8zp7OimhNzF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPGmq-SwV68y",
        "colab_type": "text"
      },
      "source": [
        "The real neuron\n",
        "\n",
        "Information $x_i$ arriving from other neurons (or environmental sensors such as the retina) is received in the dendrites. In particular, that information is weighted by *synaptic weights* $w_i$ which determine how to respond to the inputs (e.g. activation or inhibition via $x_i w_i$). All this is aggregated in the nucleus $y = \\sum_i x_i w_i + b$, and this information is then sent for further processing in the axon $y$, typically after some nonlinear processing via $\\sigma(y)$. From there it either reaches its destination (e.g. a muscle) or is fed into another neuron via its dendrites.\n",
        "\n",
        "Brain *structures* vary significantly. Some look (to us) rather arbitrary whereas others have a regular structure. For example, the visual system of many insects is consistent across members of a species. The analysis of such structures has often inspired neuroscientists to propose new architectures, and in some cases, this has been successful. However, much research in artificial neural networks has little to do with any direct inspiration in neuroscience, just as although airplanes are *inspired* by birds, the study of orninthology hasn't been the primary driver of aeronautics innovaton in the last century. Equal amounts of inspiration these days comes from mathematics, statistics, and computer science.\n",
        "\n",
        "### Vectorization for Speed\n",
        "\n",
        "In model training or prediction, we often use vector calculations and process multiple observations at the same time. To illustrate why this matters, consider two methods of adding vectors. We begin by creating two 10000 dimensional ones first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "1"
        },
        "id": "k-fCAZItV68y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from time import time\n",
        "\n",
        "a = torch.ones(10000)\n",
        "b = torch.ones(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiHMziNAV681",
        "colab_type": "text"
      },
      "source": [
        "One way to add vectors is to add them one coordinate at a time using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "id": "FNbWwzH3V682",
        "colab_type": "code",
        "outputId": "4d928700-f82f-4c42-ae06-56756a5fc856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = time()\n",
        "c = torch.zeros(10000)\n",
        "for i in range(10000):\n",
        "    c[i] = a[i] + b[i]\n",
        "time() - start"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13264822959899902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmLcNZe9V684",
        "colab_type": "text"
      },
      "source": [
        "Another way to add vectors is to add the vectors directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "id": "VxA7PJXZV684",
        "colab_type": "code",
        "outputId": "b767c491-8afa-44f8-da16-6a26f8d6f7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = time()\n",
        "d = a + b\n",
        "time() - start"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00015974044799804688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdYr0C-8V686",
        "colab_type": "text"
      },
      "source": [
        "Obviously, the latter is vastly faster than the former. Vectorizing code is a good way of getting order of magnitude speedups. Likewise, as we saw above, it also greatly simplifies the mathematics and with it, it reduces the potential for errors in the notation.\n",
        "\n",
        "## The Normal Distribution and Squared Loss\n",
        "\n",
        "The following is optional and can be skipped but it will greatly help with understanding some of the design choices in building deep learning models. As we saw above, using the squared loss $l(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$ has many nice properties, such as having a particularly simple derivative $\\partial_{\\hat{y}} l(y, \\hat{y}) = (\\hat{y} - y)$. That is, the gradient is given by the difference between estimate and observation. You might reasonably point out that linear regression is a [classical](https://en.wikipedia.org/wiki/Regression_analysis#History) statistical model. Legendre first developed the method of least squares regression in 1805, which was shortly thereafter rediscovered by Gauss in 1809. To understand this a bit better, recall the normal distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
        "\n",
        "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right)$$\n",
        "\n",
        "It can be visualized as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "id": "yjERfLD2V686",
        "colab_type": "code",
        "outputId": "f149cdfe-21cd-4e4d-eaa1-2a6a2f6b3719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.arange(-7, 7, 0.01)\n",
        "# Mean and variance pairs\n",
        "parameters = [(0,1), (0,2), (3,1)]\n",
        "\n",
        "# Display SVG rather than JPG\n",
        "display.set_matplotlib_formats('svg')\n",
        "plt.figure(figsize=(10, 6))\n",
        "for (mu, sigma) in parameters:\n",
        "    p = (1/math.sqrt(2 * math.pi * sigma**2)) * torch.exp(-(0.5/sigma**2) * (x-mu)**2)\n",
        "    plt.plot(x.numpy(), p.numpy(), label='mean ' + str(mu) + ', variance ' + str(sigma))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"357.238125pt\" version=\"1.1\" viewBox=\"0 0 601.665625 357.238125\" width=\"601.665625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 357.238125 \nL 601.665625 357.238125 \nL 601.665625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 333.36 \nL 594.465625 333.36 \nL 594.465625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7556ec106f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.088928\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −6 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(90.717834 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.608261\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(163.237167 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.127594\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(235.756501 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.646927\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(312.465677 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"388.166261\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2 -->\n      <g transform=\"translate(384.985011 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"460.685594\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 4 -->\n      <g transform=\"translate(457.504344 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"533.204927\" xlink:href=\"#m7556ec106f\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(530.023677 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3d2d6c1a41\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"318.534545\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 322.333764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"281.372643\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.05 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 285.171862)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"244.21074\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 248.009959)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"207.048838\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.15 -->\n      <g transform=\"translate(7.2 210.848057)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"169.886935\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.20 -->\n      <g transform=\"translate(7.2 173.686154)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"132.725033\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 136.524252)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"95.563131\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 99.362349)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"58.401228\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.35 -->\n      <g transform=\"translate(7.2 62.200447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3d2d6c1a41\" y=\"21.239326\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.40 -->\n      <g transform=\"translate(7.2 25.038544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p28b37558d7)\" d=\"M 61.829261 318.534545 \nL 171.333454 318.426815 \nL 182.573948 318.181928 \nL 189.825883 317.814451 \nL 195.264837 317.336277 \nL 199.978589 316.705073 \nL 203.967157 315.951735 \nL 207.59312 315.037641 \nL 210.856487 313.980446 \nL 213.757266 312.814206 \nL 216.658037 311.395152 \nL 219.196211 309.912887 \nL 221.734394 308.173759 \nL 224.272568 306.144691 \nL 226.810743 303.790668 \nL 228.986321 301.486636 \nL 231.161907 298.893374 \nL 233.337485 295.98696 \nL 235.513063 292.743486 \nL 237.688641 289.139454 \nL 239.864227 285.15217 \nL 242.402402 279.987561 \nL 244.940576 274.241495 \nL 247.478754 267.887652 \nL 250.016933 260.905421 \nL 252.555107 253.281166 \nL 255.093286 245.009366 \nL 257.994056 234.768494 \nL 260.894831 223.71003 \nL 264.158202 210.345728 \nL 267.784166 194.460587 \nL 271.77273 175.93679 \nL 277.211683 149.470444 \nL 287.364389 99.795829 \nL 290.990354 83.230654 \nL 293.891127 70.869325 \nL 296.429305 60.877893 \nL 298.967481 51.793622 \nL 301.143061 44.822173 \nL 302.956044 39.641635 \nL 304.769027 35.072599 \nL 306.582011 31.148089 \nL 308.032398 28.491923 \nL 309.482784 26.279205 \nL 310.933171 24.520389 \nL 312.020961 23.504314 \nL 313.108751 22.751007 \nL 314.196541 22.262551 \nL 315.284331 22.040273 \nL 316.372121 22.084751 \nL 317.459911 22.395851 \nL 318.547701 22.972753 \nL 319.635491 23.81393 \nL 320.723281 24.917055 \nL 321.811071 26.279205 \nL 323.261457 28.491923 \nL 324.711844 31.148089 \nL 326.16223 34.235165 \nL 327.975214 38.677791 \nL 329.788197 43.73885 \nL 331.963777 50.577177 \nL 334.139357 58.184293 \nL 336.677533 67.92957 \nL 339.578308 80.056373 \nL 342.841677 94.717736 \nL 346.830241 113.684571 \nL 354.807369 153.049997 \nL 360.608914 181.08212 \nL 364.597478 199.331564 \nL 368.223446 214.904453 \nL 371.486813 227.950512 \nL 374.387588 238.70626 \nL 377.288362 248.633695 \nL 380.189133 257.717024 \nL 382.727312 264.973193 \nL 385.265486 271.594181 \nL 387.803664 277.597805 \nL 390.341839 283.008079 \nL 392.880021 287.854035 \nL 395.418196 292.16848 \nL 397.95637 295.98696 \nL 400.494544 299.346681 \nL 403.032727 302.285604 \nL 405.570901 304.841641 \nL 408.109076 307.052 \nL 410.64725 308.952624 \nL 413.185433 310.577739 \nL 415.723607 311.959523 \nL 418.624377 313.27903 \nL 421.525157 314.360534 \nL 424.788524 315.338136 \nL 428.414487 316.180752 \nL 432.403055 316.872785 \nL 437.116807 317.450361 \nL 442.555761 317.885934 \nL 449.445099 318.206949 \nL 459.235209 318.417912 \nL 475.914657 318.517572 \nL 539.006468 318.534544 \nL 569.101989 318.534545 \nL 569.101989 318.534545 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p28b37558d7)\" d=\"M 61.829261 318.21024 \nL 77.783517 317.850868 \nL 90.111809 317.357649 \nL 100.264506 316.733296 \nL 108.966835 315.980494 \nL 116.581366 315.108459 \nL 123.470688 314.10773 \nL 129.997439 312.938105 \nL 136.161585 311.602753 \nL 141.963126 310.111948 \nL 147.402079 308.482955 \nL 152.478428 306.739485 \nL 157.554776 304.761265 \nL 162.268537 302.698034 \nL 166.982298 300.402501 \nL 171.69605 297.861759 \nL 176.409811 295.064363 \nL 181.123563 292.000778 \nL 185.837324 288.663764 \nL 190.551076 285.048854 \nL 195.264837 281.154737 \nL 199.978589 276.98369 \nL 205.054946 272.189325 \nL 210.131295 267.094442 \nL 215.570248 261.324431 \nL 221.371798 254.850708 \nL 227.898532 247.235668 \nL 235.875659 237.576533 \nL 260.169639 207.890498 \nL 265.971184 201.283397 \nL 271.047537 195.82524 \nL 275.761293 191.090076 \nL 280.112454 187.051128 \nL 284.101017 183.663628 \nL 287.726985 180.870231 \nL 291.35295 178.369836 \nL 294.616321 176.384855 \nL 297.879691 174.663385 \nL 301.143061 173.215635 \nL 304.043834 172.165571 \nL 306.944608 171.343598 \nL 309.845381 170.753649 \nL 312.746154 170.398548 \nL 315.646927 170.28 \nL 318.547701 170.398548 \nL 321.448474 170.753649 \nL 324.349247 171.343598 \nL 327.250021 172.165571 \nL 330.150794 173.215635 \nL 333.051567 174.48883 \nL 336.314937 176.180365 \nL 339.578308 178.136617 \nL 342.841677 180.346112 \nL 346.467645 183.082428 \nL 350.09361 186.096066 \nL 354.082172 189.706219 \nL 358.433332 193.962938 \nL 363.147089 198.902802 \nL 368.223446 204.543699 \nL 374.387588 211.742786 \nL 382.364715 221.435579 \nL 404.483112 248.52594 \nL 411.372451 256.497566 \nL 417.536589 263.281785 \nL 422.975542 268.947452 \nL 428.05189 273.936913 \nL 433.128248 278.620403 \nL 438.204605 282.986662 \nL 442.918357 286.751967 \nL 447.632118 290.238257 \nL 452.34587 293.448339 \nL 457.059631 296.38802 \nL 461.773391 299.065653 \nL 466.487135 301.491694 \nL 471.200896 303.678332 \nL 476.277244 305.780885 \nL 481.35361 307.639391 \nL 486.792546 309.381095 \nL 492.2315 310.887507 \nL 498.033058 312.261148 \nL 504.197187 313.486854 \nL 510.723938 314.556142 \nL 517.975864 315.509637 \nL 525.953001 316.322472 \nL 534.655312 316.983807 \nL 544.808027 317.528396 \nL 557.136301 317.955005 \nL 569.101989 318.204519 \nL 569.101989 318.204519 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p28b37558d7)\" d=\"M 61.829261 318.534545 \nL 280.112454 318.426815 \nL 291.35295 318.181928 \nL 298.604884 317.814451 \nL 304.043834 317.336277 \nL 308.757591 316.705073 \nL 312.746154 315.951735 \nL 316.372121 315.037641 \nL 319.635491 313.980446 \nL 322.536264 312.814206 \nL 325.437038 311.395152 \nL 327.975214 309.912887 \nL 330.513391 308.173759 \nL 333.051567 306.144691 \nL 335.589745 303.790668 \nL 337.765325 301.486627 \nL 339.940905 298.893374 \nL 342.116485 295.98696 \nL 344.292065 292.743486 \nL 346.467645 289.139454 \nL 348.643225 285.15217 \nL 351.181401 279.987561 \nL 353.719576 274.241495 \nL 356.257754 267.887652 \nL 358.795933 260.905421 \nL 361.334107 253.281166 \nL 363.872286 245.009366 \nL 366.773056 234.768494 \nL 369.673831 223.71003 \nL 372.937202 210.345728 \nL 376.563165 194.460587 \nL 380.551729 175.93679 \nL 385.990683 149.470444 \nL 396.143388 99.795829 \nL 399.769352 83.230654 \nL 402.670131 70.869325 \nL 405.208305 60.877893 \nL 407.746479 51.793622 \nL 409.922057 44.822173 \nL 411.735047 39.641613 \nL 413.548029 35.072599 \nL 415.361011 31.148089 \nL 416.811396 28.491923 \nL 418.261781 26.279205 \nL 419.712166 24.520389 \nL 420.799964 23.504314 \nL 421.887753 22.751007 \nL 422.975542 22.262551 \nL 424.063331 22.040273 \nL 425.15112 22.084751 \nL 426.238909 22.395851 \nL 427.326698 22.972753 \nL 428.414487 23.81393 \nL 429.502284 24.917055 \nL 430.590073 26.279205 \nL 432.040459 28.491923 \nL 433.490844 31.148089 \nL 434.941229 34.235165 \nL 436.754211 38.677791 \nL 438.567201 43.73885 \nL 440.742779 50.577177 \nL 442.918357 58.184293 \nL 445.456531 67.92957 \nL 448.35731 80.056373 \nL 451.620677 94.717736 \nL 455.609237 113.684549 \nL 463.586364 153.049986 \nL 469.387906 181.082086 \nL 473.376474 199.331531 \nL 477.002437 214.90442 \nL 480.265812 227.950512 \nL 483.166583 238.706249 \nL 486.067354 248.633668 \nL 488.968141 257.717052 \nL 491.506307 264.973182 \nL 494.04449 271.594187 \nL 496.582655 277.597788 \nL 499.120838 283.008079 \nL 501.659021 287.854035 \nL 504.197187 292.168469 \nL 506.73537 295.98696 \nL 509.273553 299.346692 \nL 511.811718 302.285597 \nL 514.349901 304.841641 \nL 516.888084 307.052008 \nL 519.42625 308.952624 \nL 521.964432 310.577739 \nL 524.502615 311.959528 \nL 527.403386 313.279032 \nL 530.304156 314.360534 \nL 533.567532 315.338139 \nL 537.193495 316.180754 \nL 541.182046 316.872783 \nL 545.895807 317.450361 \nL 551.33476 317.885934 \nL 558.224099 318.206949 \nL 568.014208 318.417912 \nL 569.101989 318.431023 \nL 569.101989 318.431023 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 333.36 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 333.36 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 333.36 \nL 594.465625 333.36 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 594.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 458.615625 59.234375 \nL 587.465625 59.234375 \nQ 589.465625 59.234375 589.465625 57.234375 \nL 589.465625 14.2 \nQ 589.465625 12.2 587.465625 12.2 \nL 458.615625 12.2 \nQ 456.615625 12.2 456.615625 14.2 \nL 456.615625 57.234375 \nQ 456.615625 59.234375 458.615625 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 460.615625 20.298437 \nL 480.615625 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- mean 0, variance 1 -->\n     <defs>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     </defs>\n     <g transform=\"translate(488.615625 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 460.615625 34.976562 \nL 480.615625 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_18\">\n     <!-- mean 0, variance 2 -->\n     <g transform=\"translate(488.615625 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 460.615625 49.654687 \nL 480.615625 49.654687 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_19\">\n     <!-- mean 3, variance 1 -->\n     <g transform=\"translate(488.615625 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.935547\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"220.214844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"315.380859\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"379.003906\" xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"410.791016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"442.578125\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"501.757812\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"563.037109\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"604.150391\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"631.933594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"693.212891\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"756.591797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"811.572266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"873.095703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"904.882812\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p28b37558d7\">\n   <rect height=\"326.16\" width=\"558\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPbFM4OLV688",
        "colab_type": "text"
      },
      "source": [
        "As can be seen in the figure above, changing the mean shifts the function, increasing the variance makes it more spread-out with a lower peak. The key assumption in linear regression with least mean squares loss is that the observations actually arise from noisy observations, where noise is added to the data, e.g. as part of the observations process.\n",
        "\n",
        "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
        "\n",
        "This allows us to write out the *likelihood* of seeing a particular $y$ for a given $\\mathbf{x}$ via\n",
        "\n",
        "$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
        "\n",
        "A good way of finding the most likely values of $b$ and $\\mathbf{w}$ is to maximize the *likelihood* of the entire dataset\n",
        "\n",
        "$$p(Y|X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)})$$\n",
        "\n",
        "The notion of maximizing the likelihood of the data subject to the parameters is well known as the *Maximum Likelihood Principle* and its estimators are usually called *Maximum Likelihood Estimators* (MLE). Unfortunately, maximizing the product of many exponential functions is pretty awkward, both in terms of implementation and in terms of writing it out on paper. Instead, a much better way is to minimize the *Negative Log-Likelihood* $-\\log P(Y|X)$. In the above case this works out to be\n",
        "\n",
        "$$-\\log P(Y|X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2$$\n",
        "\n",
        "A closer inspection reveals that for the purpose of minimizing $-\\log P(Y|X)$ we can skip the first term since it doesn't depend on $\\mathbf{w}, b$ or even the data. The second term is identical to the objective we initially introduced, but for the multiplicative constant $\\frac{1}{\\sigma^2}$. Again, this can be skipped if we just want to get the most likely solution. It follows that maximum likelihood in a linear model with additive Gaussian noise is equivalent to linear regression with squared loss.\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself.\n",
        "* Vectorizing makes everything better (mostly math) and faster (mostly code).\n",
        "* Minimizing an objective function and performing maximum likelihood can mean the same thing.\n",
        "* Linear models are neural networks, too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmqS9JflV689",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression Implementation from Scratch\n",
        "\n",
        "\n",
        "Now that you have some background on the *ideas* behind linear regression,\n",
        "we are ready to step through a hands-on implementation.\n",
        "In this section, and similar ones that follow,\n",
        "we are going to implement all parts of linear regression:\n",
        "the data pipeline, the model, the loss function,\n",
        "and the gradient descent optimizer, from scratch.\n",
        "Not surprisingly, today's deep learning frameworks\n",
        "can automate nearly all of this work,\n",
        "but if you never learn to implement things from scratch,\n",
        "then you may never truly understand how the model works.\n",
        "Moreover, when it comes time to customize models,\n",
        "defining our own layers, loss functions, etc.,\n",
        "knowing how things work under the hood will come in handy.\n",
        "Thus, we start off describing how to implement linear regression\n",
        "relying only on the primitives in the `torch.Tensor` and `autograd` packages.\n",
        "In the section immediately following, we will present the compact implementation, using all of torch's bells and whistles,\n",
        "but this is where we dive into the details.\n",
        "\n",
        "To start off, we import the packages required to run this section's experiments: we'll be using `matplotlib` for plotting, setting it to embed in the GUI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "1"
        },
        "id": "xBf0gNrJV689",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuwGqe5oV68_",
        "colab_type": "text"
      },
      "source": [
        "## Generating Data Sets\n",
        "\n",
        "For this demonstration, we will construct a simple artificial dataset\n",
        "so that we can easily visualize the data\n",
        "and compare the true pattern to the learned parameters.\n",
        "We will set the number of examples in our training set to be 1000\n",
        "and the number of features (or covariates) to 2.\n",
        "Thus our synthetic dataset will be an object\n",
        "$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
        "In this example, we will synthesize our data by sampling\n",
        "each data point $\\mathbf{x}_i$ from a Gaussian distribution.\n",
        "\n",
        "Moreover, to make sure that our algorithm works,\n",
        "we will assume that the linearity assumption holds\n",
        "with true underlying parameters $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$.\n",
        "Thus our synthetic labels will be given according to the\n",
        "following linear model which includes a noise term $\\epsilon$ to account for\n",
        "measurement errors on the features and labels:\n",
        "\n",
        "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon$$\n",
        "\n",
        "Following standard assumptions, we choose a noise term $\\epsilon$\n",
        "that obeys a normal distribution with mean of $0$,\n",
        "and in this example, we'll set its standard deviation to $0.01$.\n",
        "The following code generates our synthetic dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "id": "z2lxbXo7V68_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs = 2\n",
        "num_examples = 1000\n",
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features = torch.zeros(size=(num_examples, num_inputs)).normal_()\n",
        "labels = torch.matmul(features, true_w) + true_b\n",
        "labels += torch.zeros(size=labels.shape).normal_(std=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6xI9qZAV69B",
        "colab_type": "text"
      },
      "source": [
        "Note that each row in `features` consists of a 2-dimensional data point and that each row in `labels` consists of a 1-dimensional target value (a scalar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "id": "momdVY6SV69B",
        "colab_type": "code",
        "outputId": "f64235a8-720b-45bb-c90b-36602953e9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "features[0], labels[0]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.9421,  0.5760]), tensor(0.3502))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s43X5w_wV69C",
        "colab_type": "text"
      },
      "source": [
        "By generating a scatter plot using the second `features[:, 1]` and `labels`, we can clearly observe the linear correlation between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "id": "C4GDw7iYV69D",
        "colab_type": "code",
        "outputId": "81d21887-3ea1-4006-a11f-18ebcbeb855d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "def use_svg_display():\n",
        "    # Display in vector graphics\n",
        "    display.set_matplotlib_formats('svg')\n",
        "\n",
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    use_svg_display()\n",
        "    # Set the size of the graph to be plotted\n",
        "    plt.rcParams['figure.figsize'] = figsize\n",
        "\n",
        "set_figsize()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"357.238125pt\" version=\"1.1\" viewBox=\"0 0 594.142187 357.238125\" width=\"594.142187pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 357.238125 \nL 594.142187 357.238125 \nL 594.142187 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 28.942188 333.36 \nL 586.942188 333.36 \nL 586.942188 7.2 \nL 28.942188 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 0.5 \nC 0.132602 0.5 0.25979 0.447317 0.353553 0.353553 \nC 0.447317 0.25979 0.5 0.132602 0.5 0 \nC 0.5 -0.132602 0.447317 -0.25979 0.353553 -0.353553 \nC 0.25979 -0.447317 0.132602 -0.5 0 -0.5 \nC -0.132602 -0.5 -0.25979 -0.447317 -0.353553 -0.353553 \nC -0.447317 -0.25979 -0.5 -0.132602 -0.5 0 \nC -0.5 0.132602 -0.447317 0.25979 -0.353553 0.353553 \nC -0.25979 0.447317 -0.132602 0.5 0 0.5 \nz\n\" id=\"m371f10bfd5\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p0016759381)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"339.488522\" xlink:href=\"#m371f10bfd5\" y=\"223.416823\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"229.485777\" xlink:href=\"#m371f10bfd5\" y=\"136.244866\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"243.338624\" xlink:href=\"#m371f10bfd5\" y=\"144.7422\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.188592\" xlink:href=\"#m371f10bfd5\" y=\"131.494815\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.570945\" xlink:href=\"#m371f10bfd5\" y=\"202.665289\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.976076\" xlink:href=\"#m371f10bfd5\" y=\"147.996746\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.835597\" xlink:href=\"#m371f10bfd5\" y=\"123.409193\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"245.023557\" xlink:href=\"#m371f10bfd5\" y=\"169.53765\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"254.634721\" xlink:href=\"#m371f10bfd5\" y=\"185.00728\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"309.025198\" xlink:href=\"#m371f10bfd5\" y=\"206.914756\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"400.934666\" xlink:href=\"#m371f10bfd5\" y=\"302.217392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.34574\" xlink:href=\"#m371f10bfd5\" y=\"153.813955\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"275.623072\" xlink:href=\"#m371f10bfd5\" y=\"169.586315\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"198.644994\" xlink:href=\"#m371f10bfd5\" y=\"167.799539\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.361526\" xlink:href=\"#m371f10bfd5\" y=\"141.210032\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.535639\" xlink:href=\"#m371f10bfd5\" y=\"94.972929\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"216.222441\" xlink:href=\"#m371f10bfd5\" y=\"94.293013\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.697387\" xlink:href=\"#m371f10bfd5\" y=\"109.236082\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"195.332119\" xlink:href=\"#m371f10bfd5\" y=\"118.221212\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.196372\" xlink:href=\"#m371f10bfd5\" y=\"218.45696\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.389199\" xlink:href=\"#m371f10bfd5\" y=\"174.417757\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.274689\" xlink:href=\"#m371f10bfd5\" y=\"118.341295\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"307.68722\" xlink:href=\"#m371f10bfd5\" y=\"178.946848\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.826583\" xlink:href=\"#m371f10bfd5\" y=\"197.208193\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"421.027415\" xlink:href=\"#m371f10bfd5\" y=\"251.133055\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.983266\" xlink:href=\"#m371f10bfd5\" y=\"159.933301\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"425.572011\" xlink:href=\"#m371f10bfd5\" y=\"228.569667\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.050218\" xlink:href=\"#m371f10bfd5\" y=\"125.604109\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.723878\" xlink:href=\"#m371f10bfd5\" y=\"145.922934\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.528603\" xlink:href=\"#m371f10bfd5\" y=\"125.142579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.89497\" xlink:href=\"#m371f10bfd5\" y=\"200.308398\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.256577\" xlink:href=\"#m371f10bfd5\" y=\"103.971495\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.301155\" xlink:href=\"#m371f10bfd5\" y=\"123.702312\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.672843\" xlink:href=\"#m371f10bfd5\" y=\"110.330532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.592813\" xlink:href=\"#m371f10bfd5\" y=\"95.503212\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.584982\" xlink:href=\"#m371f10bfd5\" y=\"134.073112\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"254.24218\" xlink:href=\"#m371f10bfd5\" y=\"139.2197\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.296258\" xlink:href=\"#m371f10bfd5\" y=\"90.526183\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.353945\" xlink:href=\"#m371f10bfd5\" y=\"185.895051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"183.801529\" xlink:href=\"#m371f10bfd5\" y=\"134.54468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.905148\" xlink:href=\"#m371f10bfd5\" y=\"189.452533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"279.277243\" xlink:href=\"#m371f10bfd5\" y=\"165.777532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"210.668971\" xlink:href=\"#m371f10bfd5\" y=\"188.083825\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.751178\" xlink:href=\"#m371f10bfd5\" y=\"183.66575\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"256.338459\" xlink:href=\"#m371f10bfd5\" y=\"98.704638\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"277.744584\" xlink:href=\"#m371f10bfd5\" y=\"175.482442\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"265.468238\" xlink:href=\"#m371f10bfd5\" y=\"170.824532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.539343\" xlink:href=\"#m371f10bfd5\" y=\"138.362949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.853362\" xlink:href=\"#m371f10bfd5\" y=\"200.452787\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.152834\" xlink:href=\"#m371f10bfd5\" y=\"180.232394\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"421.071127\" xlink:href=\"#m371f10bfd5\" y=\"172.510262\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.318867\" xlink:href=\"#m371f10bfd5\" y=\"112.156721\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.5937\" xlink:href=\"#m371f10bfd5\" y=\"99.063627\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.880061\" xlink:href=\"#m371f10bfd5\" y=\"90.55458\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"247.566327\" xlink:href=\"#m371f10bfd5\" y=\"83.19787\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.729699\" xlink:href=\"#m371f10bfd5\" y=\"173.170414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.093622\" xlink:href=\"#m371f10bfd5\" y=\"179.807591\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.019131\" xlink:href=\"#m371f10bfd5\" y=\"146.117551\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"386.714548\" xlink:href=\"#m371f10bfd5\" y=\"233.241856\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"447.306088\" xlink:href=\"#m371f10bfd5\" y=\"293.193768\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"264.661636\" xlink:href=\"#m371f10bfd5\" y=\"153.435262\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.537824\" xlink:href=\"#m371f10bfd5\" y=\"158.534812\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.162714\" xlink:href=\"#m371f10bfd5\" y=\"205.379155\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"262.026811\" xlink:href=\"#m371f10bfd5\" y=\"179.766544\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"326.065948\" xlink:href=\"#m371f10bfd5\" y=\"199.803231\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.626276\" xlink:href=\"#m371f10bfd5\" y=\"170.360601\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.633839\" xlink:href=\"#m371f10bfd5\" y=\"203.233303\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"201.796492\" xlink:href=\"#m371f10bfd5\" y=\"120.400578\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"369.073711\" xlink:href=\"#m371f10bfd5\" y=\"236.073899\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.483916\" xlink:href=\"#m371f10bfd5\" y=\"82.857552\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"409.840859\" xlink:href=\"#m371f10bfd5\" y=\"215.291348\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"485.714478\" xlink:href=\"#m371f10bfd5\" y=\"267.631531\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"202.142433\" xlink:href=\"#m371f10bfd5\" y=\"91.100148\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.445936\" xlink:href=\"#m371f10bfd5\" y=\"230.692666\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.792071\" xlink:href=\"#m371f10bfd5\" y=\"227.705554\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.71075\" xlink:href=\"#m371f10bfd5\" y=\"189.151902\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"171.143123\" xlink:href=\"#m371f10bfd5\" y=\"125.435505\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"352.727944\" xlink:href=\"#m371f10bfd5\" y=\"210.079919\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"136.849658\" xlink:href=\"#m371f10bfd5\" y=\"108.512268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.141779\" xlink:href=\"#m371f10bfd5\" y=\"121.653799\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.349391\" xlink:href=\"#m371f10bfd5\" y=\"151.34987\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"295.144228\" xlink:href=\"#m371f10bfd5\" y=\"158.097842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.580967\" xlink:href=\"#m371f10bfd5\" y=\"194.758494\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"355.801403\" xlink:href=\"#m371f10bfd5\" y=\"219.102297\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"252.070472\" xlink:href=\"#m371f10bfd5\" y=\"143.635941\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.2548\" xlink:href=\"#m371f10bfd5\" y=\"210.613883\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.632638\" xlink:href=\"#m371f10bfd5\" y=\"226.444221\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.42911\" xlink:href=\"#m371f10bfd5\" y=\"100.039159\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"418.836845\" xlink:href=\"#m371f10bfd5\" y=\"272.778733\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"258.873309\" xlink:href=\"#m371f10bfd5\" y=\"153.188906\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"431.961658\" xlink:href=\"#m371f10bfd5\" y=\"197.856893\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.437189\" xlink:href=\"#m371f10bfd5\" y=\"119.634871\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"262.065231\" xlink:href=\"#m371f10bfd5\" y=\"165.122075\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.959873\" xlink:href=\"#m371f10bfd5\" y=\"109.628079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.717705\" xlink:href=\"#m371f10bfd5\" y=\"82.035015\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.885132\" xlink:href=\"#m371f10bfd5\" y=\"155.901984\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"294.81844\" xlink:href=\"#m371f10bfd5\" y=\"163.891062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"434.65263\" xlink:href=\"#m371f10bfd5\" y=\"251.803647\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.893196\" xlink:href=\"#m371f10bfd5\" y=\"226.51706\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"370.671838\" xlink:href=\"#m371f10bfd5\" y=\"228.717125\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.076325\" xlink:href=\"#m371f10bfd5\" y=\"120.845058\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.255612\" xlink:href=\"#m371f10bfd5\" y=\"149.270883\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.841028\" xlink:href=\"#m371f10bfd5\" y=\"135.997802\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.213473\" xlink:href=\"#m371f10bfd5\" y=\"174.294\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"341.651169\" xlink:href=\"#m371f10bfd5\" y=\"201.003134\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"279.333311\" xlink:href=\"#m371f10bfd5\" y=\"163.679508\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"405.123765\" xlink:href=\"#m371f10bfd5\" y=\"220.033153\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"203.180406\" xlink:href=\"#m371f10bfd5\" y=\"135.909267\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"456.5713\" xlink:href=\"#m371f10bfd5\" y=\"247.337549\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"241.728348\" xlink:href=\"#m371f10bfd5\" y=\"159.489001\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"249.487306\" xlink:href=\"#m371f10bfd5\" y=\"139.025712\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.49314\" xlink:href=\"#m371f10bfd5\" y=\"124.49034\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"322.497177\" xlink:href=\"#m371f10bfd5\" y=\"190.168492\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.106853\" xlink:href=\"#m371f10bfd5\" y=\"178.14994\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.097294\" xlink:href=\"#m371f10bfd5\" y=\"211.97064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.335476\" xlink:href=\"#m371f10bfd5\" y=\"87.166239\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.848323\" xlink:href=\"#m371f10bfd5\" y=\"164.219187\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"297.552279\" xlink:href=\"#m371f10bfd5\" y=\"200.865016\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.744264\" xlink:href=\"#m371f10bfd5\" y=\"143.471227\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"351.467081\" xlink:href=\"#m371f10bfd5\" y=\"189.977818\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.60412\" xlink:href=\"#m371f10bfd5\" y=\"173.804127\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"234.755206\" xlink:href=\"#m371f10bfd5\" y=\"126.877802\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"385.012798\" xlink:href=\"#m371f10bfd5\" y=\"202.635345\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.691794\" xlink:href=\"#m371f10bfd5\" y=\"129.063058\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.249426\" xlink:href=\"#m371f10bfd5\" y=\"215.465175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.384181\" xlink:href=\"#m371f10bfd5\" y=\"228.411229\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"303.992895\" xlink:href=\"#m371f10bfd5\" y=\"195.909276\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.670335\" xlink:href=\"#m371f10bfd5\" y=\"187.365526\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.014475\" xlink:href=\"#m371f10bfd5\" y=\"174.911154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.512081\" xlink:href=\"#m371f10bfd5\" y=\"204.942149\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.08798\" xlink:href=\"#m371f10bfd5\" y=\"143.54698\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"256.036609\" xlink:href=\"#m371f10bfd5\" y=\"174.87416\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"237.675237\" xlink:href=\"#m371f10bfd5\" y=\"169.99577\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"307.386939\" xlink:href=\"#m371f10bfd5\" y=\"157.440494\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"361.373307\" xlink:href=\"#m371f10bfd5\" y=\"185.866267\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"189.291003\" xlink:href=\"#m371f10bfd5\" y=\"148.000036\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.236735\" xlink:href=\"#m371f10bfd5\" y=\"165.500792\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"493.42641\" xlink:href=\"#m371f10bfd5\" y=\"262.053992\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.653624\" xlink:href=\"#m371f10bfd5\" y=\"132.50432\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"265.167768\" xlink:href=\"#m371f10bfd5\" y=\"161.984401\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.71163\" xlink:href=\"#m371f10bfd5\" y=\"181.059779\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"54.305824\" xlink:href=\"#m371f10bfd5\" y=\"22.025455\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.634096\" xlink:href=\"#m371f10bfd5\" y=\"151.21925\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"191.161427\" xlink:href=\"#m371f10bfd5\" y=\"105.581579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.245768\" xlink:href=\"#m371f10bfd5\" y=\"214.269149\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.340693\" xlink:href=\"#m371f10bfd5\" y=\"212.414143\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.915683\" xlink:href=\"#m371f10bfd5\" y=\"143.2429\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"238.219833\" xlink:href=\"#m371f10bfd5\" y=\"144.42442\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"416.961\" xlink:href=\"#m371f10bfd5\" y=\"260.567203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.272322\" xlink:href=\"#m371f10bfd5\" y=\"111.601187\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"424.479848\" xlink:href=\"#m371f10bfd5\" y=\"237.327045\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"279.6951\" xlink:href=\"#m371f10bfd5\" y=\"190.420587\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"400.571351\" xlink:href=\"#m371f10bfd5\" y=\"193.90591\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"248.311967\" xlink:href=\"#m371f10bfd5\" y=\"136.288524\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.43626\" xlink:href=\"#m371f10bfd5\" y=\"129.541068\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"126.438071\" xlink:href=\"#m371f10bfd5\" y=\"39.128114\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"61.105318\" xlink:href=\"#m371f10bfd5\" y=\"59.405136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.476878\" xlink:href=\"#m371f10bfd5\" y=\"159.039658\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.446704\" xlink:href=\"#m371f10bfd5\" y=\"113.246536\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"388.57863\" xlink:href=\"#m371f10bfd5\" y=\"210.39628\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.655937\" xlink:href=\"#m371f10bfd5\" y=\"221.811459\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.435243\" xlink:href=\"#m371f10bfd5\" y=\"93.74921\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"373.595934\" xlink:href=\"#m371f10bfd5\" y=\"205.194191\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"306.984197\" xlink:href=\"#m371f10bfd5\" y=\"180.834186\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.379032\" xlink:href=\"#m371f10bfd5\" y=\"196.468947\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.197615\" xlink:href=\"#m371f10bfd5\" y=\"110.592348\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"210.789226\" xlink:href=\"#m371f10bfd5\" y=\"101.744556\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.916906\" xlink:href=\"#m371f10bfd5\" y=\"217.713044\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"221.331969\" xlink:href=\"#m371f10bfd5\" y=\"153.256668\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"371.018823\" xlink:href=\"#m371f10bfd5\" y=\"233.496274\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.70815\" xlink:href=\"#m371f10bfd5\" y=\"176.867284\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"191.755738\" xlink:href=\"#m371f10bfd5\" y=\"128.484284\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.412067\" xlink:href=\"#m371f10bfd5\" y=\"146.919334\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.072416\" xlink:href=\"#m371f10bfd5\" y=\"221.616316\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"175.008845\" xlink:href=\"#m371f10bfd5\" y=\"85.017923\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.808119\" xlink:href=\"#m371f10bfd5\" y=\"171.709842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"458.751121\" xlink:href=\"#m371f10bfd5\" y=\"281.392281\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.635549\" xlink:href=\"#m371f10bfd5\" y=\"137.929179\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"449.689823\" xlink:href=\"#m371f10bfd5\" y=\"217.170695\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.045799\" xlink:href=\"#m371f10bfd5\" y=\"88.886081\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"320.410858\" xlink:href=\"#m371f10bfd5\" y=\"221.959619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.63571\" xlink:href=\"#m371f10bfd5\" y=\"209.786394\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"383.260882\" xlink:href=\"#m371f10bfd5\" y=\"232.543024\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.82874\" xlink:href=\"#m371f10bfd5\" y=\"121.030208\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"356.313689\" xlink:href=\"#m371f10bfd5\" y=\"175.993513\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.927485\" xlink:href=\"#m371f10bfd5\" y=\"194.879676\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.456758\" xlink:href=\"#m371f10bfd5\" y=\"160.660212\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"463.95568\" xlink:href=\"#m371f10bfd5\" y=\"229.782871\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"180.637625\" xlink:href=\"#m371f10bfd5\" y=\"74.207457\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.109255\" xlink:href=\"#m371f10bfd5\" y=\"158.855859\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"356.239906\" xlink:href=\"#m371f10bfd5\" y=\"201.419725\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.223233\" xlink:href=\"#m371f10bfd5\" y=\"142.38149\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"384.982767\" xlink:href=\"#m371f10bfd5\" y=\"218.224671\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.155486\" xlink:href=\"#m371f10bfd5\" y=\"223.88985\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.336053\" xlink:href=\"#m371f10bfd5\" y=\"145.353249\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.194571\" xlink:href=\"#m371f10bfd5\" y=\"103.394312\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"157.453801\" xlink:href=\"#m371f10bfd5\" y=\"95.822338\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"396.694035\" xlink:href=\"#m371f10bfd5\" y=\"206.9146\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.003784\" xlink:href=\"#m371f10bfd5\" y=\"29.910827\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"266.706271\" xlink:href=\"#m371f10bfd5\" y=\"148.409683\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"199.098353\" xlink:href=\"#m371f10bfd5\" y=\"141.533024\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"364.977737\" xlink:href=\"#m371f10bfd5\" y=\"226.908993\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.294383\" xlink:href=\"#m371f10bfd5\" y=\"206.110838\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.560118\" xlink:href=\"#m371f10bfd5\" y=\"177.722858\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.944363\" xlink:href=\"#m371f10bfd5\" y=\"140.976553\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.73104\" xlink:href=\"#m371f10bfd5\" y=\"163.034869\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"290.7959\" xlink:href=\"#m371f10bfd5\" y=\"153.795705\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.829662\" xlink:href=\"#m371f10bfd5\" y=\"164.982564\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"236.279012\" xlink:href=\"#m371f10bfd5\" y=\"152.130333\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.186791\" xlink:href=\"#m371f10bfd5\" y=\"197.797425\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"218.607457\" xlink:href=\"#m371f10bfd5\" y=\"126.795145\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"309.751063\" xlink:href=\"#m371f10bfd5\" y=\"216.20937\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"344.660892\" xlink:href=\"#m371f10bfd5\" y=\"275.121061\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"252.330846\" xlink:href=\"#m371f10bfd5\" y=\"160.671163\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.457238\" xlink:href=\"#m371f10bfd5\" y=\"148.250931\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.979476\" xlink:href=\"#m371f10bfd5\" y=\"241.499954\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"420.999673\" xlink:href=\"#m371f10bfd5\" y=\"278.91258\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.37073\" xlink:href=\"#m371f10bfd5\" y=\"143.587504\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.981869\" xlink:href=\"#m371f10bfd5\" y=\"164.833784\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"379.259507\" xlink:href=\"#m371f10bfd5\" y=\"253.506817\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"393.237669\" xlink:href=\"#m371f10bfd5\" y=\"219.316583\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"180.569463\" xlink:href=\"#m371f10bfd5\" y=\"124.412882\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.418141\" xlink:href=\"#m371f10bfd5\" y=\"196.389928\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"346.89815\" xlink:href=\"#m371f10bfd5\" y=\"255.659053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.905468\" xlink:href=\"#m371f10bfd5\" y=\"146.588303\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.375458\" xlink:href=\"#m371f10bfd5\" y=\"196.7351\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.699893\" xlink:href=\"#m371f10bfd5\" y=\"113.453491\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"404.045443\" xlink:href=\"#m371f10bfd5\" y=\"198.744986\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"219.810275\" xlink:href=\"#m371f10bfd5\" y=\"95.366066\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"320.394698\" xlink:href=\"#m371f10bfd5\" y=\"209.677415\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.061863\" xlink:href=\"#m371f10bfd5\" y=\"58.69118\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"384.934116\" xlink:href=\"#m371f10bfd5\" y=\"200.505483\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"191.15684\" xlink:href=\"#m371f10bfd5\" y=\"117.117324\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"311.071905\" xlink:href=\"#m371f10bfd5\" y=\"198.310387\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"402.468645\" xlink:href=\"#m371f10bfd5\" y=\"241.654591\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"255.132381\" xlink:href=\"#m371f10bfd5\" y=\"150.139725\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"382.36114\" xlink:href=\"#m371f10bfd5\" y=\"196.053434\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"221.397351\" xlink:href=\"#m371f10bfd5\" y=\"163.697032\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"412.100706\" xlink:href=\"#m371f10bfd5\" y=\"240.793082\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"423.238733\" xlink:href=\"#m371f10bfd5\" y=\"287.727232\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.455277\" xlink:href=\"#m371f10bfd5\" y=\"238.363696\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.899279\" xlink:href=\"#m371f10bfd5\" y=\"179.056308\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"247.545023\" xlink:href=\"#m371f10bfd5\" y=\"167.467356\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.739741\" xlink:href=\"#m371f10bfd5\" y=\"202.213852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"259.268414\" xlink:href=\"#m371f10bfd5\" y=\"159.750468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.680657\" xlink:href=\"#m371f10bfd5\" y=\"154.81753\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.176843\" xlink:href=\"#m371f10bfd5\" y=\"165.926487\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"421.776539\" xlink:href=\"#m371f10bfd5\" y=\"287.939182\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.657138\" xlink:href=\"#m371f10bfd5\" y=\"174.391017\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"174.014494\" xlink:href=\"#m371f10bfd5\" y=\"122.883545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.647602\" xlink:href=\"#m371f10bfd5\" y=\"188.648836\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"324.758841\" xlink:href=\"#m371f10bfd5\" y=\"214.858624\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"77.51716\" xlink:href=\"#m371f10bfd5\" y=\"64.315417\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.123742\" xlink:href=\"#m371f10bfd5\" y=\"105.904751\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"266.993891\" xlink:href=\"#m371f10bfd5\" y=\"196.152604\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"416.95551\" xlink:href=\"#m371f10bfd5\" y=\"259.179238\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.330097\" xlink:href=\"#m371f10bfd5\" y=\"148.434724\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.499011\" xlink:href=\"#m371f10bfd5\" y=\"160.940003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.600035\" xlink:href=\"#m371f10bfd5\" y=\"133.523972\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.189809\" xlink:href=\"#m371f10bfd5\" y=\"144.643328\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.33261\" xlink:href=\"#m371f10bfd5\" y=\"178.06094\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.339459\" xlink:href=\"#m371f10bfd5\" y=\"241.383896\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"271.802685\" xlink:href=\"#m371f10bfd5\" y=\"187.213227\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"405.790383\" xlink:href=\"#m371f10bfd5\" y=\"219.809166\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"398.575381\" xlink:href=\"#m371f10bfd5\" y=\"198.040722\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.178023\" xlink:href=\"#m371f10bfd5\" y=\"168.326587\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.31297\" xlink:href=\"#m371f10bfd5\" y=\"185.950956\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"111.479961\" xlink:href=\"#m371f10bfd5\" y=\"62.718852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.073304\" xlink:href=\"#m371f10bfd5\" y=\"135.135287\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.653134\" xlink:href=\"#m371f10bfd5\" y=\"119.449337\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"229.977045\" xlink:href=\"#m371f10bfd5\" y=\"186.537238\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"219.100497\" xlink:href=\"#m371f10bfd5\" y=\"139.44631\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.34401\" xlink:href=\"#m371f10bfd5\" y=\"127.756898\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"335.492959\" xlink:href=\"#m371f10bfd5\" y=\"215.120505\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"275.256922\" xlink:href=\"#m371f10bfd5\" y=\"149.766987\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.484401\" xlink:href=\"#m371f10bfd5\" y=\"80.087878\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.324383\" xlink:href=\"#m371f10bfd5\" y=\"151.781778\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"265.348304\" xlink:href=\"#m371f10bfd5\" y=\"153.943807\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.035834\" xlink:href=\"#m371f10bfd5\" y=\"186.418205\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"418.393785\" xlink:href=\"#m371f10bfd5\" y=\"214.36372\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.33654\" xlink:href=\"#m371f10bfd5\" y=\"121.632138\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"190.139524\" xlink:href=\"#m371f10bfd5\" y=\"174.843566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.246604\" xlink:href=\"#m371f10bfd5\" y=\"200.135895\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"331.470161\" xlink:href=\"#m371f10bfd5\" y=\"233.514062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"370.397718\" xlink:href=\"#m371f10bfd5\" y=\"217.401733\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"223.304251\" xlink:href=\"#m371f10bfd5\" y=\"135.427973\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"259.492325\" xlink:href=\"#m371f10bfd5\" y=\"135.004229\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"331.520803\" xlink:href=\"#m371f10bfd5\" y=\"187.885462\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"236.708231\" xlink:href=\"#m371f10bfd5\" y=\"188.500026\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.047935\" xlink:href=\"#m371f10bfd5\" y=\"124.899543\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.815566\" xlink:href=\"#m371f10bfd5\" y=\"161.853355\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.903548\" xlink:href=\"#m371f10bfd5\" y=\"203.647686\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.637288\" xlink:href=\"#m371f10bfd5\" y=\"122.406142\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"481.708486\" xlink:href=\"#m371f10bfd5\" y=\"227.062184\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"243.307123\" xlink:href=\"#m371f10bfd5\" y=\"144.003137\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"498.421965\" xlink:href=\"#m371f10bfd5\" y=\"318.534545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"510.450137\" xlink:href=\"#m371f10bfd5\" y=\"277.676362\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"421.654206\" xlink:href=\"#m371f10bfd5\" y=\"228.493368\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.461566\" xlink:href=\"#m371f10bfd5\" y=\"148.430245\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"344.527779\" xlink:href=\"#m371f10bfd5\" y=\"225.077525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.630901\" xlink:href=\"#m371f10bfd5\" y=\"216.103392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"445.022332\" xlink:href=\"#m371f10bfd5\" y=\"248.857082\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.512424\" xlink:href=\"#m371f10bfd5\" y=\"112.5053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.853917\" xlink:href=\"#m371f10bfd5\" y=\"177.688719\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"239.919847\" xlink:href=\"#m371f10bfd5\" y=\"101.226508\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.101441\" xlink:href=\"#m371f10bfd5\" y=\"215.842809\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"294.680689\" xlink:href=\"#m371f10bfd5\" y=\"188.483468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.826908\" xlink:href=\"#m371f10bfd5\" y=\"156.233062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"401.94879\" xlink:href=\"#m371f10bfd5\" y=\"252.331506\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"255.902578\" xlink:href=\"#m371f10bfd5\" y=\"179.343167\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.416257\" xlink:href=\"#m371f10bfd5\" y=\"180.595958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.533038\" xlink:href=\"#m371f10bfd5\" y=\"171.0391\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"396.8979\" xlink:href=\"#m371f10bfd5\" y=\"246.50566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.432127\" xlink:href=\"#m371f10bfd5\" y=\"144.552079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.53821\" xlink:href=\"#m371f10bfd5\" y=\"139.535133\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.30343\" xlink:href=\"#m371f10bfd5\" y=\"195.396764\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"352.517103\" xlink:href=\"#m371f10bfd5\" y=\"178.431039\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.950539\" xlink:href=\"#m371f10bfd5\" y=\"77.985598\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.016273\" xlink:href=\"#m371f10bfd5\" y=\"189.874305\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"107.451607\" xlink:href=\"#m371f10bfd5\" y=\"126.736826\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.127001\" xlink:href=\"#m371f10bfd5\" y=\"158.205558\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.557716\" xlink:href=\"#m371f10bfd5\" y=\"161.642546\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.2409\" xlink:href=\"#m371f10bfd5\" y=\"101.423737\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"219.507132\" xlink:href=\"#m371f10bfd5\" y=\"125.161514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.25147\" xlink:href=\"#m371f10bfd5\" y=\"200.240651\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.160101\" xlink:href=\"#m371f10bfd5\" y=\"226.637081\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"217.561815\" xlink:href=\"#m371f10bfd5\" y=\"141.38479\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.981221\" xlink:href=\"#m371f10bfd5\" y=\"207.736333\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"360.304168\" xlink:href=\"#m371f10bfd5\" y=\"251.363619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"316.333149\" xlink:href=\"#m371f10bfd5\" y=\"212.989026\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.919924\" xlink:href=\"#m371f10bfd5\" y=\"270.178291\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.497795\" xlink:href=\"#m371f10bfd5\" y=\"102.57288\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.711759\" xlink:href=\"#m371f10bfd5\" y=\"119.968694\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.989292\" xlink:href=\"#m371f10bfd5\" y=\"90.500248\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"211.571874\" xlink:href=\"#m371f10bfd5\" y=\"148.614513\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.753435\" xlink:href=\"#m371f10bfd5\" y=\"189.927797\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"351.756838\" xlink:href=\"#m371f10bfd5\" y=\"187.211597\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"168.515666\" xlink:href=\"#m371f10bfd5\" y=\"82.107394\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.586133\" xlink:href=\"#m371f10bfd5\" y=\"130.692552\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.947473\" xlink:href=\"#m371f10bfd5\" y=\"131.700227\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.730544\" xlink:href=\"#m371f10bfd5\" y=\"213.608671\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"389.860807\" xlink:href=\"#m371f10bfd5\" y=\"219.574988\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"354.427536\" xlink:href=\"#m371f10bfd5\" y=\"198.196497\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"335.531296\" xlink:href=\"#m371f10bfd5\" y=\"196.340734\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"192.237361\" xlink:href=\"#m371f10bfd5\" y=\"136.264708\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.310376\" xlink:href=\"#m371f10bfd5\" y=\"195.40295\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"247.147636\" xlink:href=\"#m371f10bfd5\" y=\"143.33793\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.290604\" xlink:href=\"#m371f10bfd5\" y=\"106.805754\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"264.394671\" xlink:href=\"#m371f10bfd5\" y=\"147.720257\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"217.312889\" xlink:href=\"#m371f10bfd5\" y=\"159.763814\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"379.453285\" xlink:href=\"#m371f10bfd5\" y=\"209.356606\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"435.072896\" xlink:href=\"#m371f10bfd5\" y=\"253.403132\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.929184\" xlink:href=\"#m371f10bfd5\" y=\"181.989935\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"262.58352\" xlink:href=\"#m371f10bfd5\" y=\"185.522119\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.641987\" xlink:href=\"#m371f10bfd5\" y=\"84.246861\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.549574\" xlink:href=\"#m371f10bfd5\" y=\"108.831562\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"274.379237\" xlink:href=\"#m371f10bfd5\" y=\"149.396807\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.787843\" xlink:href=\"#m371f10bfd5\" y=\"107.112356\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.13458\" xlink:href=\"#m371f10bfd5\" y=\"121.927273\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.293463\" xlink:href=\"#m371f10bfd5\" y=\"99.106204\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"249.938574\" xlink:href=\"#m371f10bfd5\" y=\"164.100808\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.377013\" xlink:href=\"#m371f10bfd5\" y=\"224.377075\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.48788\" xlink:href=\"#m371f10bfd5\" y=\"178.373606\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"238.318373\" xlink:href=\"#m371f10bfd5\" y=\"140.256522\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"274.168532\" xlink:href=\"#m371f10bfd5\" y=\"167.63566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"311.195766\" xlink:href=\"#m371f10bfd5\" y=\"207.671519\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.334912\" xlink:href=\"#m371f10bfd5\" y=\"148.154958\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"234.225439\" xlink:href=\"#m371f10bfd5\" y=\"108.686672\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"361.148284\" xlink:href=\"#m371f10bfd5\" y=\"200.855488\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.653557\" xlink:href=\"#m371f10bfd5\" y=\"180.8365\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.251867\" xlink:href=\"#m371f10bfd5\" y=\"121.733226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"456.343688\" xlink:href=\"#m371f10bfd5\" y=\"254.590797\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.55307\" xlink:href=\"#m371f10bfd5\" y=\"208.907914\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"561.578551\" xlink:href=\"#m371f10bfd5\" y=\"312.847217\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.063803\" xlink:href=\"#m371f10bfd5\" y=\"169.831807\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"243.450177\" xlink:href=\"#m371f10bfd5\" y=\"139.45241\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"404.714299\" xlink:href=\"#m371f10bfd5\" y=\"249.990552\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.298634\" xlink:href=\"#m371f10bfd5\" y=\"158.492985\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"352.900729\" xlink:href=\"#m371f10bfd5\" y=\"194.607463\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.259286\" xlink:href=\"#m371f10bfd5\" y=\"181.815685\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.450366\" xlink:href=\"#m371f10bfd5\" y=\"207.862144\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.782569\" xlink:href=\"#m371f10bfd5\" y=\"158.480179\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"176.892319\" xlink:href=\"#m371f10bfd5\" y=\"131.084423\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"74.954833\" xlink:href=\"#m371f10bfd5\" y=\"22.703617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"362.707933\" xlink:href=\"#m371f10bfd5\" y=\"233.850637\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"288.628644\" xlink:href=\"#m371f10bfd5\" y=\"191.107861\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"373.240478\" xlink:href=\"#m371f10bfd5\" y=\"210.778226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.121653\" xlink:href=\"#m371f10bfd5\" y=\"209.723954\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.277854\" xlink:href=\"#m371f10bfd5\" y=\"228.879516\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"218.915577\" xlink:href=\"#m371f10bfd5\" y=\"114.169525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"384.656107\" xlink:href=\"#m371f10bfd5\" y=\"227.045381\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.154793\" xlink:href=\"#m371f10bfd5\" y=\"207.915849\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.393291\" xlink:href=\"#m371f10bfd5\" y=\"162.78036\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.503216\" xlink:href=\"#m371f10bfd5\" y=\"198.950506\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"217.782663\" xlink:href=\"#m371f10bfd5\" y=\"109.953568\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.303634\" xlink:href=\"#m371f10bfd5\" y=\"205.424797\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"327.617906\" xlink:href=\"#m371f10bfd5\" y=\"217.019818\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"372.419272\" xlink:href=\"#m371f10bfd5\" y=\"211.314151\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"405.638641\" xlink:href=\"#m371f10bfd5\" y=\"236.699133\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.428664\" xlink:href=\"#m371f10bfd5\" y=\"128.984027\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.725313\" xlink:href=\"#m371f10bfd5\" y=\"105.628155\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.246747\" xlink:href=\"#m371f10bfd5\" y=\"164.126948\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.297211\" xlink:href=\"#m371f10bfd5\" y=\"150.008973\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.049381\" xlink:href=\"#m371f10bfd5\" y=\"202.042775\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.861088\" xlink:href=\"#m371f10bfd5\" y=\"143.928075\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"205.62068\" xlink:href=\"#m371f10bfd5\" y=\"144.168656\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.742087\" xlink:href=\"#m371f10bfd5\" y=\"152.929276\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.644767\" xlink:href=\"#m371f10bfd5\" y=\"138.752748\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"143.39478\" xlink:href=\"#m371f10bfd5\" y=\"161.143794\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"295.913154\" xlink:href=\"#m371f10bfd5\" y=\"157.10056\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"289.062863\" xlink:href=\"#m371f10bfd5\" y=\"166.520372\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"389.169738\" xlink:href=\"#m371f10bfd5\" y=\"210.118136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.367347\" xlink:href=\"#m371f10bfd5\" y=\"154.348723\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.15349\" xlink:href=\"#m371f10bfd5\" y=\"211.118411\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.501902\" xlink:href=\"#m371f10bfd5\" y=\"146.36401\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.863724\" xlink:href=\"#m371f10bfd5\" y=\"155.71421\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.925437\" xlink:href=\"#m371f10bfd5\" y=\"213.237753\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"398.230936\" xlink:href=\"#m371f10bfd5\" y=\"250.074552\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.004489\" xlink:href=\"#m371f10bfd5\" y=\"186.557055\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.227101\" xlink:href=\"#m371f10bfd5\" y=\"130.621458\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"380.967832\" xlink:href=\"#m371f10bfd5\" y=\"229.556484\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.082422\" xlink:href=\"#m371f10bfd5\" y=\"167.604069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.193248\" xlink:href=\"#m371f10bfd5\" y=\"112.117362\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.484136\" xlink:href=\"#m371f10bfd5\" y=\"152.730726\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"371.850035\" xlink:href=\"#m371f10bfd5\" y=\"252.748068\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.07639\" xlink:href=\"#m371f10bfd5\" y=\"224.022119\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"408.843025\" xlink:href=\"#m371f10bfd5\" y=\"217.312601\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"322.598312\" xlink:href=\"#m371f10bfd5\" y=\"194.915148\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"324.982428\" xlink:href=\"#m371f10bfd5\" y=\"155.310392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"417.59032\" xlink:href=\"#m371f10bfd5\" y=\"271.624337\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.333574\" xlink:href=\"#m371f10bfd5\" y=\"175.647095\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"413.762137\" xlink:href=\"#m371f10bfd5\" y=\"208.4069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"290.850146\" xlink:href=\"#m371f10bfd5\" y=\"188.433644\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"434.42018\" xlink:href=\"#m371f10bfd5\" y=\"240.343208\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.211848\" xlink:href=\"#m371f10bfd5\" y=\"187.169842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"258.964371\" xlink:href=\"#m371f10bfd5\" y=\"130.217826\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.52609\" xlink:href=\"#m371f10bfd5\" y=\"147.284603\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.13032\" xlink:href=\"#m371f10bfd5\" y=\"138.826538\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"430.775095\" xlink:href=\"#m371f10bfd5\" y=\"216.096007\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"437.684616\" xlink:href=\"#m371f10bfd5\" y=\"282.778781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"159.015945\" xlink:href=\"#m371f10bfd5\" y=\"114.172298\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.301168\" xlink:href=\"#m371f10bfd5\" y=\"120.339498\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.894082\" xlink:href=\"#m371f10bfd5\" y=\"181.744299\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.883762\" xlink:href=\"#m371f10bfd5\" y=\"182.965074\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.804059\" xlink:href=\"#m371f10bfd5\" y=\"233.628868\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"431.404294\" xlink:href=\"#m371f10bfd5\" y=\"192.678799\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.485823\" xlink:href=\"#m371f10bfd5\" y=\"123.51518\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.255009\" xlink:href=\"#m371f10bfd5\" y=\"221.481126\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"386.7988\" xlink:href=\"#m371f10bfd5\" y=\"221.589256\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"340.170572\" xlink:href=\"#m371f10bfd5\" y=\"198.127927\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.994937\" xlink:href=\"#m371f10bfd5\" y=\"185.926378\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.191253\" xlink:href=\"#m371f10bfd5\" y=\"106.392517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.471411\" xlink:href=\"#m371f10bfd5\" y=\"222.492985\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.409102\" xlink:href=\"#m371f10bfd5\" y=\"187.521\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.10582\" xlink:href=\"#m371f10bfd5\" y=\"153.279752\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"494.960951\" xlink:href=\"#m371f10bfd5\" y=\"251.950652\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.383423\" xlink:href=\"#m371f10bfd5\" y=\"243.284271\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.067399\" xlink:href=\"#m371f10bfd5\" y=\"145.069203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"452.089578\" xlink:href=\"#m371f10bfd5\" y=\"311.526132\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.05578\" xlink:href=\"#m371f10bfd5\" y=\"208.364436\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.367621\" xlink:href=\"#m371f10bfd5\" y=\"155.137508\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.843381\" xlink:href=\"#m371f10bfd5\" y=\"115.862014\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"433.670002\" xlink:href=\"#m371f10bfd5\" y=\"266.461619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"380.664358\" xlink:href=\"#m371f10bfd5\" y=\"236.80487\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"435.672386\" xlink:href=\"#m371f10bfd5\" y=\"303.832436\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.531577\" xlink:href=\"#m371f10bfd5\" y=\"126.863862\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.562361\" xlink:href=\"#m371f10bfd5\" y=\"131.717949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"391.276157\" xlink:href=\"#m371f10bfd5\" y=\"243.365277\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.97809\" xlink:href=\"#m371f10bfd5\" y=\"154.881263\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.161843\" xlink:href=\"#m371f10bfd5\" y=\"197.055168\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"216.65012\" xlink:href=\"#m371f10bfd5\" y=\"129.034883\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.108243\" xlink:href=\"#m371f10bfd5\" y=\"227.094413\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.170528\" xlink:href=\"#m371f10bfd5\" y=\"74.656176\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.665908\" xlink:href=\"#m371f10bfd5\" y=\"176.85742\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"370.547663\" xlink:href=\"#m371f10bfd5\" y=\"197.297739\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"252.479497\" xlink:href=\"#m371f10bfd5\" y=\"183.498151\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"169.13009\" xlink:href=\"#m371f10bfd5\" y=\"107.526494\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.639305\" xlink:href=\"#m371f10bfd5\" y=\"58.091399\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"329.432991\" xlink:href=\"#m371f10bfd5\" y=\"142.242009\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"365.451179\" xlink:href=\"#m371f10bfd5\" y=\"194.618737\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.450578\" xlink:href=\"#m371f10bfd5\" y=\"187.188709\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.757482\" xlink:href=\"#m371f10bfd5\" y=\"207.069681\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"306.573819\" xlink:href=\"#m371f10bfd5\" y=\"150.877107\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.960899\" xlink:href=\"#m371f10bfd5\" y=\"185.123947\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"174.890959\" xlink:href=\"#m371f10bfd5\" y=\"147.147037\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.918126\" xlink:href=\"#m371f10bfd5\" y=\"224.830532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.327221\" xlink:href=\"#m371f10bfd5\" y=\"224.636434\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"391.942624\" xlink:href=\"#m371f10bfd5\" y=\"253.792103\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"344.999797\" xlink:href=\"#m371f10bfd5\" y=\"193.197675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.01415\" xlink:href=\"#m371f10bfd5\" y=\"149.012051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.242718\" xlink:href=\"#m371f10bfd5\" y=\"162.188312\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.578259\" xlink:href=\"#m371f10bfd5\" y=\"180.965148\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.923372\" xlink:href=\"#m371f10bfd5\" y=\"162.681547\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.10976\" xlink:href=\"#m371f10bfd5\" y=\"122.87089\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"382.789607\" xlink:href=\"#m371f10bfd5\" y=\"190.224085\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.184365\" xlink:href=\"#m371f10bfd5\" y=\"158.668967\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"272.966531\" xlink:href=\"#m371f10bfd5\" y=\"128.050947\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"265.929746\" xlink:href=\"#m371f10bfd5\" y=\"144.34501\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.099116\" xlink:href=\"#m371f10bfd5\" y=\"158.042675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"290.295211\" xlink:href=\"#m371f10bfd5\" y=\"171.358952\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.051449\" xlink:href=\"#m371f10bfd5\" y=\"176.43325\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"415.08109\" xlink:href=\"#m371f10bfd5\" y=\"234.801687\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"435.625121\" xlink:href=\"#m371f10bfd5\" y=\"265.436041\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.717594\" xlink:href=\"#m371f10bfd5\" y=\"235.058007\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"346.040169\" xlink:href=\"#m371f10bfd5\" y=\"213.303661\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"438.022989\" xlink:href=\"#m371f10bfd5\" y=\"250.715919\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"271.880644\" xlink:href=\"#m371f10bfd5\" y=\"140.220488\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.588767\" xlink:href=\"#m371f10bfd5\" y=\"196.406455\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"471.169949\" xlink:href=\"#m371f10bfd5\" y=\"267.22903\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"475.359079\" xlink:href=\"#m371f10bfd5\" y=\"287.905628\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"384.982345\" xlink:href=\"#m371f10bfd5\" y=\"208.853924\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"218.361757\" xlink:href=\"#m371f10bfd5\" y=\"165.651187\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"144.78041\" xlink:href=\"#m371f10bfd5\" y=\"155.202376\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.449915\" xlink:href=\"#m371f10bfd5\" y=\"182.347766\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"244.466766\" xlink:href=\"#m371f10bfd5\" y=\"171.079852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"497.633747\" xlink:href=\"#m371f10bfd5\" y=\"273.257527\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.35151\" xlink:href=\"#m371f10bfd5\" y=\"143.521573\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.27071\" xlink:href=\"#m371f10bfd5\" y=\"199.145261\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.618623\" xlink:href=\"#m371f10bfd5\" y=\"94.509129\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.427503\" xlink:href=\"#m371f10bfd5\" y=\"135.302391\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.740867\" xlink:href=\"#m371f10bfd5\" y=\"185.194606\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.917153\" xlink:href=\"#m371f10bfd5\" y=\"173.530754\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.036083\" xlink:href=\"#m371f10bfd5\" y=\"183.422299\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"415.467692\" xlink:href=\"#m371f10bfd5\" y=\"243.782136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"171.135806\" xlink:href=\"#m371f10bfd5\" y=\"83.159207\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.36733\" xlink:href=\"#m371f10bfd5\" y=\"186.292685\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.026852\" xlink:href=\"#m371f10bfd5\" y=\"195.824617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"333.204191\" xlink:href=\"#m371f10bfd5\" y=\"255.751019\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"244.664854\" xlink:href=\"#m371f10bfd5\" y=\"178.275709\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"404.121153\" xlink:href=\"#m371f10bfd5\" y=\"244.23579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.05195\" xlink:href=\"#m371f10bfd5\" y=\"217.931338\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.897208\" xlink:href=\"#m371f10bfd5\" y=\"235.548436\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"318.229906\" xlink:href=\"#m371f10bfd5\" y=\"209.339803\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.830768\" xlink:href=\"#m371f10bfd5\" y=\"224.641279\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"389.013249\" xlink:href=\"#m371f10bfd5\" y=\"205.859781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"425.593611\" xlink:href=\"#m371f10bfd5\" y=\"245.775727\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"214.242692\" xlink:href=\"#m371f10bfd5\" y=\"133.64905\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.178643\" xlink:href=\"#m371f10bfd5\" y=\"197.117095\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"363.869714\" xlink:href=\"#m371f10bfd5\" y=\"210.398666\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.705841\" xlink:href=\"#m371f10bfd5\" y=\"80.841386\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"320.747477\" xlink:href=\"#m371f10bfd5\" y=\"172.08291\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.224793\" xlink:href=\"#m371f10bfd5\" y=\"150.916286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"196.131649\" xlink:href=\"#m371f10bfd5\" y=\"105.307468\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.032983\" xlink:href=\"#m371f10bfd5\" y=\"148.231443\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.713796\" xlink:href=\"#m371f10bfd5\" y=\"157.168796\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.059404\" xlink:href=\"#m371f10bfd5\" y=\"177.654835\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.027833\" xlink:href=\"#m371f10bfd5\" y=\"212.778583\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"342.275651\" xlink:href=\"#m371f10bfd5\" y=\"252.597752\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"306.493535\" xlink:href=\"#m371f10bfd5\" y=\"185.334282\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.926741\" xlink:href=\"#m371f10bfd5\" y=\"232.036597\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"390.671869\" xlink:href=\"#m371f10bfd5\" y=\"224.321175\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"213.296117\" xlink:href=\"#m371f10bfd5\" y=\"152.272438\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"303.202033\" xlink:href=\"#m371f10bfd5\" y=\"186.954485\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"237.784466\" xlink:href=\"#m371f10bfd5\" y=\"143.796332\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.725906\" xlink:href=\"#m371f10bfd5\" y=\"189.589691\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.633006\" xlink:href=\"#m371f10bfd5\" y=\"171.904064\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"309.283283\" xlink:href=\"#m371f10bfd5\" y=\"186.043051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"440.16515\" xlink:href=\"#m371f10bfd5\" y=\"232.955735\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.510492\" xlink:href=\"#m371f10bfd5\" y=\"156.612464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"352.423597\" xlink:href=\"#m371f10bfd5\" y=\"160.05432\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.280041\" xlink:href=\"#m371f10bfd5\" y=\"182.711495\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.483612\" xlink:href=\"#m371f10bfd5\" y=\"212.817919\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.61307\" xlink:href=\"#m371f10bfd5\" y=\"106.278485\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"237.333391\" xlink:href=\"#m371f10bfd5\" y=\"150.481262\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.376334\" xlink:href=\"#m371f10bfd5\" y=\"167.985434\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.749538\" xlink:href=\"#m371f10bfd5\" y=\"126.669742\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.959403\" xlink:href=\"#m371f10bfd5\" y=\"172.828253\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.318832\" xlink:href=\"#m371f10bfd5\" y=\"142.963746\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.321076\" xlink:href=\"#m371f10bfd5\" y=\"215.002201\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.833212\" xlink:href=\"#m371f10bfd5\" y=\"107.278739\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"186.334075\" xlink:href=\"#m371f10bfd5\" y=\"121.91517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"357.549204\" xlink:href=\"#m371f10bfd5\" y=\"206.775878\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"402.202429\" xlink:href=\"#m371f10bfd5\" y=\"200.10199\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"202.202726\" xlink:href=\"#m371f10bfd5\" y=\"123.105437\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.465942\" xlink:href=\"#m371f10bfd5\" y=\"123.983074\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"249.15891\" xlink:href=\"#m371f10bfd5\" y=\"167.206693\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"281.257985\" xlink:href=\"#m371f10bfd5\" y=\"156.80809\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.216181\" xlink:href=\"#m371f10bfd5\" y=\"199.725461\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"276.958444\" xlink:href=\"#m371f10bfd5\" y=\"157.014871\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"381.065413\" xlink:href=\"#m371f10bfd5\" y=\"225.436019\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"406.726629\" xlink:href=\"#m371f10bfd5\" y=\"225.14092\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"190.115686\" xlink:href=\"#m371f10bfd5\" y=\"112.827343\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"324.526711\" xlink:href=\"#m371f10bfd5\" y=\"177.14977\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"228.541129\" xlink:href=\"#m371f10bfd5\" y=\"112.125478\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.976462\" xlink:href=\"#m371f10bfd5\" y=\"210.007134\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"344.835273\" xlink:href=\"#m371f10bfd5\" y=\"235.694688\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.964626\" xlink:href=\"#m371f10bfd5\" y=\"112.161284\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.779794\" xlink:href=\"#m371f10bfd5\" y=\"249.688251\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"153.597735\" xlink:href=\"#m371f10bfd5\" y=\"120.115709\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"200.578021\" xlink:href=\"#m371f10bfd5\" y=\"157.430834\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"435.142635\" xlink:href=\"#m371f10bfd5\" y=\"289.448605\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"354.108972\" xlink:href=\"#m371f10bfd5\" y=\"192.996343\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.853674\" xlink:href=\"#m371f10bfd5\" y=\"205.872028\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"281.370088\" xlink:href=\"#m371f10bfd5\" y=\"140.347344\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.626938\" xlink:href=\"#m371f10bfd5\" y=\"157.849757\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.376968\" xlink:href=\"#m371f10bfd5\" y=\"168.962923\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.587811\" xlink:href=\"#m371f10bfd5\" y=\"212.269316\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.110398\" xlink:href=\"#m371f10bfd5\" y=\"236.522165\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.708004\" xlink:href=\"#m371f10bfd5\" y=\"202.305617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.360943\" xlink:href=\"#m371f10bfd5\" y=\"185.376724\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"288.41087\" xlink:href=\"#m371f10bfd5\" y=\"151.449139\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"289.904809\" xlink:href=\"#m371f10bfd5\" y=\"140.995609\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"400.052319\" xlink:href=\"#m371f10bfd5\" y=\"218.543558\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"327.115596\" xlink:href=\"#m371f10bfd5\" y=\"226.017771\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"277.083706\" xlink:href=\"#m371f10bfd5\" y=\"202.555869\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"244.132027\" xlink:href=\"#m371f10bfd5\" y=\"119.848562\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.880409\" xlink:href=\"#m371f10bfd5\" y=\"100.65498\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.79669\" xlink:href=\"#m371f10bfd5\" y=\"152.882928\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.839279\" xlink:href=\"#m371f10bfd5\" y=\"132.95143\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"214.36638\" xlink:href=\"#m371f10bfd5\" y=\"150.175176\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"326.607733\" xlink:href=\"#m371f10bfd5\" y=\"208.767946\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"382.995037\" xlink:href=\"#m371f10bfd5\" y=\"205.611899\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.057913\" xlink:href=\"#m371f10bfd5\" y=\"122.365882\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"279.13167\" xlink:href=\"#m371f10bfd5\" y=\"157.931651\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.829451\" xlink:href=\"#m371f10bfd5\" y=\"177.500633\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"264.458258\" xlink:href=\"#m371f10bfd5\" y=\"181.034615\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.114794\" xlink:href=\"#m371f10bfd5\" y=\"234.765532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"339.936817\" xlink:href=\"#m371f10bfd5\" y=\"221.460594\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"401.333402\" xlink:href=\"#m371f10bfd5\" y=\"208.635165\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.462552\" xlink:href=\"#m371f10bfd5\" y=\"156.913867\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.906797\" xlink:href=\"#m371f10bfd5\" y=\"164.423422\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"245.654428\" xlink:href=\"#m371f10bfd5\" y=\"162.580777\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"186.285465\" xlink:href=\"#m371f10bfd5\" y=\"155.548866\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.882079\" xlink:href=\"#m371f10bfd5\" y=\"195.193543\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"397.613921\" xlink:href=\"#m371f10bfd5\" y=\"200.631354\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.552369\" xlink:href=\"#m371f10bfd5\" y=\"118.17855\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.210668\" xlink:href=\"#m371f10bfd5\" y=\"133.466254\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"183.983924\" xlink:href=\"#m371f10bfd5\" y=\"171.021696\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"104.587001\" xlink:href=\"#m371f10bfd5\" y=\"92.600331\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"188.310121\" xlink:href=\"#m371f10bfd5\" y=\"121.448525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"324.358433\" xlink:href=\"#m371f10bfd5\" y=\"179.384003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"177.371292\" xlink:href=\"#m371f10bfd5\" y=\"68.91278\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.885901\" xlink:href=\"#m371f10bfd5\" y=\"200.246042\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.959269\" xlink:href=\"#m371f10bfd5\" y=\"210.987497\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"365.757022\" xlink:href=\"#m371f10bfd5\" y=\"250.535113\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"338.983041\" xlink:href=\"#m371f10bfd5\" y=\"152.656005\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.184351\" xlink:href=\"#m371f10bfd5\" y=\"140.127535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"378.198519\" xlink:href=\"#m371f10bfd5\" y=\"265.68787\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"213.370944\" xlink:href=\"#m371f10bfd5\" y=\"141.814573\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.542933\" xlink:href=\"#m371f10bfd5\" y=\"168.43061\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"396.187119\" xlink:href=\"#m371f10bfd5\" y=\"200.704535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"139.448961\" xlink:href=\"#m371f10bfd5\" y=\"87.529431\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.008554\" xlink:href=\"#m371f10bfd5\" y=\"174.489404\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.088732\" xlink:href=\"#m371f10bfd5\" y=\"150.986756\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.769793\" xlink:href=\"#m371f10bfd5\" y=\"237.25448\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.014076\" xlink:href=\"#m371f10bfd5\" y=\"132.453566\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"372.096417\" xlink:href=\"#m371f10bfd5\" y=\"229.665892\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.570061\" xlink:href=\"#m371f10bfd5\" y=\"176.673471\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"309.983818\" xlink:href=\"#m371f10bfd5\" y=\"134.680949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.457123\" xlink:href=\"#m371f10bfd5\" y=\"172.135525\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"149.348894\" xlink:href=\"#m371f10bfd5\" y=\"130.473686\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.278734\" xlink:href=\"#m371f10bfd5\" y=\"153.153851\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"386.026752\" xlink:href=\"#m371f10bfd5\" y=\"250.399053\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.29323\" xlink:href=\"#m371f10bfd5\" y=\"100.236136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"377.360507\" xlink:href=\"#m371f10bfd5\" y=\"252.207623\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"369.121046\" xlink:href=\"#m371f10bfd5\" y=\"205.78309\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.955468\" xlink:href=\"#m371f10bfd5\" y=\"156.439075\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.864658\" xlink:href=\"#m371f10bfd5\" y=\"149.528814\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.649182\" xlink:href=\"#m371f10bfd5\" y=\"186.244669\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"259.1674\" xlink:href=\"#m371f10bfd5\" y=\"162.540932\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"249.442852\" xlink:href=\"#m371f10bfd5\" y=\"130.811201\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"251.361864\" xlink:href=\"#m371f10bfd5\" y=\"135.569675\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"437.853852\" xlink:href=\"#m371f10bfd5\" y=\"249.402226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"432.811123\" xlink:href=\"#m371f10bfd5\" y=\"201.507455\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.117673\" xlink:href=\"#m371f10bfd5\" y=\"154.061464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"388.620274\" xlink:href=\"#m371f10bfd5\" y=\"194.789932\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"329.574505\" xlink:href=\"#m371f10bfd5\" y=\"172.059784\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.30736\" xlink:href=\"#m371f10bfd5\" y=\"165.320499\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.32185\" xlink:href=\"#m371f10bfd5\" y=\"169.789872\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.775727\" xlink:href=\"#m371f10bfd5\" y=\"193.485543\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.465042\" xlink:href=\"#m371f10bfd5\" y=\"165.15752\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.730468\" xlink:href=\"#m371f10bfd5\" y=\"211.079844\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.228424\" xlink:href=\"#m371f10bfd5\" y=\"175.74903\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.559525\" xlink:href=\"#m371f10bfd5\" y=\"186.225902\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.438093\" xlink:href=\"#m371f10bfd5\" y=\"98.944624\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.822129\" xlink:href=\"#m371f10bfd5\" y=\"80.624237\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.355014\" xlink:href=\"#m371f10bfd5\" y=\"205.367074\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.033591\" xlink:href=\"#m371f10bfd5\" y=\"161.9861\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.536721\" xlink:href=\"#m371f10bfd5\" y=\"190.545909\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"204.890075\" xlink:href=\"#m371f10bfd5\" y=\"108.726571\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.473541\" xlink:href=\"#m371f10bfd5\" y=\"152.180481\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"258.608804\" xlink:href=\"#m371f10bfd5\" y=\"196.683796\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.732201\" xlink:href=\"#m371f10bfd5\" y=\"171.605891\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"392.285032\" xlink:href=\"#m371f10bfd5\" y=\"221.413136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.840917\" xlink:href=\"#m371f10bfd5\" y=\"157.717925\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.972611\" xlink:href=\"#m371f10bfd5\" y=\"157.664841\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.682364\" xlink:href=\"#m371f10bfd5\" y=\"124.924061\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"295.126685\" xlink:href=\"#m371f10bfd5\" y=\"139.60119\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.862786\" xlink:href=\"#m371f10bfd5\" y=\"244.20479\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"301.186518\" xlink:href=\"#m371f10bfd5\" y=\"147.669875\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.955747\" xlink:href=\"#m371f10bfd5\" y=\"159.160612\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"395.904543\" xlink:href=\"#m371f10bfd5\" y=\"252.017323\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"353.267165\" xlink:href=\"#m371f10bfd5\" y=\"226.124801\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"463.935204\" xlink:href=\"#m371f10bfd5\" y=\"250.987333\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"221.27826\" xlink:href=\"#m371f10bfd5\" y=\"136.942547\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.241474\" xlink:href=\"#m371f10bfd5\" y=\"233.79506\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.736378\" xlink:href=\"#m371f10bfd5\" y=\"158.607359\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"399.177189\" xlink:href=\"#m371f10bfd5\" y=\"246.840889\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.283583\" xlink:href=\"#m371f10bfd5\" y=\"177.397225\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"217.497813\" xlink:href=\"#m371f10bfd5\" y=\"160.914938\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"396.739353\" xlink:href=\"#m371f10bfd5\" y=\"181.542667\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"280.684102\" xlink:href=\"#m371f10bfd5\" y=\"144.925141\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"254.850977\" xlink:href=\"#m371f10bfd5\" y=\"161.724879\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"438.953002\" xlink:href=\"#m371f10bfd5\" y=\"263.057692\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"205.217979\" xlink:href=\"#m371f10bfd5\" y=\"124.23532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"478.319519\" xlink:href=\"#m371f10bfd5\" y=\"220.413614\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"256.761089\" xlink:href=\"#m371f10bfd5\" y=\"136.46587\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"492.78628\" xlink:href=\"#m371f10bfd5\" y=\"266.842074\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.869992\" xlink:href=\"#m371f10bfd5\" y=\"122.924057\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"303.733955\" xlink:href=\"#m371f10bfd5\" y=\"191.325973\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.013642\" xlink:href=\"#m371f10bfd5\" y=\"69.932738\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"234.669689\" xlink:href=\"#m371f10bfd5\" y=\"106.429511\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.624387\" xlink:href=\"#m371f10bfd5\" y=\"158.541992\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.228612\" xlink:href=\"#m371f10bfd5\" y=\"171.876982\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.153842\" xlink:href=\"#m371f10bfd5\" y=\"127.312832\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.708979\" xlink:href=\"#m371f10bfd5\" y=\"228.974318\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"370.599646\" xlink:href=\"#m371f10bfd5\" y=\"246.754724\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"178.808081\" xlink:href=\"#m371f10bfd5\" y=\"160.200081\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.869619\" xlink:href=\"#m371f10bfd5\" y=\"139.785428\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"174.773313\" xlink:href=\"#m371f10bfd5\" y=\"94.491731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"472.778505\" xlink:href=\"#m371f10bfd5\" y=\"274.936904\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"272.463223\" xlink:href=\"#m371f10bfd5\" y=\"150.787858\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.303823\" xlink:href=\"#m371f10bfd5\" y=\"165.565931\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.358872\" xlink:href=\"#m371f10bfd5\" y=\"132.021604\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.600774\" xlink:href=\"#m371f10bfd5\" y=\"108.308495\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.860747\" xlink:href=\"#m371f10bfd5\" y=\"92.961182\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"277.954192\" xlink:href=\"#m371f10bfd5\" y=\"160.157984\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"380.64051\" xlink:href=\"#m371f10bfd5\" y=\"227.443977\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.210767\" xlink:href=\"#m371f10bfd5\" y=\"203.097729\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"295.611024\" xlink:href=\"#m371f10bfd5\" y=\"179.718681\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.846531\" xlink:href=\"#m371f10bfd5\" y=\"144.827805\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"239.763935\" xlink:href=\"#m371f10bfd5\" y=\"93.205971\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.284835\" xlink:href=\"#m371f10bfd5\" y=\"170.899763\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.533728\" xlink:href=\"#m371f10bfd5\" y=\"155.355401\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.590226\" xlink:href=\"#m371f10bfd5\" y=\"184.699843\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"247.232656\" xlink:href=\"#m371f10bfd5\" y=\"184.020852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.543544\" xlink:href=\"#m371f10bfd5\" y=\"160.067762\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.238701\" xlink:href=\"#m371f10bfd5\" y=\"153.759857\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"338.231689\" xlink:href=\"#m371f10bfd5\" y=\"183.114815\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.510215\" xlink:href=\"#m371f10bfd5\" y=\"184.842203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.702925\" xlink:href=\"#m371f10bfd5\" y=\"171.432898\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.656053\" xlink:href=\"#m371f10bfd5\" y=\"164.045756\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.705206\" xlink:href=\"#m371f10bfd5\" y=\"160.232062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.201885\" xlink:href=\"#m371f10bfd5\" y=\"141.968967\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.613243\" xlink:href=\"#m371f10bfd5\" y=\"169.839347\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.949629\" xlink:href=\"#m371f10bfd5\" y=\"157.065403\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.653834\" xlink:href=\"#m371f10bfd5\" y=\"241.020163\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"189.491384\" xlink:href=\"#m371f10bfd5\" y=\"123.106638\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.22094\" xlink:href=\"#m371f10bfd5\" y=\"173.851471\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.478024\" xlink:href=\"#m371f10bfd5\" y=\"124.154248\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"232.393371\" xlink:href=\"#m371f10bfd5\" y=\"151.580857\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"241.969571\" xlink:href=\"#m371f10bfd5\" y=\"182.17574\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"291.563043\" xlink:href=\"#m371f10bfd5\" y=\"172.338428\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.406322\" xlink:href=\"#m371f10bfd5\" y=\"112.420229\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"208.919052\" xlink:href=\"#m371f10bfd5\" y=\"147.978777\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.639274\" xlink:href=\"#m371f10bfd5\" y=\"151.296336\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.124139\" xlink:href=\"#m371f10bfd5\" y=\"171.236167\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"202.420131\" xlink:href=\"#m371f10bfd5\" y=\"98.968759\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"247.853891\" xlink:href=\"#m371f10bfd5\" y=\"105.132836\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.526384\" xlink:href=\"#m371f10bfd5\" y=\"229.869069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"400.653635\" xlink:href=\"#m371f10bfd5\" y=\"210.367083\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.884345\" xlink:href=\"#m371f10bfd5\" y=\"173.536079\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.98061\" xlink:href=\"#m371f10bfd5\" y=\"172.449896\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"423.945519\" xlink:href=\"#m371f10bfd5\" y=\"278.727622\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.732819\" xlink:href=\"#m371f10bfd5\" y=\"152.10172\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"295.81544\" xlink:href=\"#m371f10bfd5\" y=\"217.176196\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"137.910104\" xlink:href=\"#m371f10bfd5\" y=\"81.130122\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"272.764542\" xlink:href=\"#m371f10bfd5\" y=\"166.088937\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.308452\" xlink:href=\"#m371f10bfd5\" y=\"106.602894\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.969173\" xlink:href=\"#m371f10bfd5\" y=\"152.191618\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"281.106752\" xlink:href=\"#m371f10bfd5\" y=\"149.930314\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"187.082958\" xlink:href=\"#m371f10bfd5\" y=\"128.698239\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.981928\" xlink:href=\"#m371f10bfd5\" y=\"127.429512\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"204.429751\" xlink:href=\"#m371f10bfd5\" y=\"112.872994\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.790846\" xlink:href=\"#m371f10bfd5\" y=\"171.588582\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"415.550739\" xlink:href=\"#m371f10bfd5\" y=\"234.574774\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"389.252454\" xlink:href=\"#m371f10bfd5\" y=\"221.140174\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"264.92473\" xlink:href=\"#m371f10bfd5\" y=\"172.813448\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.774372\" xlink:href=\"#m371f10bfd5\" y=\"236.846461\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"226.538701\" xlink:href=\"#m371f10bfd5\" y=\"166.183884\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"433.607652\" xlink:href=\"#m371f10bfd5\" y=\"253.072369\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.032646\" xlink:href=\"#m371f10bfd5\" y=\"211.687255\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.367658\" xlink:href=\"#m371f10bfd5\" y=\"190.106298\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"208.461543\" xlink:href=\"#m371f10bfd5\" y=\"106.432344\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.604241\" xlink:href=\"#m371f10bfd5\" y=\"147.486671\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"196.34278\" xlink:href=\"#m371f10bfd5\" y=\"158.640565\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"403.956172\" xlink:href=\"#m371f10bfd5\" y=\"235.525312\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.830165\" xlink:href=\"#m371f10bfd5\" y=\"217.967478\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.109073\" xlink:href=\"#m371f10bfd5\" y=\"162.856743\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"364.708515\" xlink:href=\"#m371f10bfd5\" y=\"228.977558\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"241.501618\" xlink:href=\"#m371f10bfd5\" y=\"126.506325\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"347.771468\" xlink:href=\"#m371f10bfd5\" y=\"203.135737\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"237.25215\" xlink:href=\"#m371f10bfd5\" y=\"178.201388\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.336367\" xlink:href=\"#m371f10bfd5\" y=\"176.616059\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"228.16323\" xlink:href=\"#m371f10bfd5\" y=\"118.654597\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"188.340805\" xlink:href=\"#m371f10bfd5\" y=\"89.257654\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.623292\" xlink:href=\"#m371f10bfd5\" y=\"207.762576\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.986633\" xlink:href=\"#m371f10bfd5\" y=\"168.188535\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.977028\" xlink:href=\"#m371f10bfd5\" y=\"145.707003\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.714754\" xlink:href=\"#m371f10bfd5\" y=\"169.631114\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.491105\" xlink:href=\"#m371f10bfd5\" y=\"166.320441\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"223.579178\" xlink:href=\"#m371f10bfd5\" y=\"160.509553\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.401896\" xlink:href=\"#m371f10bfd5\" y=\"192.798303\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.257696\" xlink:href=\"#m371f10bfd5\" y=\"179.709658\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"134.993044\" xlink:href=\"#m371f10bfd5\" y=\"67.868832\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.619958\" xlink:href=\"#m371f10bfd5\" y=\"198.70067\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"303.78811\" xlink:href=\"#m371f10bfd5\" y=\"157.115244\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.310044\" xlink:href=\"#m371f10bfd5\" y=\"212.578105\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.030578\" xlink:href=\"#m371f10bfd5\" y=\"183.224836\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"479.774968\" xlink:href=\"#m371f10bfd5\" y=\"270.842484\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"376.232821\" xlink:href=\"#m371f10bfd5\" y=\"208.160208\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"187.69393\" xlink:href=\"#m371f10bfd5\" y=\"182.462665\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"371.973111\" xlink:href=\"#m371f10bfd5\" y=\"174.439442\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"424.164028\" xlink:href=\"#m371f10bfd5\" y=\"236.256351\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.30677\" xlink:href=\"#m371f10bfd5\" y=\"58.723995\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"294.240397\" xlink:href=\"#m371f10bfd5\" y=\"150.933973\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"239.050834\" xlink:href=\"#m371f10bfd5\" y=\"121.417774\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.948491\" xlink:href=\"#m371f10bfd5\" y=\"217.052617\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"186.164929\" xlink:href=\"#m371f10bfd5\" y=\"138.653978\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"481.507041\" xlink:href=\"#m371f10bfd5\" y=\"302.599129\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"175.986354\" xlink:href=\"#m371f10bfd5\" y=\"102.622685\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"356.036202\" xlink:href=\"#m371f10bfd5\" y=\"176.794887\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"373.327987\" xlink:href=\"#m371f10bfd5\" y=\"200.355619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.233854\" xlink:href=\"#m371f10bfd5\" y=\"148.587863\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"360.858049\" xlink:href=\"#m371f10bfd5\" y=\"242.31539\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.879556\" xlink:href=\"#m371f10bfd5\" y=\"186.681197\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.152664\" xlink:href=\"#m371f10bfd5\" y=\"148.599029\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"560.520173\" xlink:href=\"#m371f10bfd5\" y=\"318.019157\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"195.126969\" xlink:href=\"#m371f10bfd5\" y=\"150.992657\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"199.956379\" xlink:href=\"#m371f10bfd5\" y=\"110.091734\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"272.396556\" xlink:href=\"#m371f10bfd5\" y=\"169.271098\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"269.172996\" xlink:href=\"#m371f10bfd5\" y=\"164.667695\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.462896\" xlink:href=\"#m371f10bfd5\" y=\"59.090981\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"466.286288\" xlink:href=\"#m371f10bfd5\" y=\"253.782194\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"198.463261\" xlink:href=\"#m371f10bfd5\" y=\"83.617915\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.025254\" xlink:href=\"#m371f10bfd5\" y=\"123.387545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"141.223993\" xlink:href=\"#m371f10bfd5\" y=\"89.151871\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.188685\" xlink:href=\"#m371f10bfd5\" y=\"196.096761\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"277.670451\" xlink:href=\"#m371f10bfd5\" y=\"155.566252\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.116812\" xlink:href=\"#m371f10bfd5\" y=\"109.439915\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.340706\" xlink:href=\"#m371f10bfd5\" y=\"125.516073\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.787475\" xlink:href=\"#m371f10bfd5\" y=\"188.219968\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.460544\" xlink:href=\"#m371f10bfd5\" y=\"166.420208\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.173656\" xlink:href=\"#m371f10bfd5\" y=\"169.756378\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"325.650547\" xlink:href=\"#m371f10bfd5\" y=\"199.418481\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"467.599631\" xlink:href=\"#m371f10bfd5\" y=\"272.25011\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.930848\" xlink:href=\"#m371f10bfd5\" y=\"167.579959\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.625884\" xlink:href=\"#m371f10bfd5\" y=\"181.31852\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.780103\" xlink:href=\"#m371f10bfd5\" y=\"68.522836\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"400.260008\" xlink:href=\"#m371f10bfd5\" y=\"235.346437\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"394.01593\" xlink:href=\"#m371f10bfd5\" y=\"250.282917\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.385198\" xlink:href=\"#m371f10bfd5\" y=\"157.212838\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"367.358737\" xlink:href=\"#m371f10bfd5\" y=\"250.361284\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.24973\" xlink:href=\"#m371f10bfd5\" y=\"91.228517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"364.351814\" xlink:href=\"#m371f10bfd5\" y=\"206.175546\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"392.983488\" xlink:href=\"#m371f10bfd5\" y=\"240.668136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"183.63163\" xlink:href=\"#m371f10bfd5\" y=\"130.447739\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.996003\" xlink:href=\"#m371f10bfd5\" y=\"107.280564\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"106.212599\" xlink:href=\"#m371f10bfd5\" y=\"48.033625\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"262.914646\" xlink:href=\"#m371f10bfd5\" y=\"179.095658\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"395.898089\" xlink:href=\"#m371f10bfd5\" y=\"216.541872\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.635686\" xlink:href=\"#m371f10bfd5\" y=\"198.253131\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.757453\" xlink:href=\"#m371f10bfd5\" y=\"154.96921\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"357.814912\" xlink:href=\"#m371f10bfd5\" y=\"223.439909\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.984527\" xlink:href=\"#m371f10bfd5\" y=\"222.265138\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.022349\" xlink:href=\"#m371f10bfd5\" y=\"188.294743\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.476459\" xlink:href=\"#m371f10bfd5\" y=\"220.145503\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"158.161772\" xlink:href=\"#m371f10bfd5\" y=\"108.257645\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"214.921751\" xlink:href=\"#m371f10bfd5\" y=\"166.937806\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"213.498266\" xlink:href=\"#m371f10bfd5\" y=\"129.048085\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.7247\" xlink:href=\"#m371f10bfd5\" y=\"141.127507\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"272.418522\" xlink:href=\"#m371f10bfd5\" y=\"213.213956\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.980621\" xlink:href=\"#m371f10bfd5\" y=\"186.009338\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.238661\" xlink:href=\"#m371f10bfd5\" y=\"191.28574\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"171.133688\" xlink:href=\"#m371f10bfd5\" y=\"165.729246\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.706405\" xlink:href=\"#m371f10bfd5\" y=\"110.674392\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"211.08737\" xlink:href=\"#m371f10bfd5\" y=\"84.284419\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.629794\" xlink:href=\"#m371f10bfd5\" y=\"195.05731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.633336\" xlink:href=\"#m371f10bfd5\" y=\"147.934849\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"307.083616\" xlink:href=\"#m371f10bfd5\" y=\"220.916103\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"200.103634\" xlink:href=\"#m371f10bfd5\" y=\"156.617351\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"379.717904\" xlink:href=\"#m371f10bfd5\" y=\"239.597125\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"185.822702\" xlink:href=\"#m371f10bfd5\" y=\"148.91286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"366.170708\" xlink:href=\"#m371f10bfd5\" y=\"208.054441\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"197.741369\" xlink:href=\"#m371f10bfd5\" y=\"94.680735\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.148102\" xlink:href=\"#m371f10bfd5\" y=\"127.919307\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.880771\" xlink:href=\"#m371f10bfd5\" y=\"168.665422\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"496.858356\" xlink:href=\"#m371f10bfd5\" y=\"312.338691\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"423.682144\" xlink:href=\"#m371f10bfd5\" y=\"231.503423\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.977775\" xlink:href=\"#m371f10bfd5\" y=\"211.551126\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"190.164778\" xlink:href=\"#m371f10bfd5\" y=\"109.910318\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.900089\" xlink:href=\"#m371f10bfd5\" y=\"137.64506\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"370.284529\" xlink:href=\"#m371f10bfd5\" y=\"207.875292\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.112977\" xlink:href=\"#m371f10bfd5\" y=\"130.319442\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"179.236738\" xlink:href=\"#m371f10bfd5\" y=\"90.524154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"260.362299\" xlink:href=\"#m371f10bfd5\" y=\"174.667746\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"212.743014\" xlink:href=\"#m371f10bfd5\" y=\"158.157686\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"330.69882\" xlink:href=\"#m371f10bfd5\" y=\"243.726012\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"322.720875\" xlink:href=\"#m371f10bfd5\" y=\"179.938418\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"245.98713\" xlink:href=\"#m371f10bfd5\" y=\"153.677656\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.066598\" xlink:href=\"#m371f10bfd5\" y=\"246.822083\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.607228\" xlink:href=\"#m371f10bfd5\" y=\"190.452499\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.153075\" xlink:href=\"#m371f10bfd5\" y=\"163.563297\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"424.767804\" xlink:href=\"#m371f10bfd5\" y=\"209.724377\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.821712\" xlink:href=\"#m371f10bfd5\" y=\"198.576652\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"311.408806\" xlink:href=\"#m371f10bfd5\" y=\"195.296363\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"162.4559\" xlink:href=\"#m371f10bfd5\" y=\"134.444445\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"402.147626\" xlink:href=\"#m371f10bfd5\" y=\"229.185345\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.721044\" xlink:href=\"#m371f10bfd5\" y=\"217.471846\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.365542\" xlink:href=\"#m371f10bfd5\" y=\"170.898839\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.579249\" xlink:href=\"#m371f10bfd5\" y=\"142.450951\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"225.724737\" xlink:href=\"#m371f10bfd5\" y=\"215.236638\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.793385\" xlink:href=\"#m371f10bfd5\" y=\"159.009472\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"289.820025\" xlink:href=\"#m371f10bfd5\" y=\"118.919955\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.247763\" xlink:href=\"#m371f10bfd5\" y=\"145.885754\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"328.175621\" xlink:href=\"#m371f10bfd5\" y=\"182.031102\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"218.665235\" xlink:href=\"#m371f10bfd5\" y=\"119.74763\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.528514\" xlink:href=\"#m371f10bfd5\" y=\"95.725644\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"180.054857\" xlink:href=\"#m371f10bfd5\" y=\"109.007514\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"204.295394\" xlink:href=\"#m371f10bfd5\" y=\"140.571613\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"315.398839\" xlink:href=\"#m371f10bfd5\" y=\"135.081111\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"285.941406\" xlink:href=\"#m371f10bfd5\" y=\"187.990743\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.088573\" xlink:href=\"#m371f10bfd5\" y=\"239.855018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"214.722804\" xlink:href=\"#m371f10bfd5\" y=\"106.775544\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"430.636803\" xlink:href=\"#m371f10bfd5\" y=\"207.771031\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"288.859745\" xlink:href=\"#m371f10bfd5\" y=\"143.755448\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.471703\" xlink:href=\"#m371f10bfd5\" y=\"166.58826\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.39891\" xlink:href=\"#m371f10bfd5\" y=\"228.477297\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.878003\" xlink:href=\"#m371f10bfd5\" y=\"180.442858\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"336.473229\" xlink:href=\"#m371f10bfd5\" y=\"245.753091\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"346.03663\" xlink:href=\"#m371f10bfd5\" y=\"187.538933\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"184.794525\" xlink:href=\"#m371f10bfd5\" y=\"147.647866\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.617585\" xlink:href=\"#m371f10bfd5\" y=\"174.854372\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"346.541143\" xlink:href=\"#m371f10bfd5\" y=\"220.531801\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.584825\" xlink:href=\"#m371f10bfd5\" y=\"204.564465\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"227.367574\" xlink:href=\"#m371f10bfd5\" y=\"178.127862\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"313.170389\" xlink:href=\"#m371f10bfd5\" y=\"195.331526\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.841994\" xlink:href=\"#m371f10bfd5\" y=\"178.844622\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.669413\" xlink:href=\"#m371f10bfd5\" y=\"120.207588\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"216.381611\" xlink:href=\"#m371f10bfd5\" y=\"130.980561\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"176.311137\" xlink:href=\"#m371f10bfd5\" y=\"134.862419\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"425.376828\" xlink:href=\"#m371f10bfd5\" y=\"243.320464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"292.538\" xlink:href=\"#m371f10bfd5\" y=\"203.71488\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"382.215431\" xlink:href=\"#m371f10bfd5\" y=\"199.744585\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"195.160092\" xlink:href=\"#m371f10bfd5\" y=\"136.846334\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"384.172697\" xlink:href=\"#m371f10bfd5\" y=\"179.541942\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"236.640019\" xlink:href=\"#m371f10bfd5\" y=\"169.485395\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"373.938964\" xlink:href=\"#m371f10bfd5\" y=\"209.924755\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"210.055289\" xlink:href=\"#m371f10bfd5\" y=\"143.938899\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.707931\" xlink:href=\"#m371f10bfd5\" y=\"146.627242\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"334.333105\" xlink:href=\"#m371f10bfd5\" y=\"215.943974\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.921465\" xlink:href=\"#m371f10bfd5\" y=\"212.575339\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"264.188314\" xlink:href=\"#m371f10bfd5\" y=\"156.872929\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"150.890943\" xlink:href=\"#m371f10bfd5\" y=\"92.002352\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"154.521414\" xlink:href=\"#m371f10bfd5\" y=\"121.358243\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"345.425764\" xlink:href=\"#m371f10bfd5\" y=\"200.880016\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"383.793403\" xlink:href=\"#m371f10bfd5\" y=\"231.408941\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.180103\" xlink:href=\"#m371f10bfd5\" y=\"193.995429\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.839304\" xlink:href=\"#m371f10bfd5\" y=\"143.897763\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.169004\" xlink:href=\"#m371f10bfd5\" y=\"217.447345\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"170.379697\" xlink:href=\"#m371f10bfd5\" y=\"126.562098\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"387.121736\" xlink:href=\"#m371f10bfd5\" y=\"175.120895\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"412.359484\" xlink:href=\"#m371f10bfd5\" y=\"219.653478\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"208.593903\" xlink:href=\"#m371f10bfd5\" y=\"170.823926\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.140534\" xlink:href=\"#m371f10bfd5\" y=\"200.779717\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"259.216788\" xlink:href=\"#m371f10bfd5\" y=\"180.899757\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.577161\" xlink:href=\"#m371f10bfd5\" y=\"118.370856\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.474008\" xlink:href=\"#m371f10bfd5\" y=\"189.874641\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"282.227646\" xlink:href=\"#m371f10bfd5\" y=\"205.22184\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"404.574642\" xlink:href=\"#m371f10bfd5\" y=\"239.997967\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"180.451094\" xlink:href=\"#m371f10bfd5\" y=\"135.872573\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"234.730073\" xlink:href=\"#m371f10bfd5\" y=\"100.618839\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"498.985291\" xlink:href=\"#m371f10bfd5\" y=\"245.181051\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"277.241605\" xlink:href=\"#m371f10bfd5\" y=\"177.818306\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.53218\" xlink:href=\"#m371f10bfd5\" y=\"108.838358\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"231.533483\" xlink:href=\"#m371f10bfd5\" y=\"165.263795\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"343.902715\" xlink:href=\"#m371f10bfd5\" y=\"202.647128\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"290.447194\" xlink:href=\"#m371f10bfd5\" y=\"191.021286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"406.759732\" xlink:href=\"#m371f10bfd5\" y=\"188.985954\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.660811\" xlink:href=\"#m371f10bfd5\" y=\"94.120591\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"131.990006\" xlink:href=\"#m371f10bfd5\" y=\"41.088651\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.054316\" xlink:href=\"#m371f10bfd5\" y=\"147.693163\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"297.588337\" xlink:href=\"#m371f10bfd5\" y=\"162.432567\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"393.619312\" xlink:href=\"#m371f10bfd5\" y=\"188.560217\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"431.464336\" xlink:href=\"#m371f10bfd5\" y=\"264.138512\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.904687\" xlink:href=\"#m371f10bfd5\" y=\"220.334226\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.141527\" xlink:href=\"#m371f10bfd5\" y=\"185.077137\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"251.726203\" xlink:href=\"#m371f10bfd5\" y=\"132.730451\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"320.55868\" xlink:href=\"#m371f10bfd5\" y=\"256.86807\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"207.779638\" xlink:href=\"#m371f10bfd5\" y=\"102.72782\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"338.953682\" xlink:href=\"#m371f10bfd5\" y=\"215.113867\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"242.921083\" xlink:href=\"#m371f10bfd5\" y=\"166.3872\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"176.307724\" xlink:href=\"#m371f10bfd5\" y=\"78.539883\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.89468\" xlink:href=\"#m371f10bfd5\" y=\"208.869233\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.197969\" xlink:href=\"#m371f10bfd5\" y=\"165.937065\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"467.879487\" xlink:href=\"#m371f10bfd5\" y=\"293.856513\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"302.607935\" xlink:href=\"#m371f10bfd5\" y=\"195.829891\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"267.914668\" xlink:href=\"#m371f10bfd5\" y=\"115.780978\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.648792\" xlink:href=\"#m371f10bfd5\" y=\"140.542814\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.738935\" xlink:href=\"#m371f10bfd5\" y=\"215.19412\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc639406491\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"38.393451\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −3 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(31.022357 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.591259\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(115.220165 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.789066\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(199.417972 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"290.986873\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(287.805623 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.184681\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(372.003431 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"459.382488\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2 -->\n      <g transform=\"translate(456.201238 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.580296\" xlink:href=\"#mc639406491\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 3 -->\n      <g transform=\"translate(540.399046 347.958438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2e62eddb23\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m2e62eddb23\" y=\"290.778467\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 294.577686)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m2e62eddb23\" y=\"227.82651\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(15.579688 231.625729)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m2e62eddb23\" y=\"164.874554\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(15.579688 168.673773)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m2e62eddb23\" y=\"101.922597\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(9.217188 105.721816)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.942188\" xlink:href=\"#m2e62eddb23\" y=\"38.970641\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15 -->\n      <g transform=\"translate(9.217188 42.76986)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 28.942188 333.36 \nL 28.942188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 586.942188 333.36 \nL 586.942188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 28.942187 333.36 \nL 586.942188 333.36 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 28.942187 7.2 \nL 586.942188 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0016759381\">\n   <rect height=\"326.16\" width=\"558\" x=\"28.942188\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT2NxLvDV69E",
        "colab_type": "text"
      },
      "source": [
        "The plotting function `plt` as well as the `use_svg_display` and `set_figsize` functions are defined in the `d2l` package. Now that you know how to make plots yourself, we will call `d2l.plt` directly for future plotting. To print the vector diagram and set its size, we only need to call `d2l.set_figsize()` before plotting, because `plt` is a global variable in the `d2l` package.\n",
        "\n",
        "\n",
        "## Reading Data\n",
        "\n",
        "Recall that training models, consists of making multiple passes over the dataset, grabbing one mini-batch of examples at a time and using them to update our model. Since this process is so fundamental to training machine learning algortihms, we need a utility for shuffling the data and accessing in mini-batches.\n",
        "\n",
        "In the following code, we define a `data_iter` function to demonstrate one possible implementation of this functionality.\n",
        "The function takes a batch size, a design matrix containing the features,\n",
        "and a vector of labels, yielding minibatches of size `batch_size`,\n",
        "each consisting of a tuple of features and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "id": "tTaDVXqFV69F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function has been saved in the d2l package for future use\n",
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        j = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
        "        yield features[j], labels[j]\n",
        "        # The “take” function will then return the corresponding element based\n",
        "        # on the indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h11ZXIGrV69G",
        "colab_type": "text"
      },
      "source": [
        "In general, note that we want to use reasonably sized minibatches to take advantage of the GPU hardware, which excels at parallelizing operations. Because each example can be fed through our models in parallel and the gradient of the loss function for each example can also be taken in parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take to process just a single example.\n",
        "\n",
        "To build some intuition, let's read and print the first small batch of data examples. The shape of the features in each mini-batch tells us both the mini-batch size and the number of input features. Likewise, our mini-batch of labels will have a shape given by `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "id": "ZKRtlHWvV69G",
        "colab_type": "code",
        "outputId": "c13f4c47-2a9e-44db-d683-f8e0c0a7c8f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print(X, y)\n",
        "    break"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5929,  0.3397],\n",
            "        [-1.0919, -0.8626],\n",
            "        [-0.5295,  0.9464],\n",
            "        [ 0.6138, -1.1130],\n",
            "        [ 0.1486,  1.1671],\n",
            "        [-0.5697,  0.9969],\n",
            "        [-0.3902, -0.4875],\n",
            "        [ 2.1054, -1.8884],\n",
            "        [ 0.7835,  0.0846],\n",
            "        [ 0.1712,  0.9863]]) tensor([ 1.8476,  4.9383, -0.0707,  9.2313,  0.5311, -0.3344,  5.0615, 14.8318,\n",
            "         5.4780,  1.1952])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktNR7eFV69I",
        "colab_type": "text"
      },
      "source": [
        "It should be no surprise that as we run the iterator,\n",
        "we will obtain distinct minibatches each time\n",
        "until all the data has been exhausted (try this).\n",
        "While the iterator implemented above is good for didactic purposes,\n",
        "it is inefficient in ways that might get us in trouble on real problems.\n",
        "For example, it requires that we load all data in memory\n",
        "and that we perform a lot of random memory access.\n",
        "The built-in iterators implemented in torch\n",
        "are considerably efficient and they can deal\n",
        "both with data stored on file and data fed via a data stream.\n",
        "\n",
        "## Initialize Model Parameters\n",
        "\n",
        "Before we can begin optimizing our model's parameters by gradient descent,\n",
        "we need to have some parameters in the first place.\n",
        "In the following code, we initialize weights by sampling\n",
        "random numbers from a normal distribution with mean 0\n",
        "and a standard deviation of 0.01, setting the bias $b$ to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "id": "036okhMIV69I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.zeros(size=(num_inputs, 1)).normal_(std=0.01)\n",
        "b = torch.zeros(size=(1,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJSuTfK-V69K",
        "colab_type": "text"
      },
      "source": [
        "Now that we have initialized our parameters,\n",
        "our next task is to update them until they fit our data sufficiently well.\n",
        "Each update will require taking the gradient\n",
        "(a multi-dimensional derivative)\n",
        "of our loss function with respect to the parameters.\n",
        "Given this gradient, we will update each parameter\n",
        "in the direction that reduces the loss.\n",
        "\n",
        "Since nobody wants to compute gradients explicitly\n",
        "(this is tedious and error prone),\n",
        "we use automatic differentiation to compute the gradient.\n",
        "See :numref:`chapter_autograd`\n",
        "for more details.\n",
        "Recall from the autograd chapter\n",
        "that in order for `autograd` to know\n",
        "that it should store a gradient for our parameters,\n",
        "we need to invoke the `attach_grad` function,\n",
        "allocating memory to store the gradients that we plan to take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "732atNBiV69K",
        "colab_type": "code",
        "outputId": "0cd9afba-0048-492b-8c40-78e39ddc99b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w.requires_grad_(True)\n",
        "b.requires_grad_(True)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS6oDhC0V69L",
        "colab_type": "text"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "Next, we must define our model,\n",
        "relating its inputs and parameters to its outputs.\n",
        "Recall that to calculate the output of the linear model,\n",
        "we simply take the matrix-vector dot product\n",
        "of the examples $\\mathbf{X}$ and the models weights $w$,\n",
        "and add the offset $b$ to each example.\n",
        "Note that below `torch.matmul(X, w)` is a vector and `b` is a scalar.\n",
        "Recall that when we add a vector and a scalar,\n",
        "the scalar is added to each component of the vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "LJyCyNPcV69M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function has been saved in the d2l package for future use\n",
        "def linreg(X, w, b):\n",
        "    return torch.matmul(X, w) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZEpep1PV69N",
        "colab_type": "text"
      },
      "source": [
        "## Define the Loss Function\n",
        "\n",
        "Since updating our model requires taking the gradient of our loss function,\n",
        "we ought to define the loss function first.\n",
        "Here we will use the squared loss function\n",
        "as described in the previous section.\n",
        "In the implementation, we need to transform the true value `y` into the predicted value's shape `y_hat`.\n",
        "The result returned by the following function\n",
        "will also be the same as the `y_hat` shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "10"
        },
        "id": "_BaH6JghV69O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function has been saved in the d2l package for future use\n",
        "def squared_loss(y_hat, y):\n",
        "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3y07A6V69P",
        "colab_type": "text"
      },
      "source": [
        "## Define the Optimization Algorithm\n",
        "\n",
        "As we discussed in the previous section,\n",
        "linear regression has a closed-form solution.\n",
        "However, this isn't a book about linear regression,\n",
        "its a book about deep learning.\n",
        "Since none of the other models that this book introduces\n",
        "can be solved analytically, we will take this opportunity to introduce your first working example of stochastic gradient descent (SGD).\n",
        "\n",
        "\n",
        "At each step, using one batch randomly drawn from our dataset,\n",
        "we'll estimate the gradient of the loss with respect to our parameters.\n",
        "Then, we'll update our parameters a small amount\n",
        "in the direction that reduces the loss.\n",
        "Assuming that the gradient has already been calculated,\n",
        "each parameter (`param`) already has its gradient stored in `param.grad`.\n",
        "The following code applies the SGD update,\n",
        "given a set of parameters, a learning rate, and a batch size.\n",
        "The size of the update step is determined by the learning rate `lr`.\n",
        "Because our loss is calculated as a sum over the batch of examples,\n",
        "we normalize our step size by the batch size (`batch_size`),\n",
        "so that the magnitude of a typical step size\n",
        "doesn't depend heavily on our choice of the batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "id": "zNAQ-BXaV69Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function has been saved in the d2l package for future use\n",
        "def sgd(params, lr, batch_size): \n",
        "    for param in params:\n",
        "        param.data.sub_(lr*param.grad/batch_size) #??\n",
        "        param.grad.data.zero_() #??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4rhsMDtV69R",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have all of the parts in place,\n",
        "we are ready to implement the main training loop.\n",
        "It is crucial that you understand this code\n",
        "because you will see training loops that are nearly identical to this one\n",
        "over and over again throughout your career in deep learning.\n",
        "\n",
        "In each iteration, we will grab minibatches of models,\n",
        "first passing them through our model to obtain a set of predictions.\n",
        "After calculating the loss, we will call the `backward` function\n",
        "to backpropagate through the network, storing the gradients\n",
        "with respect to each parameter in its corresponding `.grad` attribute.\n",
        "Finally, we will call the optimization algorithm `sgd`\n",
        "to update the model parameters.\n",
        "Since we previously set the batch size `batch_size` to 10,\n",
        "the loss shape `l` for each small batch is (10, 1).\n",
        "\n",
        "In summary, we'll execute the following loop:\n",
        "\n",
        "* Initialize parameters $(\\mathbf{w}, b)$\n",
        "* Repeat until done\n",
        "    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{\\mathcal{B}} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^i, y^i, \\mathbf{w}, b)$\n",
        "    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
        "\n",
        "In the code below, `l` is a vector of the losses\n",
        "for each example in the minibatch.\n",
        "Because `l` is not a scalar variable,\n",
        "running `l.backward()` adds together the elements in `l`\n",
        "to obtain the new variable and then calculates the gradient.\n",
        "\n",
        "In each epoch (a pass through the data),\n",
        "we will iterate through the entire dataset\n",
        "(using the `data_iter` function) once\n",
        "passing through every examples in the training dataset\n",
        "(assuming the number of examples is divisible by the batch size).\n",
        "The number of epochs `num_epochs` and the learning rate `lr` are both hyper-parameters, which we set here to $3$ and $0.03$, respectively. Unfortunately, setting hyper-parameters is tricky\n",
        "and requires some adjustment by trial and error.\n",
        "We elide these details for now but revise them\n",
        "later in\n",
        ":numref:`chapter_optimization`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "id": "naEUaVEtV69R",
        "colab_type": "code",
        "outputId": "81ef72f1-5493-494a-be87-11649137a49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "lr = 0.03  # Learning rate\n",
        "num_epochs = 3  # Number of iterations\n",
        "net = linreg  # Our fancy linear model\n",
        "loss = squared_loss  # 0.5 (y-y')^2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Assuming the number of examples can be divided by the batch size, all\n",
        "    # the examples in the training data set are used once in one epoch\n",
        "    # iteration. The features and tags of mini-batch examples are given by X\n",
        "    # and y respectively\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "        l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
        "        l.mean().backward()  # Compute gradient on l with respect to [w,b]\n",
        "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
        "    with torch.no_grad():\n",
        "        train_l = loss(net(features, w, b), labels)\n",
        "        print('epoch %d, loss %f' % (epoch + 1, train_l.mean().numpy()))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 9.158298\n",
            "epoch 2, loss 4.943856\n",
            "epoch 3, loss 2.668999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv1OiKd6V69T",
        "colab_type": "text"
      },
      "source": [
        "In this case, because we used synthetic data (that we synthesized ourselves!),\n",
        "we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "nNMUA2uKV69T",
        "colab_type": "code",
        "outputId": "6e07bed1-1979-4db6-e5bb-a26e49b21baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Error in estimating w', true_w - w.reshape(true_w.shape))\n",
        "print('Error in estimating b', true_b - b)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in estimating w tensor([ 0.8165, -1.3380], grad_fn=<SubBackward0>)\n",
            "Error in estimating b tensor([1.6589], grad_fn=<RsubBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVvha45V69U",
        "colab_type": "text"
      },
      "source": [
        "Note that we should not take it for granted\n",
        "that we are able to recover the parameters accurately.\n",
        "This only happens for a special category problems:\n",
        "strongly convex optimization problems with 'enough' data to ensure\n",
        "that the noisy samples allow us to recover the underlying dependency.\n",
        "In most cases this is *not* the case.\n",
        "In fact, the parameters of a deep network are rarely the same (or even close) between two different runs, unless all conditions are identical,\n",
        "including the order in which the data is traversed.\n",
        "However, in machine learning we are typically less concerned\n",
        "with recovering true underlying parameters,\n",
        "and more concerned with parameters that lead to accurate prediction.\n",
        "Fortunately, even on difficult optimization problems,\n",
        "that stochastic gradient descent can often lead to remarkably good solutions,\n",
        "due in part to the fact that for the models we will be working with,\n",
        "there exist many sets of parameters that work well.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We saw how a deep network can be implemented\n",
        "and optimized from scratch, using just `torch.Tensor` and `autograd`,\n",
        "without any need for defining layers, fancy optimizers, etc.\n",
        "This only scratches the surface of what is possible.\n",
        "In the following sections, we will describe additional models\n",
        "based on the concepts that we have just introduced\n",
        "and learn how to implement them more concisely.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ai3iZkPnHaR",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Given a batch size b and a certain number of training examples n, how often are the parameters updated in each epoch (the number of epochs is num_epochs)? Compute the number of updates and return it from the following function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4Av7c6nIzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_num_updates(num_epochs, n, b):\n",
        "  num_updates = 0\n",
        "  ## Edge case is not covered here, \n",
        "  #if(num_epochs == 0):\n",
        "  num_updates =  n/b\n",
        "  #else:\n",
        "   # num_updates = num_epochs * (n/b)\n",
        "  ## end of function\n",
        "  return num_updates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QWkLqJ5jod0",
        "colab_type": "code",
        "outputId": "f191b994-ddea-49d7-dd90-32e44afba4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#testing code\n",
        "compute_num_updates(1,3000,25)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zya6Txj0oXBS",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Deep learning is all about iterating and looking for the best loss for your application. Another loss function is the *L1* loss, which gives comparatively more importance to small errors and less for larger errors. Implement and return the *L1* loss, given `y` and `y_hat`. (Hint: assume both lists have the same shape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCpEUt2zo9BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l1_loss(y, y_hat):\n",
        "  ## write your code here\n",
        "   return torch.sum(torch.abs(y- y_hat))\n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNCFnRfSV69U",
        "colab_type": "text"
      },
      "source": [
        "# Concise Implementation of Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66bS8p98V69V",
        "colab_type": "text"
      },
      "source": [
        "The surge of deep learning has inspired the development of a variety of mature software frameworks, that\n",
        "automate much of the repetitive work of implementing deep learning models. In the previous section we\n",
        "relied only on NDarray for data storage and linear algebra and the auto-differentiation capabilities in the\n",
        "autograd package. In practice, because many of the more abstract operations, e.g. data iterators, loss\n",
        "functions, model architectures, and optimizers, are so common, deep learning libraries will give us library\n",
        "functions for these as well.\n",
        "\n",
        "We have used DataLoader to load the MNIST dataset in Section 4.5. In this section, we will learn how we can\n",
        "implement the linear regression model in Section 5.2 much more concisely with DataLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQYUjSg5V69V",
        "colab_type": "text"
      },
      "source": [
        "##  Generating Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFr3mZuWV69V",
        "colab_type": "text"
      },
      "source": [
        "To start, we will generate the same data set as that used in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd7Ej51rV69V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"generate y = X w + b + noise\"\"\"\n",
        "    X = np.random.normal(scale=1, size=(num_examples, len(w)))\n",
        "    y = np.dot(X, w) + b\n",
        "    y += np.random.normal(scale=0.01, size=y.shape)\n",
        "    X=torch.from_numpy(X).float()\n",
        "    y=torch.from_numpy( y).float().reshape(-1,1)\n",
        "    return X,y\n",
        "true_w = torch.Tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su3r2_T9V69W",
        "colab_type": "text"
      },
      "source": [
        "##  Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgUmmq5XV69X",
        "colab_type": "text"
      },
      "source": [
        "Rather than rolling our own iterator, we can call upon DataLoader module to read data. The first step will be to instantiate an ArrayDataset, which takes in one or more NDArrays as arguments. Here, we pass in features and\n",
        "labels as arguments. Next, we will use the ArrayDataset to instantiate a DataLoader, which also requires\n",
        "that we specify a batch_size and specify a Boolean value shuffle indicating whether or not we want the\n",
        "DataLoader to shuffle the data on each epoch (pass through the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUkg6xYAV69X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset=TensorDataset(*(features,labels))\n",
        "    dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)\n",
        "    return dataloader\n",
        "batch_size =10\n",
        "data_iter = load_array((features, labels), batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj_FSE1JV69Y",
        "colab_type": "text"
      },
      "source": [
        "Now we can use data_iter in much the same way as we called the data_iter function in the previous\n",
        "section. To verify that it’s working, we can read and print the first mini-batch of instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDc4a6C0V69Y",
        "colab_type": "code",
        "outputId": "da93a481-d779-4492-b3bf-3e868349e92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "for X, y in data_iter:\n",
        "    print(X)\n",
        "    print(y)\n",
        "    break"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0785, -1.4960],\n",
            "        [-0.0339,  0.8784],\n",
            "        [ 0.8951, -0.1676],\n",
            "        [-1.4306, -1.5102],\n",
            "        [ 1.4581, -0.8288],\n",
            "        [ 1.0428, -1.1541],\n",
            "        [ 1.0670,  1.9533],\n",
            "        [-0.4586,  0.4199],\n",
            "        [-0.1341, -2.2937],\n",
            "        [-1.5932,  1.7815]])\n",
            "tensor([[ 9.4553],\n",
            "        [ 1.1455],\n",
            "        [ 6.5742],\n",
            "        [ 6.4717],\n",
            "        [ 9.9324],\n",
            "        [10.2104],\n",
            "        [-0.3171],\n",
            "        [ 1.8716],\n",
            "        [11.7359],\n",
            "        [-5.0466]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsPxeKISV69Z",
        "colab_type": "text"
      },
      "source": [
        "##  Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ief1DBgzV69Z",
        "colab_type": "text"
      },
      "source": [
        "When we implemented linear regression from scratch in the previous section, we had to define the model\n",
        "parameters and explicitly write out the calculation to produce output using basic linear algebra operations.\n",
        "You should know how to do this. But once your models get more complex, even qualitatively simple changes\n",
        "to the model might result in many low-level changes.\n",
        "\n",
        "We import torch.nn as nn .For standard operations, we can use nn's predefined layers, which allow us to focus especially on thelayers used to construct the model rather than having to focus on the implementation.\n",
        "To define a linear model, we first import the nn module, which defines a large number of neural network\n",
        "layers (note that “nn” is an abbreviation for neural networks). We will first define a model variable net,\n",
        "which is a Sequential instance. In nn, a Sequential instance can be regarded as a container that\n",
        "concatenates the various layers in sequence. When input data is given, each layer in the container will be\n",
        "calculated in order, and the output of one layer will be the input of the next layer. In this example, since\n",
        "our model consists of only one layer, we do not really need Sequential. But since nearly all of our future\n",
        "models will involve multiple layers, let’s get into the habit early.\n",
        "Recall the architecture of a single layer network. The layer is fully connected since it connects all inputs\n",
        "with all outputs by means of a matrix-vector multiplication. In nn, the fully-connected layer is defined\n",
        "in the Linear class. Since we only want to generate a single scalar output,we set that number to 1.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1-8TFMQ8pA1p4fpRTILCFXv_3X5a5RkST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5f6ul-cV69b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegressionModel(torch.nn.Module): \n",
        "    def __init__(self): \n",
        "        super(LinearRegressionModel, self).__init__() \n",
        "        self.layer1 = torch.nn.Linear(2, 1, bias=True)\n",
        "    def forward(self, x): \n",
        "        y_pred = self.layer1(x)\n",
        "        return y_pred \n",
        "    \n",
        "net = LinearRegressionModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KSRca5kV69c",
        "colab_type": "text"
      },
      "source": [
        "##  Initialize Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AefztaZQV69c",
        "colab_type": "text"
      },
      "source": [
        "Before using net, we need to initialize the model parameters, such as the weights and biases in the linear\n",
        "regression model. we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01.\n",
        "The bias parameter will be initialized to zero by default. Both weight and bias will be attached with\n",
        "gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu44i8J6V69c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.layer1.weight.data=torch.Tensor(np.random.normal(size=(1,2),scale=0.01,loc=0))\n",
        "net.layer1.bias.data=torch.Tensor([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSPNDetZV69d",
        "colab_type": "text"
      },
      "source": [
        "The code above looks straightforward but in reality something quite strange is happening here. We are\n",
        "initializing parameters for a network even though we haven’t yet told nn how many dimensions the input\n",
        "will have. It might be 2 as in our example or it might be 2,000, so we couldn’t just preallocate enough space\n",
        "to make it work.\n",
        "nn let’s us get away with this because behind the scenes, the initialization is deferred until the first time\n",
        "that we attempt to pass data through our network. Just be careful to remember that since the parameters\n",
        "have not been initialized yet we cannot yet manipulate them in any way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zB4BdaV69d",
        "colab_type": "text"
      },
      "source": [
        "##  Define the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDfQE8UwV69d",
        "colab_type": "text"
      },
      "source": [
        "In nn, there are many loss modules that defines various loss functions and we will directly use its implementation of squared loss (MSELoss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41046gF0V69e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.MSELoss() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-Ru3BRV69e",
        "colab_type": "text"
      },
      "source": [
        "## Define the Optimization Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Lbt1NsV69f",
        "colab_type": "text"
      },
      "source": [
        "Not surpisingly, we aren’t the first people to implement mini-batch stochastic gradient descent, and thus\n",
        "torch supports SGD alongside a number of variations on this algorithm through its Trainer class. When\n",
        "we instantiate the Trainer, we’ll specify the parameters to optimize over (obtainable from our net via net.parameters()), the optimization algortihm we wish to use (sgd), and a dictionary of hyper-parameters\n",
        "required by our optimization algorithm. SGD just requires that we set the value learning_rate, (here we\n",
        "set it to 0.03).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDfVjP18V69f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr = 0.03) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYjH6byaV69g",
        "colab_type": "text"
      },
      "source": [
        "##  Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QSo_-4V69g",
        "colab_type": "text"
      },
      "source": [
        "You might have noticed that expressing our model through torch requires comparatively few lines of code.\n",
        "We didn’t have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. Once we start working with much more complex models, the benefits of relying on torch\n",
        "abstractions will grow considerably. But once we have all the basic pieces in place, the training loop itself strikingly similar to what we did when implementing everything from scratch.\n",
        "To refresh your memory: for some number of epochs, we’ll make a complete pass over the dataset\n",
        "(train_data), grabbing one mini-batch of inputs and corresponding ground-truth labels at a time. \n",
        "\n",
        "For\n",
        "each batch, we’ll go through the following ritual:\n",
        "\n",
        "• Generate predictions by calling net(X) and calculate the loss l (the forward pass).\n",
        "\n",
        "• Calculate gradients by calling l.backward() (the backward pass).\n",
        "\n",
        "• Update the model parameters by invoking our SGD optimizer (note that trainer already knows which parameters to optimize over, so we just need to pass in the batch size.\n",
        "\n",
        "For good measure, we compute the loss after each epoch and print it to monitor progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofqcGmNPV69g",
        "colab_type": "code",
        "outputId": "8aed81b0-7c04-48f0-9af4-01aa7edb38fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs): \n",
        "    for X,y in data_iter:\n",
        "        l=loss(net(X) ,y)\n",
        "        trainer.zero_grad()  # gradient is reset to zero after each stochastic gradient descent iteration\n",
        "        l.backward() \n",
        "        trainer.step() \n",
        "    l_epoch = loss(net(features), labels) \n",
        "    print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.09838234633207321\n",
            "epoch 2, loss 0.10901139676570892\n",
            "epoch 3, loss 0.10704496502876282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KTBoSvKmhJ_",
        "colab_type": "text"
      },
      "source": [
        "The model parameters we have learned and the actual model parameters are compared as below. We get\n",
        "the layer we need from the net and access its weight (weight) and bias (bias). The parameters we have\n",
        "learned and the actual parameters are very close."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Od4letZV69i",
        "colab_type": "code",
        "outputId": "3873a708-4b12-4b37-ccbe-cf6af4c320c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "w = list(net.parameters())[0][0]\n",
        "print('Error in estimating w', true_w.reshape(w.shape) - w)\n",
        "b = list(net.parameters())[1][0]\n",
        "print('Error in estimating b', true_b - b)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in estimating w tensor([-0.0010, -0.0033], grad_fn=<SubBackward0>)\n",
            "Error in estimating b tensor(0.0017, grad_fn=<RsubBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJLq-ainV69h",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: You probably have asked yourself what zero_grad() is good for. It makes sure that the gradient is reset to zero after each stochastic gradient descent iteration. Without it, you would end up summing the gradients. As an exercise, wrap above code into a function and transform it to use batch gradient descent instead of mini-batch gradient descent. Return the loss after a single epoch, after updating the parameters. (Hint: You do not need a data_iter. You need to call the loss function two times)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8zXJ7B-mmY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gradient_descent(features, labels):\n",
        "  ## write your code here\n",
        "        l=loss(net(features) ,labels)\n",
        "        #print(l)\n",
        "        trainer.zero_grad()  # gradient is reset to zero after each gradient descent iteration\n",
        "        l.backward() \n",
        "        trainer.step() \n",
        "        return loss(net(features) ,labels)\n",
        "    \n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSBdek2ITTw4",
        "colab_type": "code",
        "outputId": "4e206b07-17bf-4365-f0c4-e82ea8020ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Test Batch gradient descent\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs): \n",
        "    l_epoch = batch_gradient_descent(features,labels)\n",
        "    print('epoch {}, loss {}'.format(epoch+1, l_epoch))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 42.35150909423828\n",
            "epoch 2, loss 152960.890625\n",
            "epoch 3, loss 555480768.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZF0Wte7RfsG",
        "colab_type": "text"
      },
      "source": [
        "**in a batch gradient descent you process the entire training set in one iteration. Whereas, in a mini-batch gradient descent you process a small subset of the training set in each iteration.\n",
        "Also compare stochastic gradient descent, where you process a single example from the training set in each iteration.**"
      ]
    }
  ]
}
