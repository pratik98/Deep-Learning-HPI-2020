{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "week4_804861.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkD79Wpw2wx",
        "colab_type": "text"
      },
      "source": [
        "# Softmax Regression\n",
        "\n",
        "\n",
        "In the last two chapters, we worked through implementations\n",
        "linear regression, building everything from scratch and again using pytorch to automate the most repetitive work.\n",
        "\n",
        "Regression is the hammer we reach for\n",
        "when we want to answer *how much?* or *how many?* questions.\n",
        "If you want to predict the number of dollars (the *price*)\n",
        "at which a house will be sold,\n",
        "or the number of wins a baseball team might have,\n",
        "or the number of days that a patient\n",
        "will remain hospitalized before being discharged,\n",
        "then you're probably looking for a regression model.\n",
        "\n",
        "In practice, we're more often interested in classification:\n",
        "asking not *how much* but *which one*.\n",
        "\n",
        "* Does this email belong in the spam folder or the inbox?\n",
        "* Is this customer more likely *to sign up* or *not to sign up* for a subscription service?\n",
        "* Does this image depict a donkey, a dog, a cat, or a rooster?\n",
        "* Which movie is user most likely to watch next?\n",
        "\n",
        "Colloquially, we use the word *classification* to describe two subtly different problems: (i) those where we are interested only in *hard* assignments of examples to categories, and (ii) those where we wish to make *soft assignments*, i.e., to assess the *probability* that each category applies. One reason why the distinction between these tasks gets blurred is because most often, even when we only care about hard assignments, we still use models that make soft assignments.\n",
        "\n",
        "\n",
        "## Classification Problems\n",
        "\n",
        "To get our feet wet, let's start off with a somewhat contrived image classification problem. Here, each input will be a grayscale 2-by-2 image. We can represent each pixel location as a single scalar, representing each image with four features $x_1, x_2, x_3, x_4$. Further, let's assume that each image belongs to one among the categories \"cat\", \"chicken\" and \"dog\".\n",
        "\n",
        "First, we have to choose how to represent the labels. We have two obvious choices. Perhaps the most natural impulse would be to choose $y \\in \\{1, 2, 3\\}$, where the integers represent {dog, cat, chicken} respectively. This is a great way of *storing* such information on a computer.\n",
        "If the categories had some natural ordering among them, say if we were trying to predict {baby, child, adolescent, adult}, then it might even make sense to cast this problem as a regression and keep the labels in this format.\n",
        "\n",
        "But general classification problems do not come with natural orderings among the classes. To deal with problems like this, statisticians invented an alternative way to represent categorical data: the one hot encoding. Here we have a vector with one component for every possible category. For a given instance, we set the component corresponding to *its category* to 1, and set all other components to 0.\n",
        "\n",
        "$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}$$\n",
        "\n",
        "In our case, $y$ would be a three-dimensional vector, with $(1,0,0)$ corresponding to \"cat\", $(0,1,0)$ to \"chicken\" and $(0,0,1)$ to \"dog\".\n",
        "\n",
        "### Network Architecture\n",
        "\n",
        "In order to estimate multiple classes, we need a model with multiple outputs, one per category. This is one of the main differences between classification and regression models. To address classification with linear models, we will need as many linear functions as we have outputs. Each output will correspond to its own linear function. In our case, since we have 4 features and 3 possible output categories, we will need 12 scalars to represent the weights,  ($w$ with subscripts) and 3 scalars to represent the biases ($b$ with subscripts). We compute these three outputs, $o_1, o_2$, and $o_3$, for each input:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\\\\n",
        "o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\\\\n",
        "o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We can depict this calculation with the neural network diagram below. Just as in linear regression, softmax regression is also a single-layer neural network. And since the calculation of each output, $o_1, o_2$, and $o_3$, depends on all inputs, $x_1$, $x_2$, $x_3$, and $x_4$, the output layer of softmax regression can also be described as fully connected layer.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=120yYCvUv0WQYjrVqtdJjIU0xy2CoEu0g)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5GRboDSw2w5",
        "colab_type": "text"
      },
      "source": [
        "Softmax regression is a single-layer neural network.  \n",
        "\n",
        "\n",
        "### Softmax Operation\n",
        "\n",
        "To express the model more compactly, we can use linear algebra notation. In vector form, we arrive at $\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$, a form better suited both for mathematics, and for writing code. Note that we have gathered all of our weights into a $3\\times4$ matrix and that for a given example $\\mathbf{x}$ our outputs are given by a matrix vector product of our weights by our inputs plus our biases $\\mathbf{b}$.\n",
        "\n",
        "\n",
        "If we are interested in hard classifications, we need to convert these outputs into a discrete prediction. One straightforward way to do this is to treat the output values $o_i$ as the relative confidence levels that the item belongs to each category $i$. Then we can choose the class with the largest output value as our prediction $\\operatorname*{argmax}_i o_i$. For example, if $o_1$, $o_2$, and $o_3$ are 0.1, 10, and 0.1, respectively, then we predict category 2, which represents \"chicken\".\n",
        "\n",
        "However, there are a few problems with using the output from the output layer directly. First, because the range of output values from the output layer is uncertain, it is difficult to judge the meaning of these values. For instance, the output value 10 from the previous example appears to indicate that we are *very confident* that the image category is *chicken*. But just how confident? Is it 100 times more likely to be a chicken than a dog or are we less confident?\n",
        "\n",
        "Moreover how do we train this model. If the argmax matches the label, then we have no error at all! And if if the argmax is not equal to the label, then no infinitesimal change in our weights will decrease our error. That takes gradient-based learning off the table.\n",
        "\n",
        "We might like for our outputs to correspond to probabilities, but then we would need a way to guarantee that on new (unseen) data the probabilities would be nonnegative and sum up to 1. Moreover, we would need a training objective that encouraged the model to actually estimate *probabilities*.\n",
        "Fortunately, statisticians have conveniently invented a model\n",
        "called softmax logistic regression that does precisely this.\n",
        "\n",
        "In order to ensure that our outputs are nonnegative and sum to 1,\n",
        "while requiring that our model remains differentiable,\n",
        "we subject the outputs of the linear portion of our model\n",
        "to a nonlinear *softmax* function:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\text{ where }\n",
        "\\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}\n",
        "$$\n",
        "\n",
        "It is easy to see $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$ with $0 \\leq \\hat{y}_i \\leq 1$ for all $i$. Thus, $\\hat{y}$ is a proper probability distribution and the values of $o$ now assume an easily quantifiable meaning. Note that we can still find the most likely class by\n",
        "\n",
        "$$\n",
        "\\hat{\\imath}(\\mathbf{o}) = \\operatorname*{argmax}_i o_i = \\operatorname*{argmax}_i \\hat y_i\n",
        "$$\n",
        "\n",
        "In short, the softmax operation preserves the orderings of its inputs, and thus does not alter the predicted category vs our simpler *argmax* model. However, it gives the outputs $\\mathbf{o}$ proper meaning: they are the pre-softmax values determining the probabilities assigned to each category. Summarizing it all in vector notation we get ${\\mathbf{o}}^{(i)} = \\mathbf{W} {\\mathbf{x}}^{(i)} + {\\mathbf{b}}$ where ${\\hat{\\mathbf{y}}}^{(i)} = \\mathrm{softmax}({\\mathbf{o}}^{(i)})$.\n",
        "\n",
        "\n",
        "### Vectorization for Minibatches\n",
        "\n",
        "Again, to improve computational efficiency and take advantage of GPUs, we will typically carry out vector calculations for mini-batches of data. Assume that we are given a mini-batch $\\mathbf{X}$ of examples with dimensionality $d$ and batch size $n$. Moreover, assume that we have $q$ categories (outputs). Then the minibatch features $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$, weights $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$ and the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^q$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
        "\\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O})\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This accelerates the dominant operation into a matrix-matrix product $\\mathbf{W} \\mathbf{X}$ vs the matrix-vector products we would be executing if we processed one example at a time. The softmax itself can be computed by exponentiating all entries in $\\mathbf{O}$ and then normalizing them by the sum appropriately.\n",
        "\n",
        "## Loss Function\n",
        "\n",
        "Now that we have some mechanism for outputting probabilities, we need to transform this into a measure of how accurate things are, i.e. we need a *loss function*. For this, we use the same concept that we already encountered in linear regression, namely likelihood maximization.\n",
        "\n",
        "### Log-Likelihood\n",
        "\n",
        "The softmax function maps $\\mathbf{o}$ into a vector of probabilities corresponding to various outcomes, such as $p(y=\\mathrm{cat}|\\mathbf{x})$. This allows us to compare the estimates with reality, simply by checking how well it predicted what we observe.\n",
        "\n",
        "$$\n",
        "p(Y|X) = \\prod_{i=1}^n p(y^{(i)}|x^{(i)})\n",
        "\\text{ and thus }\n",
        "-\\log p(Y|X) = \\sum_{i=1}^n -\\log p(y^{(i)}|x^{(i)})\n",
        "$$\n",
        "\n",
        "\n",
        "Maximizing $p(Y|X)$ (and thus equivalently minimizing $-\\log p(Y|X)$)\n",
        "corresponds to predicting the label well.\n",
        "This yields the loss function (we dropped the superscript $(i)$ to avoid notation clutter):\n",
        "\n",
        "$$\n",
        "l = -\\log p(y|x) = - \\sum_j y_j \\log \\hat{y}_j\n",
        "$$\n",
        "\n",
        "Here we used that by construction $\\hat{y} = \\mathrm{softmax}(\\mathbf{o})$ and moreover, that the vector $\\mathbf{y}$ consists of all zeroes but for the correct label, such as $(1, 0, 0)$. Hence the sum over all coordinates $j$ vanishes for all but one term. Since all $\\hat{y}_j$ are probabilities, their logarithm is never larger than $0$. Consequently, the loss function is minimized if we correctly predict $y$ with *certainty*, i.e. if $p(y|x) = 1$ for the correct label.\n",
        "\n",
        "### Softmax and Derivatives\n",
        "\n",
        "Since the Softmax and the corresponding loss are so common, it is worth while understanding a bit better how it is computed. Plugging $o$ into the definition of the loss $l$ and using the definition of the softmax we obtain:\n",
        "\n",
        "$$\n",
        "l = -\\sum_j y_j \\log \\hat{y}_j = \\sum_j y_j \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j\n",
        "= \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j\n",
        "$$\n",
        "\n",
        "To understand a bit better what is going on, consider the derivative with respect to $o$. We get\n",
        "\n",
        "$$\n",
        "\\partial_{o_j} l = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = \\Pr(y = j|x) - y_j\n",
        "$$\n",
        "\n",
        "In other words, the gradient is the difference between the probability assigned to the true class by our model, as expressed by the probability $p(y|x)$, and what actually happened, as expressed by $y$. In this sense, it is very similar to what we saw in regression, where the gradient was the difference between the observation $y$ and estimate $\\hat{y}$. This is not coincidence. In any [exponential family](https://en.wikipedia.org/wiki/Exponential_family) model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice.\n",
        "\n",
        "### Cross-Entropy Loss\n",
        "\n",
        "Now consider the case where we don't just observe a single outcome but maybe, an entire distribution over outcomes. We can use the same representation as before for $y$. The only difference is that rather than a vector containing only binary entries, say $(0, 0, 1)$, we now have a generic probability vector, say $(0.1, 0.2, 0.7)$. The math that we used previously to define the loss $l$ still works out fine, just that the interpretation is slightly more general. It is the expected value of the loss for a distribution over labels.\n",
        "\n",
        "$$\n",
        "l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_j y_j \\log \\hat{y}_j\n",
        "$$\n",
        "\n",
        "This loss is called the cross-entropy loss and it is one of the most commonly used losses for multiclass classification. To demystify its name we need some information theory. The following section can be skipped if needed.\n",
        "\n",
        "## Information Theory Basics\n",
        "\n",
        "Information theory deals with the problem of encoding, decoding, transmitting and manipulating information (aka data), preferentially in as concise form as possible.\n",
        "\n",
        "### Entropy\n",
        "\n",
        "A key concept is how many bits of information (or randomness) are contained in data. It can be measured as the [entropy](https://en.wikipedia.org/wiki/Entropy) of a distribution $p$ via\n",
        "\n",
        "$$\n",
        "H[p] = \\sum_j - p(j) \\log p(j)\n",
        "$$\n",
        "\n",
        "One of the fundamental theorems of information theory states that in order to encode data drawn randomly from the distribution $p$ we need at least $H[p]$ 'nats' to encode it. If you wonder what a 'nat' is, it is the equivalent of bit but when using a code with base $e$ rather than one with base 2. One nat is $\\frac{1}{\\log(2)} \\approx 1.44$ bit. $H[p] / \\log 2$ is often also called the binary entropy.\n",
        "\n",
        "To make this all a bit more theoretical consider the following: $p(1) = \\frac{1}{2}$ whereas $p(2) = p(3) = \\frac{1}{4}$. In this case we can easily design an optimal code for data drawn from this distribution, by using `0` to encode 1, `10` for 2 and `11` for 3. The expected number of bit is $1.5 = 0.5 * 1 + 0.25 * 2 + 0.25 * 2$. It is easy to check that this is the same as the binary entropy $H[p] / \\log 2$.\n",
        "\n",
        "### Kullback Leibler Divergence\n",
        "\n",
        "One way of measuring the difference between two distributions arises directly from the entropy. Since $H[p]$ is the minimum number of bits that we need to encode data drawn from $p$, we could ask how well it is encoded if we pick the 'wrong' distribution $q$. The amount of extra bits that we need to encode $q$ gives us some idea of how different these two distributions are. Let us compute this directly - recall that to encode $j$ using an optimal code for $q$ would cost $-\\log q(j)$ nats, and we need to use this in $p(j)$ of all cases. Hence we have\n",
        "\n",
        "$$\n",
        "D(p\\|q) = -\\sum_j p(j) \\log q(j) - H[p] = \\sum_j p(j) \\log \\frac{p(j)}{q(j)}\n",
        "$$\n",
        "\n",
        "Note that minimizing $D(p\\|q)$ with respect to $q$ is equivalent to minimizing the cross-entropy loss. This can be seen directly by dropping $H[p]$ which doesn't depend on $q$. We thus showed that softmax regression tries the minimize the surprise (and thus the number of bits) we experience when seeing the true label $y$ rather than our prediction $\\hat{y}$.\n",
        "\n",
        "## Model Prediction and Evaluation\n",
        "\n",
        "After training the softmax regression model, given any example features, we can predict the probability of each output category. Normally, we use the category with the highest predicted probability as the output category. The prediction is correct if it is consistent with the actual category (label). In the next part of the experiment, we will use accuracy to evaluate the model’s performance. This is equal to the ratio between the number of correct predictions and the total number of predictions.\n",
        "\n",
        "## Summary\n",
        "\n",
        "* We introduced the softmax operation which takes a vector maps it into probabilities.\n",
        "* Softmax regression applies to classification problems. It uses the probability distribution of the output category in the softmax operation.\n",
        "* Cross entropy is a good measure of the difference between two probability distributions. It measures the number of bits needed to encode the data given our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2VpF3svw2w5",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification Data (Fashion-MNIST)\n",
        "\n",
        "Before we implement softmax regression ourselves, let's pick a real dataset to work with. To make things visually compelling, we will pick an image classification dataset. The most commonly used image classification data set is the [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digit recognition data set, proposed by LeCun, Cortes and Burges in the 1990s. However, even simple models achieve classification accuracy over 95% on MNIST, so it is hard to spot the differences between better models and weaker ones. In order to get a better intuition, we will use the qualitatively similar, but comparatively complex [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, proposed by [Xiao, Rasul and Vollgraf](https://arxiv.org/abs/1708.07747) in 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biN7AVeUw2w6",
        "colab_type": "text"
      },
      "source": [
        "## Getting the Data\n",
        "\n",
        "First, import the packages or modules required in this section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7iG_K8kw2w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8xlBJncw2w_",
        "colab_type": "text"
      },
      "source": [
        "Conveniently, PyTorch's `torchvision.datasets` package provides easy access to a number of benchmark vision datasets for testing our models.\n",
        "The first time we invoke `data.vision.FashionMNIST(train=True)`\n",
        "to collect the training data,\n",
        "PyTorch will automatically retrieve the dataset via our Internet connection.\n",
        "Subsequently, PyTorch will use the already-downloaded local copy.\n",
        "We specify whether we are requesting the training set or the test set\n",
        "by setting the value of the parameter `train` to `True` or `False`, respectively.\n",
        "Recall that we will only be using the training data for training,\n",
        "holding out the test set for a final evaluation of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZNhGJ8yw2xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# By default pytorch torchvision datasets are of type PIL.\n",
        "# Define a transform \"trans\" to change the PIL to Tensor format.\n",
        "trans = transforms.ToTensor() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc4buFew2xD",
        "colab_type": "text"
      },
      "source": [
        "The `ToTensor` Transform also moves the image channel from the last dimension to the first dimension to facilitate the convolutional neural network calculations introduced later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwCuG_Qw2xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train = torchvision.datasets.FashionMNIST(root=\"./\", train=True, transform=trans, target_transform=None, download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(root=\"./\", train=False, transform=trans, target_transform=None, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv69j3NDw2xI",
        "colab_type": "text"
      },
      "source": [
        "The number of images for each category in the training set and the testing set is 6,000 and 1,000, respectively. Since there are 10 categories, the numbers of examples in the training set and the test set are 60,000 and 10,000, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEjFrg1Pw2xI",
        "colab_type": "code",
        "outputId": "4dff1ff5-1e85-411d-b985-4d64230a3533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(mnist_train), len(mnist_test)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj5og6yNw2xM",
        "colab_type": "text"
      },
      "source": [
        "We can access any example by indexing into the dataset using square brackets `[]`. In the following code, we access the image and label corresponding to the first example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caoE1Grew2xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature, label = mnist_train[0] # accessing the first example in the training set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubp-FnOLMDP6",
        "colab_type": "code",
        "outputId": "1de31afb-0658-47cd-fbca-e9803828283a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " label = mnist_train[0][1]\n",
        " label"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32nnOrIRIHky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c9c4fa5-a73e-416d-90a1-3faf63575810"
      },
      "source": [
        "print(feature)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
            "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
            "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
            "          0.0157, 0.0000, 0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
            "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0471, 0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
            "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
            "          0.3020, 0.5098, 0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
            "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
            "          0.5529, 0.3451, 0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
            "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
            "          0.4824, 0.7686, 0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
            "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
            "          0.8745, 0.9608, 0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
            "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
            "          0.8627, 0.9529, 0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
            "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
            "          0.8863, 0.7725, 0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
            "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
            "          0.9608, 0.4667, 0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
            "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
            "          0.8510, 0.8196, 0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
            "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
            "          0.8549, 1.0000, 0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
            "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
            "          0.8784, 0.9569, 0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
            "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
            "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
            "          0.9137, 0.9333, 0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
            "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
            "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
            "          0.8627, 0.9098, 0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
            "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
            "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
            "          0.8706, 0.8941, 0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
            "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
            "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
            "          0.8745, 0.8784, 0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
            "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
            "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
            "          0.8627, 0.8667, 0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
            "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
            "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
            "          0.7098, 0.8039, 0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
            "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
            "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
            "          0.6549, 0.6941, 0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
            "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
            "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
            "          0.7529, 0.8471, 0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
            "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
            "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
            "          0.3882, 0.2275, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
            "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovgqnr4JIUes",
        "colab_type": "code",
        "outputId": "b1acb0e6-1db9-4df1-e398-729aea7cb376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(label)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVBbkUhSw2xR",
        "colab_type": "text"
      },
      "source": [
        "Our example, stored here in the variable `feature` corresponds to an image with a height and width of 28 pixels. PyTorch automatically scales it into a tensor with each pixel value between 0 and 1. It is stored in a 3D Tensor. Its first dimension is the number of channels. Since the data set is a grayscale image, the number of channels is 1. When we encounter color, images, we'll have 3 channels for red, green, and blue. To keep things simple, we will record the shape of the image with the height and width of $h$ and $w$ pixels, respectively, as $h \\times w$ or `(h, w)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUvYbRZw2xS",
        "colab_type": "code",
        "outputId": "0d349cf1-43e3-44fe-9c1f-d841a67a6542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature.shape, feature.dtype"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FJOruWHw2xW",
        "colab_type": "text"
      },
      "source": [
        "The label of each image is represented as a scalar in PyTorch. Its type is a 64-bit integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zuV_U0nw2xW",
        "colab_type": "code",
        "outputId": "7cf925d6-b862-4a92-931a-be95ec97df00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "label, type(label)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, int)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydm8U4gow2xa",
        "colab_type": "text"
      },
      "source": [
        "There are 10 categories in Fashion-MNIST: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag and ankle boot. The following function can convert a numeric label into a corresponding text label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4UkUZfow2xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_fashion_mnist_labels(labels):\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "    return [text_labels[int(i)] for i in labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaacrkxuw2xf",
        "colab_type": "text"
      },
      "source": [
        "The following defines a function that can draw multiple images and corresponding labels in a single line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHz0phYfw2xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_fashion_mnist(images, labels):    \n",
        "    # Here _ means that we ignore (not use) variables\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
        "    for f, img, lbl in zip(figs, images, labels):\n",
        "        f.imshow(img.reshape((28, 28)).numpy())\n",
        "        f.set_title(lbl)\n",
        "        f.axes.get_xaxis().set_visible(False)\n",
        "        f.axes.get_yaxis().set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPK5sh-Dw2xj",
        "colab_type": "text"
      },
      "source": [
        "Next, let's take a look at the image contents and text labels for the first nine examples in the training data set.\n",
        "\n",
        "Note: PyTorch DataLoader objects don't support regular array slicing. You can instead iterate through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJByWiz3w2xk",
        "colab_type": "code",
        "outputId": "64494920-3a09-4c4e-96c0-00981e475048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "X=[]\n",
        "y=[]\n",
        "for idx, data in enumerate(mnist_train):\n",
        "    if(idx>=0 and idx<10):\n",
        "        X.append(data[0])\n",
        "        y.append(data[1])\n",
        "    if (idx>=10):\n",
        "        break\n",
        "show_fashion_mnist(X, get_fashion_mnist_labels(y))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 10 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"89.763888pt\" version=\"1.1\" viewBox=\"0 0 687.5 89.763888\" width=\"687.5pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 89.763888 \nL 687.5 89.763888 \nL 687.5 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 79.063888 \nL 67.445763 79.063888 \nL 67.445763 22.318125 \nL 10.7 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p641c5f6b1e)\">\n    <image height=\"57\" id=\"imagef43ccef77a\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"10.7\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 79.063888 \nL 10.7 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 67.445763 79.063888 \nL 67.445763 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 79.063888 \nL 67.445763 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 22.318125 \nL 67.445763 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- ankle boot -->\n    <defs>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(7.349756 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 78.794915 79.063888 \nL 135.540678 79.063888 \nL 135.540678 22.318125 \nL 78.794915 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p00902d3985)\">\n    <image height=\"57\" id=\"image669e634150\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"78.794915\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 78.794915 79.063888 \nL 78.794915 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 135.540678 79.063888 \nL 135.540678 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 78.794915 79.063888 \nL 135.540678 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 78.794915 22.318125 \nL 135.540678 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- t-shirt -->\n    <defs>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n    </defs>\n    <g transform=\"translate(89.237172 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 146.889831 79.063888 \nL 203.635593 79.063888 \nL 203.635593 22.318125 \nL 146.889831 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p6ac2aa3d2a)\">\n    <image height=\"57\" id=\"image5c83af1c0e\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"146.889831\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 146.889831 79.063888 \nL 146.889831 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 203.635593 79.063888 \nL 203.635593 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 146.889831 79.063888 \nL 203.635593 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 146.889831 22.318125 \nL 203.635593 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- t-shirt -->\n    <g transform=\"translate(157.332087 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 214.984746 79.063888 \nL 271.730508 79.063888 \nL 271.730508 22.318125 \nL 214.984746 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p7534ed0fae)\">\n    <image height=\"57\" id=\"image79e7bc5fbf\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"214.984746\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 214.984746 79.063888 \nL 214.984746 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 271.730508 79.063888 \nL 271.730508 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 214.984746 79.063888 \nL 271.730508 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 214.984746 22.318125 \nL 271.730508 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- dress -->\n    <defs>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n    </defs>\n    <g transform=\"translate(227.273877 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"163.863281\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"215.962891\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 283.079661 79.063888 \nL 339.825424 79.063888 \nL 339.825424 22.318125 \nL 283.079661 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p26f2a58bfe)\">\n    <image height=\"57\" id=\"image93b81694e3\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"283.079661\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 283.079661 79.063888 \nL 283.079661 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 339.825424 79.063888 \nL 339.825424 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 283.079661 79.063888 \nL 339.825424 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 283.079661 22.318125 \nL 339.825424 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- t-shirt -->\n    <g transform=\"translate(293.521917 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 351.174576 79.063888 \nL 407.920339 79.063888 \nL 407.920339 22.318125 \nL 351.174576 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p4dc924e48e)\">\n    <image height=\"57\" id=\"imaged549bbd3a4\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"351.174576\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 351.174576 79.063888 \nL 351.174576 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 407.920339 79.063888 \nL 407.920339 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 351.174576 79.063888 \nL 407.920339 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 351.174576 22.318125 \nL 407.920339 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- pullover -->\n    <defs>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g transform=\"translate(355.221208 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 419.269492 79.063888 \nL 476.015254 79.063888 \nL 476.015254 22.318125 \nL 419.269492 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pba321a0a0d)\">\n    <image height=\"57\" id=\"image7cef789f3a\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"419.269492\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADkAAAA5CAYAAACMGIOFAAAABHNCSVQICAgIfAhkiAAACwJJREFUaIHtmUtvZMd1x3+nqu6jHySbr3lSmuFII2kc2ZaFJIYVBHGAwPkCARQDQTYBssgXyC7IMosgm3yBbAIEyCLIwjACZ+EgcuxACeR4pCgjazijec9wyCbZ7O77qKqTxW12k6MZeRaSDNA8QOPi1q1bdf7n/69Tp27L78kfKMfczC/bgS/DTkAeFzsBeVzsBORxsROQx8VOQB4X+5UA6T6XUUQ+l2E+V9PZueO5QUqSItaAtWAMIgJGIMsQY8A5sAaMQc0h0CKznxEI8YgDR54Z0zwLEUJAimrWx8zGUWdBFYnNOGrN7D5GKCu0rgnbOxDDc4IUQRIH1iIHYKxtgOZZM2maoMaAM6g9BNIYVAAjqAiiioSJc5Nuak3z3AmE5rmpPCZNpvOrSDO2CJoYRGkAiaBGpvcSFBmVSFkhuwNU4wSkCCbLGsdb+QzAU0y9b6Im0rBae6T2UFSTNjNj5MBBa8DZCeDJuHbCnBUgomKABkDMhNB26GKOOkPIGiBqAQXRSYAE1AghAYlgK51cO5gyks11MLU/xGSSIM4hnXYz+ZMgY5zIKIKGqWyb9nCon529q9qoQAQShyYOmchOzRM5b8KWGiEmhuiEmArRCT4X1EyGVpBwABDUQsgEIrixYgJIEGxlsGULCRFn5+dnE4VA3N45NG/jHMbC4jyaJ/iFVUJmKBcdPjf4FsSkibI2ZByR4lS1NZj6wInm3taK8c1PPJNrxBYBpwoTWctEFNO1bpgGBAG1gngl2SuROiBF3azp4RiNEUcrh6jgPYSAjsdoCCBNpE2eIWmC5gmhmzE6k1J3DONVwbeg6kViHsApWJ2CRBSRiWpVkLHFjAVTC6YSbAFuJNhSceNGask4YkqDrT1SR0zpZ04fyN8YNHFTsFNC6gDbu1DVxNGowRICqOKIijhLPLtMzBPG51r43DBeMoQc6jkImVL3AqSRpDsmTQNzrYLceVquJjWB1HqcRMwk7EvpkHlXEFUIGMroqKIjTCiuoqMMDq+GanItg2NcJ+yWKXWwVKUjxpTo26CCxuZdMdpgDofk4g12uAzaqCnZM6z9cIzdqyZr0jnqpTZVz7F1xVHPK2FtTN6uWF/qs5SNON/aYcGOWXAjEgnkUmFFqdUS1GAlYohYUSyRi8ljTtsxEQgICYoVyEXIxJBgScRiEKzMKNmPBdvRM4yG7ZhTq2OoKUENhTbZNqqhUssgtgCwRApNuF0sAbCcDPlJf537t9dpbSW43W9fwrcM278Gfj6QLQ/oZDUr3SEtV9N1JYkJ+GjYJ2M/ZBjRCajm2iinYbBURx0tG/YUuakxk+cH1jYVHVNiiRiJpBLIpaZSS8RQ6QJFTHiaxYk+K3XEiSKiGmosZUxwJmKJtG3J6XzA/12G0WmH679q8R1l7c17vNDtM+9KnAlYGufGMSWoNIxFQxUbEAfOZ8Y3oCf993yLoU+JCFGF1ASMRKIaogotW9Oy9ZH327aijpYyzpJ9ZjxdW5IYTy4eI3E6R62zfgGZqYlIYpqgLaVD/LmSetHhzv/7mLrt2Nw5z92Fc/jzJWmrZmV+SOb8lLVTrQFdV5EaT8tU1GqJahiHlKhCx5Vkxk+vB4FyJpJIYBwSarVkxpMZ38jsEMsBQ8Dgo6VWwzimU9UkElh0I17OH2BRAkIqgTkzZhgzPqlWiWIYxRQbI9vSZeDzaYp35t/eo5XnnN+6TLmSs/lGTtXLuH/OkeU1iQukLtB2FYlEOrYBQYAaKKOlim7CjtKWCgx0bUluahJpmNwPOYOQH2kDqKNjFJtATQGG2do76JdIoGdGpNLsybnUrNoxOzHlge9RqJu+50xk36fThOygqWLcvW3sTs6ZokdoGcr5jOjy6f53I1/hegZ7r3ry5TG/sfYJr3QecTHfIpHAKKYUMWEUU3w01Goxoiy5IQt2RDZZn3aydg+cj2JIJFDiqLVh0keLM4GOK7GipMZTRscP9l7n3ccXuP+fZzFBiE7xXaV7cZdTc/v87upHJMbTrzs8Lrp0P8jIdnUG0t+526D+qEGePWXhmzwn+8NvsPtSl51TbVYWBryW3WfJjvhpscb9epH9kDGOKXW0eDXkpmbBjsilJpeagCHqLJsmEqjFAuAn67KagGzZulmzpuJhNc+HO6e58z9nufxXPyMOh43kr1xm47srXL/Y4k/W3iE3Nf/l1+kXLVber8kfDJFn/hdyuDQ7aHIOvnGFYjVn56WEchGKc55koWS5t89iPuaN3h1eaT1g23cZxSb1R6S5qpAZP5VsZmpGMWUUmqy9U7cxEkkkUmuT5Dq2YjUdTBKO5ceP17nz4/O4kZDugS2UVj9SdQw7rzZlnqmErA/n/2UT2Rk84xQiMq140Fldqt7Du1fJgNMAxqLf+irD8y0evdlm88Ux31y6yVutG9z282z6ea4VZ7lX9PBqqKOll4zpJSPmbEFG3Uh2UiYZiXRtyWIy4nHdZavsALAKnE52+Wp+m+/MX2XnQpuN6hT/unmFDz58gSt/cYPY77PUas3q7xDwDzchhs9g0jQSIoan30+C4S68QOx12V+fo+gZButQrgQW13Z5ZXmTC+1tXsy2GMWUMjYZttmCmqx9wHCTrYWdus1m2aXtKpbSEQ/Gc9zoLzPYb8H9ZhGpA00UbXuoDW7b0XokrH1/Eykq4lwLKT1x4xZaV59xntSjm7hY25z5qkOHXlX8zVsAtH8KbWB1fh7ptLnz9iXe/fU2p14b8Futj9nTjEHMuVmtcrNYoQyOMjoy40kk4EwkM54bwxbXHpzi7OIeX5u7y7XdUwz/d5Gla7D09/+NWINZXaG6sMK9324zulzxve/+NX+3/S1+8v5vkm6N8XMZtvAYa9Aa7CX5yl8+E+gTxy1hcvL+RRYiadrD7GZ8sHOOf9h5gx/uvcqHxXnulosELGeyPb7euUXH1tNKZhwS9kPOMKSsdoasdx6TWGVvwdAvuiy98xD1HkFxXkhCjoxTvr/0Eu9vnWWgXeqFjLmNIfbxHnFnF2L8BSCPMPu8AANaVdiN+7Tfu8vqNWXxasJ4r8e9sscD7ZBkgTfm7/D2/Aap2WNX25QxYavuENVgrXI6H7CW7fC19m3+9MyP+Kf4Feb+eYgWBVqUxP4O7uf3WNgU+v4sxbBF762HbC6knPreJv72nam/n8+HrM8IjO4PscDiz1PS/YziRs6d3ov8zctn+MHLV6ZdV7N91tuPGYScjiuZdwW1Wj6pVvikWmFvq8O5pwRZ94f0Pq5wRcrDiwvYvjt6iP/CQQKh34d+H3P7DnMizB0499bXufXmJYoVKM/UXFjf5PeXrlLEhEHWotam6N4Yr/De5hrZ3eRTzgOErW2yd4ZkL19k79IiyUDQuv5yQQJMT8+HPxM+3GXxI0f1wFLcdTy6dY4/33gbO1dzenmXC3N9vr10jY3Bawx/tMrS9dhsYU+aKuo9tr/H8vs93DigRXmkyxcP0ljEyPSUfmDh4xuk12+SAt3JnixGiN98nbu/c4ZHr8/zx6f/gxsPVrj8t1eJ+/uoPn23U+/xd+/R/cf7AMQn+n05TH6WqU4LDo0HDGfsFW3+rPojuj/LGmaeARB4anV22L54kBoB++l2EcRaNOqRAiNcv0ln4xZda5HEoSGgdfXs8UUQ15w+1NdPBfqlMKnxGTKL+qmi44BZjeGZTn+6f5yM9UtjcibH52p/ss/zTPG0hHTIfiX+1ToBeVzsBORxsROQx8VOQB4XOwF5XOwE5HGxE5DHxf4fdFmjtPlwa0UAAAAASUVORK5CYII=\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 419.269492 79.063888 \nL 419.269492 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 476.015254 79.063888 \nL 476.015254 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 419.269492 79.063888 \nL 476.015254 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 419.269492 22.318125 \nL 476.015254 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- sneaker -->\n    <g transform=\"translate(423.930185 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"177.001953\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"238.28125\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"292.566406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"354.089844\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 487.364407 79.063888 \nL 544.110169 79.063888 \nL 544.110169 22.318125 \nL 487.364407 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p701b8cafeb)\">\n    <image height=\"57\" id=\"image74c0e72ed6\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"487.364407\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 487.364407 79.063888 \nL 487.364407 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 544.110169 79.063888 \nL 544.110169 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 487.364407 79.063888 \nL 544.110169 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 487.364407 22.318125 \nL 544.110169 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_8\">\n    <!-- pullover -->\n    <g transform=\"translate(491.411038 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g id=\"patch_42\">\n    <path d=\"M 555.459322 79.063888 \nL 612.205085 79.063888 \nL 612.205085 22.318125 \nL 555.459322 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p0d27178b9c)\">\n    <image height=\"57\" id=\"imagee53632b3d4\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"555.459322\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADkAAAA5CAYAAACMGIOFAAAABHNCSVQICAgIfAhkiAAADdhJREFUaIHlmtmPXFdexz+/c+5WS3d1dXe1l7azODYZJ54MIZpMMmSEGIF4A6GZBxAS8w5C4nWEhJAQbzwi/gfEMkIImIcRGgQMTDILIaPETuIljtvudlcvVV3LrXvPOT8ebvXmLbFjOxb8pFJX37r33PM9v9/5/rYjvyLfVP6PS/R5T+BhiUQRpt2G4PGbW6D7ujOf47weqtiTy1z73TOsffN5bGv20G+frEkREINYC0YQETAGrK2+772l+p0oQoyp7gHUOXAOHedoUaDeH1rlhyESRYSZGoOnAnFfOBIn9wFSpFqVNIXFNiGLcM0EHxsm7QifCAioCEVLcHXI5xXfDGgSIFKSGzHZujB/vqR+fg3d7uG3ew8NoMkyzNEltp9v8ZtvvMlb3aeRRg3WD4A0MzMVHmtATKWNqfYwAq0ZNIkpO3V8aimbBp8IedvgM1ABBIpZxWfgFwtqMxNm6zmNpOBytIjPEpJ+RNxrEwMyGD48jVqL1lJcZjia9pjLxniTHbol2vr1FwmRMGkLIYGyARopPlX0wI41hSC6D8pnlaY0CiAQbUXEQyHppxBS1pbrxPM57faAueM5N56epftGxvx/LXPk+zFsbuO7G2AsEkfgfWXa9yvGoGlMuuP5yx98nXgz4vTw4mGQ+YIhRJAvKiGpTI0oYDKPGCU4A0HwwwicIKF6UBsOk3qS1GFtYDxpYgpD1hOioeIzS2EyyqxkJp7Q6HQJi8KH156hPDZL4jx0N5DpPn9QnYoIwQripwvdE/D+MMjaejXr+tp0YbxBgsFODMYpNg9IUEyeY1xASo/Glg9+u0HnC9v82vJ7vFi7xvefOssH/Q7rf3+Szo/7dN4CUWX7bIuLJ9pEb2zy7bP/zN98veCnp08y971jLHx8HTQQJpMHN10jhDQib1tmX9hgc30WSdPDIJOdgKgiTitQY4cpA6Y3QkoH47wyo6JEvUcnEyRNMe4lZtIJZ7PrfDldYa0+RxksG+UJ7No2YWubMBwyxwvYYoa1lzMW7ICvti/SinP+490v0ZlroaMROhw+GEAAMagRfAzLs31GebrH7Hsgmz/6CAD1Uzt0DjSgpUNVK4II078AGhBjUKuk1vHh5AjDkPIXP/sl9FKDp85P8OtdtKz2l1y6Rmu9yeD4M/ye/R1+9dQFfn/pX3jvjSN85M/Qebsk/ae3HhykVpYGkNmSKPIVYR4E6VbX7n/gEJBSGBQp7w+XWI1a+Gt15i5BcnOIn0z2bvX9PvT7zFw7yehyg9XjM5yOla90rvCd59sM1xJSY0HDZ2ZbI3d+/oHCujDOee6v+hTfW+Bq0kGNcPrGDnZriHY37/jM7Jsf07jS5n84w582XyMygW+89BO+0/0KnbOnkc0e7sbqg0yn4gwHG3mDfJxAOAz2/kHuRjnvXGA3rtCgqAbcPTThVq7DynWar73OD9ef4aX5Fb7eeo+/XfwFJkeapM7DjfueTQUoKBJgVMZ4Z26ziPsDaSz29DOEVp2ilRBSg80DxgWSK13C2jqhKCH4uw6x9GafrcEx/uH1JcxXlKeXu1z+1gLz/95h4f2L9zbZg2Hk7n1GwAi2VFbXW0g3qXjlQUGKEUKrTt7JGC9GuAyiHGyhxN06bMaIc2i4+xjmyg0WeiOGy8e4eq7NsXqf01/o8m9XvkQnTVHn7h4UyJQ1b3mBSuW/wyAmGck+iT4ISPWeaHWL+rBOPGzgahE+E9QITAp0PN5n4btI6A+QoqSxcpS3PzpBnDrqWcHkZMGlP3qZxXeU5l//8HaNiiB7rGnRoPsWI2CcEm9bkr7cZkn3Z66q6M4OFAUxYOsJxXyGTw1S3kMDB4coC7QsyLY9cjNl0ohxMxGNuTHzyxtsjI8xkySVC7qH2d8qEpRoJNic24jHnpIX/uS+gHqFokDGOdIfEW/lJN0RobuJluWnHiYrLLNrCS5OcSdKyiKi36/jjSF/cZlo8SjxheuHH1Ld/0wDQZNl6EKLkFomLUs8UBrv3kTzfTd23+xaaQLI8/t99JC4K1cxV67SPPIaIyBMLDKICE2Pf3XAFrPUkwQtin3TvRMp7bKrBzsBW3CbJj+fysABlpy90GP+uzXqFxN0xoFCfqNBPq9c+8NXGH7jVTD27mNN2VWCEg2VaBymCfz+M48fpEhVZZgClaurLPxglfqqEmUOghBvG9ysp/a1LhsvWkwS33NIFYGgRBPFFlqBjveN9JP3pMhh//RZZLeUIoLYqJqIKjKZkJomrqijIvjncmzNMykiCmvIf24Zc/w46Qfrt5GRyTLC0hwhNri6xZZKujoAH/Y44pM1KWbfP31GgGJt5QbEINYgSYI6h9/uEV+8wZG3dki3hGePdplpjilHMXauoPHLN1l/2SBJfM8Fl1B9SOLpvdW8P5l47uXZ70f0QCaDB7XVak+vhf4O9prQ+e+YNXOSfFGxT+VEkaf0hmLRsf5bLxHlSjxS7CSQbBcUiWUyH4NCuu2wk4A2apW19Ppo+FQgH2Jl7cBYt/rUMBoRRiOSfMKJtQ7dVxfoP+eIooDzlrSds/HVFMkt8bYhGlrqqxEoqIF4pNRWS0zpCfUEs1un4kkrLotUSfn6FvPvxIifZXRE6J8pYWql0Y5h7jwgSlkHUTAF2FKJtkfgA6FVR63s5ZVPDsjpyofxGEYjWF9n7sfQ/vIXudxq4muKbwSiHaH9bg83m9I9lyEKNq+qGrK9UxW25uqVG5nKkwNSFbg9cbY3Njn2nxnFrGXUiUh7ilqDHTsW38lxNTtNFgz5C8uoFSZzEfHQU7+c4Uv3BIGEO+5/d22F9NoK9U6HxvPLlUnHFtvLkZ9eIF3qMPnaCcq6MDmdEKzga5BsC40sw+STJwzkrtwhb9TRiGRlCz/XZPhMEz2WEl58mWJG6J+CaCh03naohZ2TFSxtzWCsfQJB7gYMU9LYrbSH4ZBweYg9/SyjpVnyBWF0qsDWHfOtIetX2zT+bgtEGC0tAODbdUwWP2EgdzWoAfXcOQixFlcTyqYys1iVMje2msQ9i1qLqJLsTLVvKtN+clp3B/zaXjoV7tAviSyuBq4VeGnpBp3mEFlNSbsVILVCuuVItz06LTw/Xk2KHJ70LaYJVBq8l6hiPEQ9ww8+PIUOI2Y/NmSbiqii1qKRoCJobKrA/dGguYPsaWrqJnYBxhES3UfDxwdMAVlXsCsZ8VCZ+XiCzavV0djgMjMtidgqBXu0yA7Irh88dG36vzEVyPApQkjvsbmiVqoAwFelj10TF63qPUh1XfzjBAmHTXX3+xSYqn6qKro4TzJQfFzFrMYrey0xVQgBm4cq63GVCT96kMYi1mJaM0ijju+0mCxkpBs5dr2HDkeEnUHVbvs0yYDzROPqvqIpeC/41BAioZiryt0SwJSBaFAgPjx6kCZLkTQlPH2U8bEG3Zci8hfGpO/N0nm7Tv1KH+1ufOrxtCxJtx0QMe4Y1ICvGVxq2HnKYApov19ic090swcPNazb83G3aMNaSOJq3wnYHHQrISTQezZi52Qb8/rrNFccjfduojuDqgN9N3EOm3vsxGJKQCCfs5RNYfiUJxoa2h+A+AD5BJ0UD9lPirktc5coqioAtrpeW1da5y0hVnZeHfPyt97hu3/850z+YJOtV4/hTx2/9ztKh+3lxAOHHVcEMzgh9M4EvvrKBVrnNipSyh1hu4ff2noMe9L7vd6EWogmimwrITJMXMa/mjN8W5SNXoPkWYOrNWklP098cwf//kUwVSFLfajKod4j+QRTZoQYwi4BlfBu9whb6zOc2Sww/RF++t6HB1L1jp48TCaVuQQlRELS90SDktYHHjNxlPM13l08hz1rSX+xS2+Usd5Laf9kic6HVzBZiplvo3mO726gRYls9TAzdVy9jp92zuO+IX9zgfl1JTp/CXfgVNaj12RQ1Dlsb0zWjREXEK97vs3VLPm8wdcV5y3WBmzDkS/EmHNnEBdQ1aowIFWpUWaahHo8dSEQD3YZValtBrQoD3HDIwe52/vgg8tEFwW7fAx3ZA6NDCFJ2T4dM3hthAZhp18jij31Rs7Os5aPfmOebF1pv1+QxBauG6TRoHh6gXwhAQPREBbezTETX511GBWEAy2CxwJyT4KvfP0ox+7kVfJrDNGwjh/GyNgQDQyuoQzmLBgYnyhxjQhIiE8kZE+/gsuE8YLB14SyAeIABVN4TH+MjCe3ddYee6rlu13Y2NwLyttz5yjm6tRuKrOXx4yOpfSeyxieKvnml3/EZtHgYn+RxdqAN+Y/pFvO8LPecdZGTforbTSKCYkBr+jVFfwdmsCPP5+cEtRu2BpvjWlcz6htOOL1AXURfJoSbMw/zr+IKy3lIGG1NUNiPEWwbOZ18iIGq7i60ns2JlmMaM6eJepN4PylQ2eD5LEd6t3NQm6JT/f8qPeVu7G2uhZHSJZVMW1Q9PgiW19sMZk1jI8qrgZ+oSSulZzsbBFJwKnh0kdLvPBnXfzKDXR6CuXxaPJA3nirl7mtfe7c3uRg/zSlNcLMXEY6G2NLSzEjjCTCZZYrfgFjlCQtkUGVth1s3z1Z5Y97iF/fIOr1iYyhEUWwtMDgXAc7DtQvbKBxxORkG5uPCesbqNtvCD8ekNNcUsNniCKDJ+T7ZmCtJbs5ix0W+GvXkSQhFUGcx5fu8frJPblLRPSg4re3sT8ZV0fjnKvCvcsfV0Wwz3LE5YkSVcLBlr5qFXTcQZ6cat0jlP8XIP8XDt4F1lD4G+UAAAAASUVORK5CYII=\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 555.459322 79.063888 \nL 555.459322 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 612.205085 79.063888 \nL 612.205085 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 555.459322 79.063888 \nL 612.205085 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 555.459322 22.318125 \nL 612.205085 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_9\">\n    <!-- sandal -->\n    <g transform=\"translate(564.074391 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_10\">\n   <g id=\"patch_47\">\n    <path d=\"M 623.554237 79.063888 \nL 680.3 79.063888 \nL 680.3 22.318125 \nL 623.554237 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5ab29258a1)\">\n    <image height=\"57\" id=\"image9d5028505f\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"623.554237\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.063888\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path d=\"M 623.554237 79.063888 \nL 623.554237 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path d=\"M 680.3 79.063888 \nL 680.3 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path d=\"M 623.554237 79.063888 \nL 680.3 79.063888 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path d=\"M 623.554237 22.318125 \nL 680.3 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_10\">\n    <!-- sandal -->\n    <g transform=\"translate(632.169306 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p641c5f6b1e\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"10.7\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p00902d3985\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"78.794915\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p6ac2aa3d2a\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"146.889831\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p7534ed0fae\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"214.984746\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p26f2a58bfe\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"283.079661\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p4dc924e48e\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"351.174576\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pba321a0a0d\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"419.269492\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p701b8cafeb\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"487.364407\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p0d27178b9c\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"555.459322\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p5ab29258a1\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"623.554237\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ton7Y_l-w2xn",
        "colab_type": "text"
      },
      "source": [
        "## Reading a Minibatch\n",
        "\n",
        "To make our life easier when reading from the training and test sets we use a `DataLoader` rather than creating one from scratch, as we did in `chapter_linear_scratch`. Recall that a data loader reads a mini-batch of data with an example number of `batch_size` each time.\n",
        "\n",
        "In practice, reading data can often be a significant performance bottleneck for training, especially when the model is simple or when the computer is fast. A handy feature of PyTorch's `DataLoader` is the ability to use multiple processes to speed up data reading. For instance, we can set aside 4 processes to read the data (via `num_workers`).\n",
        "\n",
        "We've already applied required transformations before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2yhMtX-w2xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "if sys.platform.startswith('win'):\n",
        "    # set 0 for windows\n",
        "    # 0 means no additional processes are needed to speed up the reading of data\n",
        "    num_workers = 0\n",
        "else:\n",
        "    num_workers = 4\n",
        "\n",
        "train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-JI6u6ww2xr",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the time it takes to read all the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focyja2Tw2xs",
        "colab_type": "code",
        "outputId": "84d90fff-5a7c-461c-aa27-12e8864ee277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = time.time()\n",
        "for X, y in train_iter:\n",
        "    continue\n",
        "'%.2f sec' % (time.time() - start)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.68 sec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvkQOTbZw2xw",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Fashion-MNIST is an apparel classification data set containing 10 categories, which we will use to test the performance of different algorithms in later chapters.\n",
        "* We store the shape of image using height and width of $h$ and $w$ pixels, respectively, as $h \\times w$ or `(h, w)`.\n",
        "* Data iterators are a key component for efficient performance. Use existing ones if available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3rsCs1jk29n",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** the `torchvision` library has multiple transforms that can be applied on Tensors and Images. Implement a function that takes one MNIST image, similar to a one from the list of images above, and *returns* a random square crop from this image. Note that `crop_size` is just an integer. (Hint1: you should use a `torchvision.transform` internally)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaU5kUNYnVFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_square_crop(img, crop_size):\n",
        "  to_pil_image = transforms.ToPILImage()\n",
        "  pil_img = to_pil_image(img)\n",
        "  result = None\n",
        "  ## write your code here\n",
        "  #transforms.RandomSizedCrop(size)\n",
        "  #result = transforms.functional.center_crop(pil_img,crop_size)\n",
        "  top = torch.LongTensor(1).random_(0, 10).item() \n",
        "  left = torch.LongTensor(1).random_(0, 10).item() \n",
        "  #result = transforms.functional.crop(pil_img, top, left, crop_size, crop_size)\n",
        "  cropObj = transforms.RandomCrop(crop_size)\n",
        "  result = cropObj(pil_img)\n",
        "  ## end of function\n",
        "  to_tensor = transforms.ToTensor()\n",
        "  return to_tensor(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHJC-iHuTJD0",
        "colab_type": "code",
        "outputId": "d515ebf9-f45a-435d-ec3a-9a84334fb40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.LongTensor(1).random_(0, 10).item() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMg27oHMQ4J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_fashion_mnist_modified(images, labels,size):    \n",
        "    # Here _ means that we ignore (not use) variables\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
        "    for f, img, lbl in zip(figs, images, labels):\n",
        "        f.imshow(img.reshape((size, size)).numpy())\n",
        "        f.set_title(lbl)\n",
        "        f.axes.get_xaxis().set_visible(False)\n",
        "        f.axes.get_yaxis().set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaCBOPLxPRiY",
        "colab_type": "code",
        "outputId": "1a36eb91-0d19-477a-85df-e8d586498584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# test random_square_crop()\n",
        "# print display first 3 croped images \n",
        "X=[]\n",
        "y=[]\n",
        "size = 25\n",
        "for idx, data in enumerate(mnist_train):\n",
        "    if(idx>=0 and idx<3):\n",
        "        X.append(random_square_crop(data[0],size))\n",
        "        y.append(data[1])\n",
        "    if (idx>=3):\n",
        "        break\n",
        "show_fashion_mnist_modified(X, get_fashion_mnist_labels(y),size)\n"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 3 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"229.959301pt\" version=\"1.1\" viewBox=\"0 0 687.5 229.959301\" width=\"687.5pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 229.959301 \nL 687.5 229.959301 \nL 687.5 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 219.259301 \nL 207.641176 219.259301 \nL 207.641176 22.318125 \nL 10.7 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p6ba8548cf0)\">\n    <image height=\"197\" id=\"image5eea3217d7\" transform=\"scale(1 -1)translate(0 -197)\" width=\"197\" x=\"10.7\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.259301\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 219.259301 \nL 10.7 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 207.641176 219.259301 \nL 207.641176 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 219.259301 \nL 207.641176 219.259301 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 22.318125 \nL 207.641176 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- ankle boot -->\n    <defs>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(77.447463 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"124.658203\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"182.568359\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"210.351562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"271.875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"303.662109\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"367.138672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"428.320312\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"489.501953\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 247.029412 219.259301 \nL 443.970588 219.259301 \nL 443.970588 22.318125 \nL 247.029412 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p2b4954b082)\">\n    <image height=\"197\" id=\"image812fadf9de\" transform=\"scale(1 -1)translate(0 -197)\" width=\"197\" x=\"247.029412\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.259301\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 247.029412 219.259301 \nL 247.029412 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 443.970588 219.259301 \nL 443.970588 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 247.029412 219.259301 \nL 443.970588 219.259301 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 247.029412 22.318125 \nL 443.970588 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- t-shirt -->\n    <defs>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n    </defs>\n    <g transform=\"translate(327.569375 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 483.358824 219.259301 \nL 680.3 219.259301 \nL 680.3 22.318125 \nL 483.358824 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p1f4cbc95d8)\">\n    <image height=\"197\" id=\"imaged9ee9a955f\" transform=\"scale(1 -1)translate(0 -197)\" width=\"197\" x=\"483.358824\" xlink:href=\"data:image/png;base64,\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\" y=\"-22.259301\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 483.358824 219.259301 \nL 483.358824 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 680.3 219.259301 \nL 680.3 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 483.358824 219.259301 \nL 680.3 219.259301 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 483.358824 22.318125 \nL 680.3 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- t-shirt -->\n    <g transform=\"translate(563.898787 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6ba8548cf0\">\n   <rect height=\"196.941176\" width=\"196.941176\" x=\"10.7\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p2b4954b082\">\n   <rect height=\"196.941176\" width=\"196.941176\" x=\"247.029412\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p1f4cbc95d8\">\n   <rect height=\"196.941176\" width=\"196.941176\" x=\"483.358824\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L93Qidrow2xx",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of Softmax Regression from Scratch\n",
        "\n",
        ":label:`chapter_softmax_scratch`\n",
        "\n",
        "\n",
        "Just as we implemented linear regression from scratch,\n",
        "we believe that multiclass logistic (softmax) regression\n",
        "is similarly fundamental and you ought to know\n",
        "the gory details of how to implement it from scratch.\n",
        "As with linear regression, after doing things by hand\n",
        "we will breeze through an implementation in pytorch for comparison.\n",
        "To begin, let's import our packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tPFU0CHw2xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "from torchvision import transforms\n",
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.distributions import normal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzomkXXIw2x1",
        "colab_type": "text"
      },
      "source": [
        "We will work with the Fashion-MNIST dataset just introduced,\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "cuing up an iterator with batch size 256."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdlBe0Yvw2x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "trans = transforms.ToTensor() \n",
        "mnist_train = torchvision.datasets.FashionMNIST(root=\"./\", train=True, transform=trans, target_transform=None, download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(root=\"./\", train=False, transform=trans, target_transform=None, download=True)\n",
        "\n",
        "if sys.platform.startswith('win'):\n",
        "    num_workers = 0\n",
        "else:\n",
        "    num_workers = 4\n",
        "\n",
        "train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-MPKwR8w2x6",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Model Parameters\n",
        "\n",
        "Just as in linear regression, we represent each example as a vector.\n",
        "Since each example is a $28 \\times 28$ image,\n",
        "we can flatten each example, treating them as $784$ dimensional vectors.\n",
        "In the future, we'll talk about more sophisticated strategies\n",
        "for exploiting the spatial structure in images,\n",
        "but for now we treat each pixel location as just another feature.\n",
        "\n",
        "Recall that in softmax regression,\n",
        "we have as many outputs as there are categories.\n",
        "Because our dataset has $10$ categories,\n",
        "our network will have an output dimension of $10$.\n",
        "Consequently, our weights will constitute a $784 \\times 10$ matrix\n",
        "and the biases will constitute a $1 \\times 10$ vector.\n",
        "As with linear regression, we will initialize our weights $W$\n",
        "with Gaussian noise and our biases to take the initial value $0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "6UabEJZww2x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "W = normal.Normal(loc = 0, scale = 0.01).sample((num_inputs, num_outputs))\n",
        "b = torch.zeros(num_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlTTjTIOWc-k",
        "colab_type": "code",
        "outputId": "222b056f-86aa-43c7-8d94-5c243d41cb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "W[783]"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0065, -0.0071, -0.0115,  0.0056, -0.0173, -0.0093, -0.0052, -0.0021,\n",
              "        -0.0193, -0.0023])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGW-8_Ezw2x-",
        "colab_type": "text"
      },
      "source": [
        "Recall that we need to *attach gradients* to the model parameters.\n",
        "More literally, we are allocating memory for future gradients to be stored\n",
        "and notifiying PyTorch that we want gradients to be calculated with respect to these parameters in the first place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "10"
        },
        "id": "5J_fcKZ3w2x-",
        "colab_type": "code",
        "outputId": "391684f6-d9ea-405f-b798-e8055cc34a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W.requires_grad_(True)\n",
        "b.requires_grad_(True)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oX_ntFTw2yB",
        "colab_type": "text"
      },
      "source": [
        "## The Softmax\n",
        "\n",
        "Before implementing the softmax regression model,\n",
        "let's briefly review how `torch.sum` work\n",
        "along specific dimensions in a PyTorch tensor.\n",
        "Given a matrix `X` we can sum over all elements (default) or only\n",
        "over elements in the same column (`dim=0`) or the same row (`dim=1`).\n",
        "Note that if `X` is an array with shape `(2, 3)`\n",
        "and we sum over the columns (`torch.sum(X, dim=0`),\n",
        "the result will be a (1D) vector with shape `(3,)`.\n",
        "If we want to keep the number of axes in the original array\n",
        "(resulting in a 2D array with shape `(1,3)`),\n",
        "rather than collapsing out the dimension that we summed over\n",
        "we can specify `keepdim=True` when invoking `torch.sum`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "id": "K9PxpqKEw2yC",
        "colab_type": "code",
        "outputId": "0061bd2a-f9c8-4816-db45-178e3fba89ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "torch.sum(X, dim=0, keepdim=True), torch.sum(X, dim=1, keepdim=True)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[5, 7, 9]]), tensor([[ 6],\n",
              "         [15]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur13NL27w2yF",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to implement the softmax function.\n",
        "Recall that softmax consists of two steps:\n",
        "First, we exponentiate each term (using `torch.exp`).\n",
        "Then, we sum over each row (we have one row per example in the batch)\n",
        "to get the normalization constants for each example.\n",
        "Finally, we divide each row by its normalization constant,\n",
        "ensuring that the result sums to $1$.\n",
        "Before looking at the code, let's recall\n",
        "what this looks expressed as an equation:\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(X_{ij})}{\\sum_k \\exp(X_{ik})}\n",
        "$$\n",
        "\n",
        "The denominator, or normalization constant,\n",
        "is also sometimes called the partition function\n",
        "(and its logarithm the log-partition function).\n",
        "The origins of that name are in [statistical physics](https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics))\n",
        "where a related equation models the distribution\n",
        "over an ensemble of particles)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "id": "MzWPSo7Rw2yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    X_exp = torch.exp(X)\n",
        "    partition = torch.sum(X_exp, dim=1, keepdim=True)\n",
        "    return X_exp / partition  # The broadcast mechanism is applied here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPB6XxCpw2yI",
        "colab_type": "text"
      },
      "source": [
        "As you can see, for any random input, we turn each element into a non-negative number. Moreover, each row sums up to 1, as is required for a probability.\n",
        "Note that while this looks correct mathematically,\n",
        "we were a bit sloppy in our implementation\n",
        "because failed to take precautions against numerical overflow or underflow\n",
        "due to large (or very small) elements of the matrix,\n",
        "as we did in\n",
        ":numref:`chapter_naive_bayes`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "eOSmeZoww2yJ",
        "colab_type": "code",
        "outputId": "6a319d52-fee7-4898-f7fb-5bb8a439b50c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = normal.Normal(loc = 0, scale = 1).sample((2, 5))\n",
        "X_prob = softmax(X)\n",
        "X_prob, torch.sum(X_prob, dim=1)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1581, 0.3690, 0.0803, 0.3103, 0.0823],\n",
              "         [0.3476, 0.1297, 0.1079, 0.3421, 0.0727]]), tensor([1.0000, 1.0000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIT9FD3w2yO",
        "colab_type": "text"
      },
      "source": [
        "## The Model\n",
        "\n",
        "Now that we have defined the softmax operation,\n",
        "we can implement the softmax regression model.\n",
        "The below code defines the forward pass through the network.\n",
        "Note that we flatten each original image in the batch\n",
        "into a vector with length `num_inputs` with the `view` function\n",
        "before passing the data through our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "id": "Q3X7Qqhsw2yP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net(X):\n",
        "    return softmax(torch.matmul(X.reshape((-1, num_inputs)), W) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Rxpv2Sw2yS",
        "colab_type": "text"
      },
      "source": [
        "## The Loss Function\n",
        "\n",
        "Next, we need to implement the cross entropy loss function,\n",
        "introduced in :numref:`chapter_softmax`.\n",
        "This may be the most common loss function\n",
        "in all of deep learning because, at the moment,\n",
        "classification problems far outnumber regression problems.\n",
        "\n",
        "\n",
        "Recall that cross entropy takes the negative log likelihood\n",
        "of the predicted probability assigned to the true label $-\\log p(y|x)$.\n",
        "Rather than iterating over the predictions with a Python `for` loop\n",
        "(which tends to be inefficient), we can use the `gather` function\n",
        "which allows us to select the appropriate terms\n",
        "from the matrix of softmax entries easily.\n",
        "Below, we illustrate the `gather` function on a toy example,\n",
        "with 3 categories and 2 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkoFXd_Mw2yS",
        "colab_type": "code",
        "outputId": "a2dfdb7a-7397-488b-ce14-71ee2ab2b31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y = torch.tensor([0, 2])\n",
        "torch.gather(y_hat, 1, y.unsqueeze(dim=1)) # y has to be unsqueezed so that shape(y_hat) = shape(y)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1000],\n",
              "        [0.5000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49mz76mEw2yV",
        "colab_type": "text"
      },
      "source": [
        "Now we can implement the cross-entropy loss function efficiently\n",
        "with just one line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "16"
        },
        "id": "S3tZsMhSw2yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    return -torch.gather(y_hat, 1, y.unsqueeze(dim=1)).log()\n",
        "\n",
        "# y.unsqueeze(dim=1) -> [[0],\n",
        "#                       [2]]\n",
        "# https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms\n",
        "# https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch \n",
        "\n",
        "#torch.gather creates a new tensor from the input tensor by taking the values from each row along the input dimension dim. \n",
        "#The values in torch.LongTensor, passed as index, specify which value to take from each 'row'. \n",
        "#The dimension of the output tensor is same as the dimension of index tensor. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00hHeDRvw2yZ",
        "colab_type": "text"
      },
      "source": [
        "## Classification Accuracy\n",
        "\n",
        "Given the predicted probability distribution `y_hat`,\n",
        "we typically choose the class with highest predicted probability\n",
        "whenever we must output a *hard* prediction. Indeed, many applications require that we make a choice. Gmail must catetegorize an email into Primary, Social, Updates, or Forums. It might estimate probabilities internally, but at the end of the day it has to choose one among the categories.\n",
        "\n",
        "When predictions are consistent with the actual category `y`, they are correct. The classification accuracy is the fraction of all predictions that are correct. Although we cannot optimize accuracy directly (it is not differentiable), it's often the performance metric that we care most about, and we will nearly always report it when training classifiers.\n",
        "\n",
        "To compute accuracy we do the following:\n",
        "First, we execute `y_hat.argmax(dim=1)`\n",
        "to gather the predicted classes\n",
        "(given by the indices for the largest entires each row).\n",
        "The result has the same shape as the variable `y`.\n",
        "Now we just need to check how frequently the two match. The result is PyTorch tensor containing entries of 0 (false) and 1 (true). Since the attribute `mean` can only calculate the mean of floating types,\n",
        "we also need to convert the result to `float`. Taking the mean yields the desired result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "17"
        },
        "id": "9Xoycotnw2yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y):\n",
        "    return (y_hat.argmax(dim=1) == y).float().mean().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HFrssHzw2yc",
        "colab_type": "text"
      },
      "source": [
        "We will continue to use the variables `y_hat` and `y`\n",
        "defined in the `gather` function,\n",
        "as the predicted probability distribution and label, respectively.\n",
        "We can see that the first example's prediction category is 2\n",
        "(the largest element of the row is 0.6 with an index of 2),\n",
        "which is inconsistent with the actual label, 0.\n",
        "The second example's prediction category is 2\n",
        "(the largest element of the row is 0.5 with an index of 2),\n",
        "which is consistent with the actual label, 2.\n",
        "Therefore, the classification accuracy rate for these two examples is 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "18"
        },
        "id": "gP_SK1ASw2yd",
        "colab_type": "code",
        "outputId": "14b5b4ab-1924-4c14-8672-7003397d88f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(y_hat, y)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ7dZ4npw2yh",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we can evaluate the accuracy for model `net` on the data set\n",
        "(accessed via `data_iter`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "19"
        },
        "id": "AZaIP_8Ew2yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The function will be gradually improved: the complete implementation will be\n",
        "# discussed in the \"Image Augmentation\" section\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    for X, y in data_iter:\n",
        "        acc_sum += (net(X).argmax(dim=1) == y).sum().item()\n",
        "        n += y.size()[0]  # y.size()[0] = batch_size\n",
        "    return acc_sum / n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pqfr0c3w2yl",
        "colab_type": "text"
      },
      "source": [
        "Because we initialized the `net` model with random weights,\n",
        "the accuracy of this model should be close to random guessing,\n",
        "i.e. 0.1 for 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "id": "4DPnCy2Bw2yl",
        "colab_type": "code",
        "outputId": "8004bfa0-0859-46d1-9d1a-322d8b0cfe75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate_accuracy(test_iter, net)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0851"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofOHHe-ww2yo",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "The training loop for softmax regression should look strikingly familiar\n",
        "if you read through our implementation\n",
        "of linear regression earlier in this chapter.\n",
        "Again, we use the mini-batch stochastic gradient descent\n",
        "to optimize the loss function of the model.\n",
        "Note that the number of epochs (`num_epochs`),\n",
        "and learning rate (`lr`) are both adjustable hyper-parameters.\n",
        "By changing their values, we may be able to increase the classification accuracy of the model. In practice we'll want to split our data three ways\n",
        "into training, validation, and test data, using the validation data to choose the best values of our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "id": "vBjoEY9Dw2yp",
        "colab_type": "code",
        "outputId": "78e0f0d2-e0f2-4612-d273-cf20c7113477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "num_epochs, lr = 5, 0.001\n",
        "\n",
        "def train(net, train_iter, test_iter, loss, num_epochs, batch_size, trainer, params=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum = 0.0, 0.0\n",
        "        for X, y in train_iter:\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y).sum()\n",
        "            trainer.zero_grad() # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "            l.backward()                \n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()            \n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / len(train_iter), test_acc))\n",
        "\n",
        "trainer = torch.optim.SGD([W, b], lr) \n",
        "train(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, trainer, [W, b])"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 223.1888, train acc 188.617, test acc 0.807\n",
            "epoch 2, loss 158.3152, train acc 205.115, test acc 0.820\n",
            "epoch 3, loss 143.2626, train acc 208.477, test acc 0.828\n",
            "epoch 4, loss 134.9171, train acc 210.855, test acc 0.830\n",
            "epoch 5, loss 133.4325, train acc 211.213, test acc 0.824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oM_8pbYw2ys",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Now that training is complete, our model is ready to classify some images.\n",
        "Given a series of images, we will compare their actual labels\n",
        "(first line of text output) and the model predictions\n",
        "(second line of text output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "g8afzfbWw2yt",
        "colab_type": "code",
        "outputId": "7936b072-a6ed-4134-f133-677b83cf3654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "for X, y in test_iter:\n",
        "    break\n",
        "\n",
        "# the functions \"get_fashion_mnist_labels\" and \"show_fashion_mnist\" are defined above in this notebook\n",
        "true_labels = get_fashion_mnist_labels(y.detach().numpy())\n",
        "pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1).detach().numpy())\n",
        "titles = [truelabel + '\\n' + predlabel for truelabel, predlabel in zip(true_labels, pred_labels)]\n",
        "\n",
        "show_fashion_mnist(X[10:20], titles[10:20])"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 10 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"103.201263pt\" version=\"1.1\" viewBox=\"0 0 687.5 103.201263\" width=\"687.5pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 103.201263 \nL 687.5 103.201263 \nL 687.5 -0 \nL 0 -0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 92.501263 \nL 67.445763 92.501263 \nL 67.445763 35.7555 \nL 10.7 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd98c542848)\">\n    <image height=\"57\" id=\"image2bdb5d870e\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"10.7\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 92.501263 \nL 10.7 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 67.445763 92.501263 \nL 67.445763 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 92.501263 \nL 67.445763 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 35.7555 \nL 67.445763 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- coat -->\n    <defs>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n    </defs>\n    <g transform=\"translate(26.073506 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- coat -->\n    <g transform=\"translate(26.073506 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 78.794915 92.501263 \nL 135.540678 92.501263 \nL 135.540678 35.7555 \nL 78.794915 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p626ef3da95)\">\n    <image height=\"57\" id=\"image72082c2574\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"78.794915\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 78.794915 92.501263 \nL 78.794915 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 135.540678 92.501263 \nL 135.540678 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 78.794915 92.501263 \nL 135.540678 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 78.794915 35.7555 \nL 135.540678 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- sandal -->\n    <defs>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n    </defs>\n    <g transform=\"translate(87.409984 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n    <!-- sandal -->\n    <g transform=\"translate(87.409984 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 146.889831 92.501263 \nL 203.635593 92.501263 \nL 203.635593 35.7555 \nL 146.889831 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa659087da4)\">\n    <image height=\"57\" id=\"image373770aa67\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"146.889831\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 146.889831 92.501263 \nL 146.889831 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 203.635593 92.501263 \nL 203.635593 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 146.889831 92.501263 \nL 203.635593 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 146.889831 35.7555 \nL 203.635593 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- sneaker -->\n    <defs>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n    </defs>\n    <g transform=\"translate(151.550524 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"115.478516\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"177.001953\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"238.28125\" xlink:href=\"#DejaVuSans-107\"/>\n     <use x=\"292.566406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"354.089844\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- sandal -->\n    <g transform=\"translate(155.504899 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"113.378906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"176.757812\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"240.234375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"301.513672\" xlink:href=\"#DejaVuSans-108\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 214.984746 92.501263 \nL 271.730508 92.501263 \nL 271.730508 35.7555 \nL 214.984746 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5e4aa3ae82)\">\n    <image height=\"57\" id=\"image44d4b9135e\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"214.984746\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 214.984746 92.501263 \nL 214.984746 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 271.730508 92.501263 \nL 271.730508 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 214.984746 92.501263 \nL 271.730508 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 214.984746 35.7555 \nL 271.730508 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- dress -->\n    <g transform=\"translate(227.273877 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"163.863281\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"215.962891\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n    <!-- dress -->\n    <g transform=\"translate(227.273877 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"163.863281\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"215.962891\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 283.079661 92.501263 \nL 339.825424 92.501263 \nL 339.825424 35.7555 \nL 283.079661 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pc450d5a50b)\">\n    <image height=\"57\" id=\"imaged8478f380d\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"283.079661\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 283.079661 92.501263 \nL 283.079661 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 339.825424 92.501263 \nL 339.825424 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 283.079661 92.501263 \nL 339.825424 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 283.079661 35.7555 \nL 339.825424 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- coat -->\n    <g transform=\"translate(298.453167 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- coat -->\n    <g transform=\"translate(298.453167 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 351.174576 92.501263 \nL 407.920339 92.501263 \nL 407.920339 35.7555 \nL 351.174576 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p385b881181)\">\n    <image height=\"57\" id=\"imagecd50ced354\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"351.174576\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 351.174576 92.501263 \nL 351.174576 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 407.920339 92.501263 \nL 407.920339 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 351.174576 92.501263 \nL 407.920339 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 351.174576 35.7555 \nL 407.920339 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- trouser -->\n    <defs>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n    </defs>\n    <g transform=\"translate(358.105895 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(358.105895 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"78.072266\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"139.253906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"202.632812\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"254.732422\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"316.255859\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 419.269492 92.501263 \nL 476.015254 92.501263 \nL 476.015254 35.7555 \nL 419.269492 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pdeb40b58c5)\">\n    <image height=\"57\" id=\"image91ec74c57b\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"419.269492\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 419.269492 92.501263 \nL 419.269492 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 476.015254 92.501263 \nL 476.015254 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 419.269492 92.501263 \nL 476.015254 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 419.269492 35.7555 \nL 476.015254 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- pullover -->\n    <defs>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g transform=\"translate(423.316123 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n    <!-- pullover -->\n    <g transform=\"translate(423.316123 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"243.603516\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"302.783203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"364.306641\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 487.364407 92.501263 \nL 544.110169 92.501263 \nL 544.110169 35.7555 \nL 487.364407 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd699367f84)\">\n    <image height=\"57\" id=\"image551acbbc8e\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"487.364407\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 487.364407 92.501263 \nL 487.364407 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 544.110169 92.501263 \nL 544.110169 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 487.364407 92.501263 \nL 544.110169 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 487.364407 35.7555 \nL 544.110169 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_8\">\n    <!-- coat -->\n    <g transform=\"translate(502.737913 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- coat -->\n    <g transform=\"translate(502.737913 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"116.162109\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"177.441406\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g id=\"patch_42\">\n    <path d=\"M 555.459322 92.501263 \nL 612.205085 92.501263 \nL 612.205085 35.7555 \nL 555.459322 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p44164b092a)\">\n    <image height=\"57\" id=\"image494c22d89e\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"555.459322\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 555.459322 92.501263 \nL 555.459322 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 612.205085 92.501263 \nL 612.205085 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 555.459322 92.501263 \nL 612.205085 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 555.459322 35.7555 \nL 612.205085 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_9\">\n    <!-- bag -->\n    <defs>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n    </defs>\n    <g transform=\"translate(572.537203 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"124.755859\" xlink:href=\"#DejaVuSans-103\"/>\n    </g>\n    <!-- bag -->\n    <g transform=\"translate(572.537203 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"124.755859\" xlink:href=\"#DejaVuSans-103\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_10\">\n   <g id=\"patch_47\">\n    <path d=\"M 623.554237 92.501263 \nL 680.3 92.501263 \nL 680.3 35.7555 \nL 623.554237 35.7555 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p134ce76d32)\">\n    <image height=\"57\" id=\"image9e4c7cbe0c\" transform=\"scale(1 -1)translate(0 -57)\" width=\"57\" x=\"623.554237\" xlink:href=\"data:image/png;base64,\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\" y=\"-35.501263\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path d=\"M 623.554237 92.501263 \nL 623.554237 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path d=\"M 680.3 92.501263 \nL 680.3 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path d=\"M 623.554237 92.501263 \nL 680.3 92.501263 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path d=\"M 623.554237 35.7555 \nL 680.3 35.7555 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_10\">\n    <!-- t-shirt -->\n    <defs>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n    </defs>\n    <g transform=\"translate(633.996494 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n    <!-- t-shirt -->\n    <g transform=\"translate(633.996494 29.7555)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"75.292969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"127.392578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"190.771484\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"218.554688\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"259.667969\" xlink:href=\"#DejaVuSans-116\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd98c542848\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"10.7\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p626ef3da95\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"78.794915\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pa659087da4\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"146.889831\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p5e4aa3ae82\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"214.984746\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pc450d5a50b\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"283.079661\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p385b881181\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"351.174576\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pdeb40b58c5\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"419.269492\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"pd699367f84\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"487.364407\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p44164b092a\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"555.459322\" y=\"35.7555\"/>\n  </clipPath>\n  <clipPath id=\"p134ce76d32\">\n   <rect height=\"56.745763\" width=\"56.745763\" x=\"623.554237\" y=\"35.7555\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GPYmu7w2yw",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "With softmax regression, we can train models for multi-category classification. The training loop is very similar to that in linear regression: retrieve and read data, define models and loss functions,\n",
        "then train models using optimization algorithms. As you'll soon find out, most common deep learning models have similar training procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38wFQlq5w2yw",
        "colab_type": "text"
      },
      "source": [
        "# Concise Implementation of Softmax Regression\n",
        "\n",
        "Just as PyTorch made it much easier to implement linear regression, we'll find it similarly (or possibly more)\n",
        "convenient for implementing classification models. Again, we begin with our import ritual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXA_XBgcw2yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnM23r-Cw2y0",
        "colab_type": "text"
      },
      "source": [
        "Let's stick with the Fashion-MNIST dataset and keep the batch size at $256$ as in the last section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSab6J62w2y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "# By default pytorch torchvision datasets are of type PIL.\n",
        "# Define a transform \"trans\" to change the PIL to Tensor format.\n",
        "trans = torchvision.transforms.ToTensor() \n",
        "mnist_train = torchvision.datasets.FashionMNIST(root=\"./\", train=True, transform=trans, target_transform=None, download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(root=\"./\", train=False, transform=trans, target_transform=None, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQuTYGmzEXpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if sys.platform.startswith('win'):\n",
        "    # set 0 for windows\n",
        "    # 0 means no additional processes are needed to speed up the reading of data\n",
        "    num_workers = 0\n",
        "else:\n",
        "    num_workers = 4\n",
        "\n",
        "train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxqiAjbEw2y4",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Model Parameters\n",
        "\n",
        "As mentioned in `chapter_softmax`, the output layer of softmax regression is a fully connected (`Linear`) layer. Therefore, to implement our model, we just need to add one `Linear` layer with 10 outputs to our `Sequential`. Again, here, the `Sequential` isn't really necessary, but we might as well form the habit since it will be ubiquitous when implementing deep models. Again, we initialize the weights at random with zero mean and standard deviation 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAS0Yg8lw2y4",
        "colab_type": "code",
        "outputId": "38da8ec1-e13a-4005-e9ec-734ba1c0695d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(-1,784)\n",
        "    \n",
        "net = nn.Sequential(Reshape(), nn.Linear(784, 10))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Reshape()\n",
              "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCGXS2ew2y9",
        "colab_type": "text"
      },
      "source": [
        "## The Softmax\n",
        "\n",
        "In the previous example, we calculated our model's output and then ran this\n",
        "output through the cross-entropy loss. Mathematically, that's a perfectly reasonable thing to do. However,\n",
        "computationally, things can get hairy when dealing with exponentiation due to\n",
        "numerical stability issues, a matter we've already discussed a few times\n",
        "(e.g. in `chapter_naive_bayes`) and in the problem set of the previous chapter).\n",
        "Recall that the softmax function calculates $\\hat y_j = \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}$, where $\\hat y_j$\n",
        "is the j-th element of ``yhat`` and $z_j$ is the j-th element of the input\n",
        "``y_linear`` variable, as computed by the softmax.\n",
        "\n",
        "If some of the $z_i$ are very large (i.e. very positive),\n",
        "$e^{z_i}$ might be larger than the largest number\n",
        "we can have for certain types of ``float`` (i.e. overflow).\n",
        "This would make the denominator (and/or numerator) ``inf`` and we get zero,\n",
        "or ``inf``, or ``nan`` for $\\hat y_j$.\n",
        "In any case, we won't get a well-defined return value for ``cross_entropy``. This is the reason we subtract $\\text{max}(z_i)$\n",
        "from all $z_i$ first in ``softmax`` function.\n",
        "You can verify that this shifting in $z_i$\n",
        "will not change the return value of ``softmax``.\n",
        "\n",
        "After the above subtraction/ normalization step,\n",
        "it is possible that $z_j$ is very negative.\n",
        "Thus, $e^{z_j}$ will be very close to zero\n",
        "and might be rounded to zero due to finite precision (i.e underflow),\n",
        "which makes $\\hat y_j$ zero and we get ``-inf`` for $\\text{log}(\\hat y_j)$.\n",
        "A few steps down the road in backpropagation,\n",
        "we start to get horrific not-a-number (``nan``) results printed to screen.\n",
        "\n",
        "Our salvation is that even though we're computing these exponential functions, we ultimately plan to take their log in the cross-entropy functions.\n",
        "It turns out that by combining these two operators\n",
        "``softmax`` and ``cross_entropy`` together,\n",
        "we can escape the numerical stability issues\n",
        "that might otherwise plague us during backpropagation.\n",
        "As shown in the equation below, we avoided calculating $e^{z_j}$\n",
        "but directly used $z_j$ due to $\\log(\\exp(\\cdot))$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log{(\\hat y_j)} & = \\log\\left( \\frac{e^{z_j}}{\\sum_{i=1}^{n} e^{z_i}}\\right) \\\\\n",
        "& = \\log{(e^{z_j})}-\\text{log}{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)} \\\\\n",
        "& = z_j -\\log{\\left( \\sum_{i=1}^{n} e^{z_i} \\right)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We'll want to keep the conventional softmax function handy\n",
        "in case we ever want to evaluate the probabilities output by our model.\n",
        "But instead of passing softmax probabilities into our new loss function,\n",
        "we'll just pass $\\hat{y}$ and compute the softmax and its log\n",
        "all at once inside the softmax_cross_entropy loss function,\n",
        "which does smart things like the log-sum-exp trick ([see on Wikipedia](https://en.wikipedia.org/wiki/LogSumExp)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBnzvdFrw2y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3iv3YPDw2zD",
        "colab_type": "text"
      },
      "source": [
        "## Optimization Algorithm\n",
        "\n",
        "We use the mini-batch random gradient descent\n",
        "with a learning rate of $0.1$ as the optimization algorithm.\n",
        "Note that this is the same choice as for linear regression\n",
        "and it illustrates the general applicability of the optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ui1tWpw2zE",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Next, we use the training functions defined in the last section to train a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L-77QQGw2zE",
        "colab_type": "code",
        "outputId": "71776776-409d-48eb-a31f-a9fd170bafb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "num_epochs = 10\n",
        "lr = 0.1\n",
        "\n",
        "trainer = torch.optim.SGD(net.parameters(), lr) \n",
        "\n",
        "for epoch in range(num_epochs): \n",
        "  train_loss_epoch = 0.0\n",
        "  net.train()\n",
        "  for X, y in train_iter:  \n",
        "    y_hat = net(X)\n",
        "    l = loss(y_hat, y)\n",
        "    train_loss_epoch += l\n",
        "    trainer.zero_grad() \n",
        "    l.backward() \n",
        "    trainer.step() \n",
        "  train_loss_epoch /= len(train_iter)\n",
        "  \n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss_epoch = 0.0\n",
        "    for X_test, y_test in test_iter:\n",
        "      b_l = loss(net(X_test), y_test) \n",
        "      test_loss_epoch += b_l\n",
        "    test_loss_epoch /= len(test_iter)  \n",
        "  print('epoch {}, train loss {}, test loss {}'.format(epoch+1, train_loss_epoch, test_loss_epoch)) "
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 0.7851331830024719, test loss 0.6357836127281189\n",
            "epoch 2, train loss 0.5712599754333496, test loss 0.5887656211853027\n",
            "epoch 3, train loss 0.5256920456886292, test loss 0.5510953664779663\n",
            "epoch 4, train loss 0.5016950964927673, test loss 0.5167006254196167\n",
            "epoch 5, train loss 0.485610693693161, test loss 0.5023133158683777\n",
            "epoch 6, train loss 0.4740023612976074, test loss 0.500957727432251\n",
            "epoch 7, train loss 0.4647983908653259, test loss 0.5098615884780884\n",
            "epoch 8, train loss 0.4588271379470825, test loss 0.49713334441185\n",
            "epoch 9, train loss 0.4521200656890869, test loss 0.4949071407318115\n",
            "epoch 10, train loss 0.44766396284103394, test loss 0.4818187355995178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po4aUnePw2zH",
        "colab_type": "text"
      },
      "source": [
        "Note that in many cases, PyTorch takes specific precautions\n",
        "in addition to the most well-known tricks for ensuring numerical stability.\n",
        "This saves us from many common pitfalls that might befall us\n",
        "if we were to code all of our models from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrJH5iHZxbSY",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** We will copy the code from above, and will place it in the cell below. Change the function below to compute the train and test accuracies.\n",
        "However, this time compute top5 accuracy, and return it for test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfsddH6gtE0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, absolute_import\n",
        "\n",
        "__all__ = ['accuracy']\n",
        "\n",
        "def accuracy_topk(output, target, topk=(5,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk) \n",
        "    #print(maxk) #5\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    #print(pred) for a single test sample\n",
        "    #tensor([[9, 2, 1,  ..., 8, 1, 5],\n",
        "     #   [7, 6, 0,  ..., 6, 3, 7],\n",
        "      #  [5, 4, 4,  ..., 3, 4, 8],\n",
        "      #  [8, 0, 3,  ..., 0, 2, 9],\n",
        "       # [2, 8, 2,  ..., 4, 0, 2]])\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    \n",
        "    #print(correct)\n",
        "    #tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
        "    #    [False, False, False,  ..., False, False, False],\n",
        "     #   [False, False, False,  ..., False, False, False],\n",
        "      #  [False, False, False,  ..., False, False, False],\n",
        "       # [False, False, False,  ..., False, False, False]])\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        #print(correct_k)\n",
        "        #tensor(9948.)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBIOtmXqw2zI",
        "colab_type": "code",
        "outputId": "b0897925-3ce9-4807-8756-ca3fb9cbe711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net.apply(init_weights) # this line is to reset the weights of the network\n",
        "\n",
        "def good_num_epochs():\n",
        "  ## compute the top5 accuracy and return it  \n",
        "  lr = 0.1\n",
        "  trainer = torch.optim.SGD(net.parameters(), lr) \n",
        "  num_epochs = 10\n",
        "  top5_acc_test = 0.0 #top 5 test accuracy?\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for epoch in range(num_epochs): \n",
        "    train_loss_epoch = 0.0\n",
        "    net.train()\n",
        "    for X, y in train_iter:  \n",
        "      y_hat = net(X)\n",
        "      l = loss(y_hat, y)\n",
        "      train_loss_epoch += l\n",
        "      trainer.zero_grad() \n",
        "      l.backward() \n",
        "      trainer.step() \n",
        "    train_loss_epoch /= len(train_iter)\n",
        "    train_acc = evaluate_accuracy(train_iter,net)\n",
        "  test_iter = DataLoader(mnist_test, len(mnist_test), shuffle=False, num_workers=0)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "      test_loss_epoch = 0.0\n",
        "      for X_test, y_test in test_iter:\n",
        "        y_test_hat = net(X_test)\n",
        "        b_l = loss(net(X_test), y_test) \n",
        "        test_loss_epoch += b_l\n",
        "        total += y_test.size(0)\n",
        "        correct += (y_test_hat.argmax(dim=1) == y_test).sum().item() # implement top5 score \n",
        "      test_loss_epoch /= len(test_iter)      \n",
        "  #test_iter = DataLoader(mnist_test[0], 1, shuffle=False, num_workers=0)  \n",
        "    #top5_acc_test = correct / total\n",
        "  test_acc = evaluate_accuracy(test_iter,net)\n",
        "  top5_acc_test = accuracy_topk(y_test_hat,y_test)[0].item()/100\n",
        "  #top5_acc_test = round(top5_acc_test[0].item(),4)\n",
        "    #print('epoch {}, train loss {}, test loss {} , train acc {}, test acc {}'.format(epoch+1, train_loss_epoch, test_loss_epoch,train_acc,test_acc)) \n",
        "  #print(test_acc)\n",
        "  ## end of change area\n",
        "  #return round(top5_acc_test[0].item(),4)\n",
        "  return top5_acc_test\n",
        " \n",
        "good_num_epochs()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9956999969482422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_ma2FHESWY",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1H37UZjESWY",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "\\text{accuracy} = \\frac{1}{N} \\text{# correctly classified}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8N7wNZOESWZ",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjJMinqHESWZ",
        "colab_type": "text"
      },
      "source": [
        "|          | Predicted Positive | Predicted Negative |\n",
        "|----------| ------------------ |:------------------:|\n",
        "| **Positive** | True Positive ($TP$) | False Negative ($FN$)|\n",
        "| **Negative** | False Positive ($FP$)| True Negative ($TN$) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xrw0B8QESWa",
        "colab_type": "text"
      },
      "source": [
        "#### Trade-off between Sensitivity and Specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya2ib5iFESWa",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "\\text{sensitivity} = \\frac{TP}{TP + FN}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-0CDZxPESWb",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation}\n",
        "\\text{specificity} = \\frac{TP}{TP + FP}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ODQFbKNvMH",
        "colab_type": "text"
      },
      "source": [
        "#### Question on Confusion Matrix (TODO)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLSEf4wGPh0J",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** as you have noticed already, the Fashion-MNIST dataset offers multiple categories (classes), this is a multi-class classification problem. Another classification task would be binary classification between cats and dogs, for instance. Using Fashion-MNIST data, create a model that classifies between trousers and t-shirts only. This would cast the problem as a binary classification task.\n",
        "\n",
        "*Note:* you can use the same network architecture from above (with only one layer). The only difference is in the loss function you use, in pytorch. Because now the output of the model should be using \"Sigmoid\", instead of a \"Softmax\". \n",
        "\n",
        "Write a function that computes the following metrics on the *test set* of Fashion-MNIST using those two classes (trousers vs. t-shirts). Compute the true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, sensitivity, specificity, f1-score, and accuracy. [Read more here](https://en.wikipedia.org/wiki/Sensitivity_and_specificity). The function should return a dictionary with all these values as entries (please fill them in as normal float numbers). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oYvInFt20jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BCE_loss1(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWrohgJtPhGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "def trousers_vs_t_shirts():  \n",
        "  output_metrics = dict()\n",
        "  output_metrics[\"true_positive_rate\"] = None\n",
        "  output_metrics[\"false_positive_rate\"] = None\n",
        "  output_metrics[\"true_negative_rate\"] = None\n",
        "  output_metrics[\"false_negative_rate\"] = None\n",
        "  output_metrics[\"precision\"] = None\n",
        "  output_metrics[\"recall\"] = None\n",
        "  output_metrics[\"sensitivity\"] = None\n",
        "  output_metrics[\"specificity\"] = None\n",
        "  output_metrics[\"f1_score\"] = None\n",
        "  output_metrics[\"accuracy\"] = None  \n",
        "  np.random.seed(0)\n",
        "  torch.manual_seed(0)\n",
        "  \n",
        "  ## write your code here\n",
        "  # 1. create a network with a sigmoid output layer.\n",
        "        \n",
        "  net = nn.Sequential(Reshape(), nn.Linear(784, 1))\n",
        "  net.apply(init_weights)\n",
        "  loss = nn.BCEWithLogitsLoss()\n",
        "         \n",
        "    \n",
        "  num_epochs = 10\n",
        "  lr = 0.1\n",
        "  trainer = torch.optim.SGD(net.parameters(), lr) \n",
        "\n",
        "  for epoch in range(num_epochs): \n",
        "      train_loss_epoch = 0.0\n",
        "      net.train()\n",
        "      for X, y in train_iter:  \n",
        "          indices_train = (y == 0) | (y == 1) \n",
        "          X_train = X[indices_train]\n",
        "          y_train = y[indices_train]\n",
        "                    \n",
        "          y_hat = net(X_train.type(torch.FloatTensor))\n",
        "          y_train = y_train.type_as(y_hat)\n",
        "          l = loss(y_hat, y_train.reshape(y_hat.shape))\n",
        "          train_loss_epoch += l\n",
        "          trainer.zero_grad() \n",
        "          l.backward() \n",
        "          trainer.step()\n",
        "      #print('epoch {}'.format(epoch+1))\n",
        "          \n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss_epoch = 0.0\n",
        "    if sys.platform.startswith('win'):\n",
        "      # set 0 for windows\n",
        "      # 0 means no additional processes are needed to speed up the reading of data\n",
        "      num_workers = 0\n",
        "    else:\n",
        "      num_workers = 4\n",
        "    \n",
        "    test_iter = DataLoader(mnist_test, len(mnist_test), shuffle=False, num_workers=num_workers)\n",
        "    for X_test, y_test in test_iter:\n",
        "        indices_test = (y_test == 0) | (y_test == 1) \n",
        "        X_test = X_test[indices_test]\n",
        "        y_test = y_test[indices_test]\n",
        "        y_test_hat = net(X_test.type(torch.FloatTensor))\n",
        "        y_test = y_test.type_as(y_test_hat)\n",
        "        y_test_hat = y_test_hat.reshape(y_test.shape)\n",
        "        b_l = loss(y_test_hat, y_test) \n",
        "        test_loss_epoch += b_l\n",
        "        total = y_test.size(0)\n",
        "        # The predicted results are processed through BCElosswithlogits, hence the outputs are\n",
        "        # passed through the sigmoid layer to turn it into probabilities. \n",
        "        y_test_hat = torch.sigmoid(y_test_hat)\n",
        "        \n",
        "        #print(total)\n",
        "        #print(y_test_hat)\n",
        "        correct = (y_test_hat>0.5) == y_test    \n",
        "        #print(y_test_hat)\n",
        "        trousers_hat = correct.float().sum()    \n",
        "        #print(trousers_hat)\n",
        "        t_shirts_hat = total - trousers_hat\n",
        "        #print(t_shirts_hat)\n",
        "        trousers = (y_test > 0).float().sum()\n",
        "        t_shirts =total - trousers\n",
        "        #print(trousers)\n",
        "        #print(t_shirts)\n",
        "                \n",
        "  #print(BCE_loss(y_test_hat,y_test))   \n",
        "  #2. fill in the correct values in the output_metrics dictionary\n",
        " # Confusion matrix\n",
        "  confusion_mat=confusion_matrix(y_test.numpy(), y_test_hat.round().numpy())\n",
        " \n",
        " # FP = confusion_mat.sum(axis=0) - np.diag(confusion_mat) \n",
        "  FP = confusion_mat[0][1]\n",
        " \n",
        "  #FN = confusion_mat.sum(axis=1) - np.diag(confusion_mat)\n",
        "  FN = confusion_mat[1][0]\n",
        "  \n",
        "  #TP = np.diag(confusion_mat)\n",
        "  TP = confusion_mat[1][1]\n",
        " \n",
        "  #TN = confusion_mat.sum() - (FP + FN + TP)  \n",
        "  TN = confusion_mat[0][0]\n",
        "\n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  TPR = TP/(TP+FN)\n",
        "  # Specificity or true negative rate\n",
        "  TNR = TN/(TN+FP) \n",
        "  # Precision or positive predictive value\n",
        "  PPV = TP/(TP+FP)\n",
        "  # Negative predictive value\n",
        "  NPV = TN/(TN+FN)\n",
        "  # Fall out or false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  # False negative rate\n",
        "  FNR = FN/(TP+FN)\n",
        "  # False discovery rate\n",
        "  FDR = FP/(TP+FP)\n",
        "\n",
        "  # Overall accuracy\n",
        "  ACC = (TP+TN)/(TP+FP+FN+TN)\n",
        "\n",
        "  clss_report = classification_report(y_test.numpy(), y_test_hat.round().numpy(), target_names=[\"t_shirts\",\"trousers\"])\n",
        "  #print(clss_report)\n",
        "  output_metrics[\"true_positive_rate\"] = TPR\n",
        "  output_metrics[\"false_positive_rate\"] = FPR\n",
        "  output_metrics[\"true_negative_rate\"] = TNR\n",
        "  output_metrics[\"false_negative_rate\"] = FNR\n",
        "  output_metrics[\"precision\"] = PPV\n",
        "  output_metrics[\"recall\"] = NPV\n",
        "  output_metrics[\"sensitivity\"] = TPR\n",
        "  output_metrics[\"specificity\"] = TNR\n",
        "  output_metrics[\"f1_score\"] =2 * ((PPV * NPV)/ (PPV + NPV))\n",
        "  output_metrics[\"accuracy\"] = ACC\n",
        "  #print(clss_report)\n",
        "  \n",
        "  ## end of function\n",
        "  return output_metrics\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaYK-ItZykVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "7867182a-77e2-439b-e513-8c5d74e1b296"
      },
      "source": [
        "  trousers_vs_t_shirts()"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9845,\n",
              " 'f1_score': 0.98451205743566,\n",
              " 'false_negative_rate': 0.019,\n",
              " 'false_positive_rate': 0.012,\n",
              " 'precision': 0.9879154078549849,\n",
              " 'recall': 0.9811320754716981,\n",
              " 'sensitivity': 0.981,\n",
              " 'specificity': 0.988,\n",
              " 'true_negative_rate': 0.988,\n",
              " 'true_positive_rate': 0.981}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    }
  ]
}